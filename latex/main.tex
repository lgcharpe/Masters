\documentclass[UKenglish]{ifimaster}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc,url}
\urlstyle{sf}
\usepackage{babel,textcomp,csquotes,duomasterforside,varioref,graphicx}
\usepackage[backend=biber,style=numeric-comp]{biblatex}

\title{The title of my thesis}
\subtitle{Any short subtitle}
\author{Lucas Charpentier}

\bibliography{mybib}

\begin{document}
\duoforside[program={Computational Science},
    option={Imaging and Biomedical Computing},
    dept={Departement of Informatics \and Departement of Physics},
    long
    ]
\frontmatter{}
\maketitle{}

\chapter*{Abstract}
\tableofcontents{}
\listoffigures{}
\listoftables{}

\chapter*{Preface}

\mainmatter{}

\chapter{Introduction}
    \section{Background and Motivation}
    
    
    \section{Problem Statement}


    \section{Thesis Outline}


\chapter{Planning the project}
    \section{Machine Learning}
        \subsection{Supervised Learning}
        
        
        \subsection{Unsupervised Learning}

    
    \section{Artificial Neural Networks}
        \subsection{Perceptron}


        \subsection{Multilayer Perceptron}


        \subsection{Training a Neural Network}

    
    \section{Convolutional Neural Network}
        \subsection{Convolutional Layers}


        \subsection{Pooling Layers}

    
    \section{Neural Network Training Optimization}
        \subsection{Weight Initialization}


        \subsection{Training Batch Size}


        \subsection{Dropout}


    \section{Network Pruning}

    
    \section{Datasets}
        \subsection{MNIST}


        \subsection{Fashion MNIST}


        \subsection{CIFAR-10}


    \section{Architectures}
        \subsection{VGG-16}

\chapter{Single Layer ANN}
    \section{Pruning Nodes at Random}


    \section{Estimating Node Importance based on Loss and Accuracy}
    
    
    \section{Pruning Nodes based on the Loss and Accuracy}


    \section{Effects of Changing Training Batch Size on Node Importance}


    \section{Effects of Using Droput}


\chapter{Multi-Layer Perceptron}
    \section{Pruning network with pre-calculated importance}


    \section{Greedy approach to pruning instead of Exhaustive approach}


    \section{Iterative weight initialization using Node importance}



\chapter{Convolutional Neural Network}
    \section{Looking at effects of per class accuracy after pruning}


    \section{Pruning based on class accuracy}


\chapter{Case study: Reducing a VGG-16 model trained on X dataset}

\chapter{Conclusion}
    \section{Summary}


    \section{Future Works}


\backmatter{}
\printbibliography
\end{document}