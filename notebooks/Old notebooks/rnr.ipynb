{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import tqdm\n",
    "import IProgress\n",
    "from hfunc import models\n",
    "from hfunc import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "class_accuracy = metrics.ClassAccuracy()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2552 - accuracy: 0.9270 - val_loss: 0.1353 - val_accuracy: 0.9600\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1147 - accuracy: 0.9665 - val_loss: 0.1004 - val_accuracy: 0.9676\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0785 - accuracy: 0.9771 - val_loss: 0.0842 - val_accuracy: 0.9742\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0592 - accuracy: 0.9822 - val_loss: 0.0774 - val_accuracy: 0.9759\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0470 - accuracy: 0.9850 - val_loss: 0.0789 - val_accuracy: 0.9773\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 0.0797 - val_accuracy: 0.9760\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0290 - accuracy: 0.9912 - val_loss: 0.0880 - val_accuracy: 0.9747\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0236 - accuracy: 0.9929 - val_loss: 0.0762 - val_accuracy: 0.9788\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0185 - accuracy: 0.9945 - val_loss: 0.0720 - val_accuracy: 0.9802\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.0820 - val_accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0820 - accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "old = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_nodes(acc, loss, weights, n, to_test, x_train, y_train, v=0, remove='positive'):\n",
    "    new_loss = loss\n",
    "    new_acc = acc\n",
    "    if remove == 'positive':\n",
    "        best_score = 0\n",
    "    elif remove == 'always':\n",
    "        best_score = -1e20\n",
    "    else:\n",
    "        best_score = 0.5\n",
    "    best_model = copy.deepcopy(weights)\n",
    "    nodes_removed = np.array([])\n",
    "    for _ in tqdm.trange(to_test):   \n",
    "        new = copy.deepcopy(weights)\n",
    "        to_drop = np.random.choice(len(new[1]), n, replace=False)\n",
    "        for i in to_drop:\n",
    "            new[0][:,i] = 0\n",
    "            new[1][i] = 0\n",
    "            new[2][i,:] = 0\n",
    "        model.set_weights(new)\n",
    "        new_loss, new_acc = model.evaluate(x_train, y_train, verbose=v)\n",
    "        score = (1 - (new_loss / loss)) + ((new_acc / acc) - 1)\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_model = copy.deepcopy(new)\n",
    "            nodes_removed = to_drop.copy()\n",
    "    return best_model, best_score, nodes_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:24<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "to_check = 100\n",
    "new_loss = loss\n",
    "new_acc = acc\n",
    "best_score = 0\n",
    "best_model = copy.deepcopy(old)\n",
    "\n",
    "for i in tqdm.trange(to_check):   \n",
    "    new = copy.deepcopy(old)\n",
    "    #for i in range(len(old)):\n",
    "    #    new[i] = old[i].copy()\n",
    "    to_drop = np.random.choice(len(new[1]), n, replace=False)\n",
    "    # for i in to_drop:\n",
    "    new[0][:,i] = 0\n",
    "    new[1][i] = 0\n",
    "    new[2][i,:] = 0\n",
    "    model.set_weights(new)\n",
    "    new_loss, new_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    score = (1 - (new_loss / loss)) + ((new_acc / acc) - 1)\n",
    "    score1 = (1 - (new_loss / loss))\n",
    "    score2 = ((new_acc / acc) - 1)\n",
    "    if best_score < score1 or best_score < score2:\n",
    "        best_score = score1\n",
    "        best_model = copy.deepcopy(new)\n",
    "    if best_score < score2:\n",
    "        best_score = score2\n",
    "        best_model = copy.deepcopy(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16169311, -0.10345983,  0.00614873, -0.0794104 ,  0.16868322,\n",
       "        0.        , -0.01383742,  0.16401708,  0.1587898 ,  0.18996462,\n",
       "        0.08559503,  0.1642694 , -0.1717523 ,  0.0043246 ,  0.12378834,\n",
       "        0.07347573,  0.16368593,  0.09373966,  0.10564376,  0.21958637,\n",
       "       -0.11250514, -0.09608269,  0.13167442, -0.08325607, -0.07184245,\n",
       "       -0.02978066, -0.08470738,  0.19127788,  0.19457266, -0.03510207,\n",
       "       -0.02099682, -0.04862405, -0.14117543, -0.04790575, -0.07372855,\n",
       "        0.10529654, -0.02385138,  0.01524916,  0.14234325,  0.06515182,\n",
       "       -0.03439229, -0.00954995, -0.07570472, -0.15109669, -0.02583034,\n",
       "       -0.213333  , -0.05014271,  0.1049145 ,  0.16233799,  0.0308647 ,\n",
       "        0.02549643, -0.157727  , -0.00328106, -0.06673614,  0.10244828,\n",
       "        0.13220301,  0.1977525 , -0.14430088,  0.16211155, -0.05398158,\n",
       "       -0.16460484, -0.02360076,  0.04884092,  0.16300909, -0.02028583,\n",
       "        0.13412923, -0.02871647,  0.19245613,  0.11810467,  0.05915386,\n",
       "        0.1433333 , -0.04670026,  0.03778929, -0.02738326, -0.08355177,\n",
       "        0.03617357,  0.13240467,  0.09429705, -0.02910172, -0.04146278,\n",
       "       -0.14817794,  0.03748836,  0.16505972, -0.16122498,  0.04286274,\n",
       "        0.1764238 ,  0.01374647, -0.01076427, -0.09932166,  0.02624885,\n",
       "        0.10457129, -0.06714795,  0.05469109, -0.11158726, -0.02893667,\n",
       "        0.0258835 , -0.14535412, -0.02084865,  0.12366269,  0.23023956,\n",
       "       -0.05359233,  0.02897407, -0.12164412, -0.11644156, -0.05652458,\n",
       "       -0.09706265,  0.0548278 ,  0.03988459, -0.0478976 ,  0.0565267 ,\n",
       "       -0.04952962,  0.00674148,  0.14993918, -0.13138935, -0.05532293,\n",
       "        0.25505477,  0.21359238,  0.01200306,  0.08712043,  0.14292626,\n",
       "        0.03213211,  0.038551  , -0.16425046,  0.02123144,  0.10235342,\n",
       "        0.0296388 , -0.16317123, -0.01011675], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0799 - accuracy: 0.9787\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(best_model)\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "  0%|                                                                                           | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:29<00:00,  1.17s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:27<00:00,  1.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:29<00:00,  1.17s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:29<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:29<00:00,  1.17s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:29<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:29<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:28<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:29<00:00,  1.19s/it]\n",
      " 40%|████████████████████████████████▊                                                 | 10/25 [00:11<00:17,  1.16s/it]"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_model = copy.deepcopy(old)\n",
    "to_test = 25\n",
    "for i in range(1, 65):\n",
    "    temp_model, temp_score, nodes_removed = remove_random_nodes(acc, loss, old, i, to_test, x_train, y_train)\n",
    "    if temp_score > best_score:\n",
    "        best_model = temp_model\n",
    "        best_score = temp_score\n",
    "        print(\"Found new best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a restriced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "best_weights, _, nodes_removed = remove_random_nodes(acc, loss, old, n, 50, x_train, y_train)\n",
    "\n",
    "new_weights = [np.zeros((best_weights[0].shape[0], best_weights[0].shape[1] - n)), np.zeros((best_weights[1].shape[0] - n)), np.zeros((best_weights[2].shape[0] - n, best_weights[2].shape[1])), best_weights[3]]\n",
    "\n",
    "j = 0\n",
    "for i in range(len(best_weights[1])):\n",
    "    if i not in nodes_removed:\n",
    "        new_weights[0][:, j] = best_weights[0][:, i]\n",
    "        new_weights[1][j] = best_weights[1][i]\n",
    "        new_weights[2][j, :] = best_weights[2][i, :]\n",
    "        j = j + 1\n",
    "    \n",
    "new_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128 - n, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "new_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_model(model, shrinkage_factor, x_train, y_train, size, to_test, v=0, remove='positive'):\n",
    "    \n",
    "    n = shrinkage_factor\n",
    "    loss, acc = model.evaluate(x_train, y_train, verbose=2)\n",
    "    old = model.get_weights()\n",
    "    best_weights, _, nodes_removed = remove_random_nodes(acc, loss, old, n, to_test, x_train, y_train, v, remove)\n",
    "\n",
    "    if nodes_removed.size:\n",
    "        new_weights = [np.zeros((best_weights[0].shape[0], best_weights[0].shape[1] - n)), np.zeros((best_weights[1].shape[0] - n)), np.zeros((best_weights[2].shape[0] - n, best_weights[2].shape[1])), best_weights[3]]\n",
    "\n",
    "        j = 0\n",
    "        for i in range(len(best_weights[1])):\n",
    "            if i not in nodes_removed:\n",
    "                new_weights[0][:, j] = best_weights[0][:, i]\n",
    "                new_weights[1][j] = best_weights[1][i]\n",
    "                new_weights[2][j, :] = best_weights[2][i, :]\n",
    "                j = j + 1\n",
    "\n",
    "        new_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(size - n, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    else:\n",
    "        print(\"Shrinking unsuccessful\")\n",
    "        return model, size\n",
    "    \n",
    "    new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    new_model.set_weights(new_weights)\n",
    "    print(\"Shrinking successful\")\n",
    "    return new_model, size-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "size = 128\n",
    "to_test = 25\n",
    "for _ in range(4):\n",
    "    model.fit(x_train, y_train, epochs=1)\n",
    "    model, size = shrink_model(model, 2, x_train, y_train, size, to_test)\n",
    "    print(len(model.get_weights()[1]))\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 6\n",
    "\n",
    "best_models = []\n",
    "sizes = []\n",
    "scores = []\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "start_weights = copy.deepcopy(model.get_weights())\n",
    "model.fit(x_train, y_train, epochs=7)\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"#############################\")\n",
    "best_models += [model]\n",
    "scores += [(loss, acc)]\n",
    "sizes +=[128]\n",
    "for i in range(1, 6):\n",
    "    print(f\"Starting to shrinking the model by {i}\")\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.set_weights(start_weights)\n",
    "    size = 128\n",
    "    to_test = 15\n",
    "    for _ in range(rep):\n",
    "        model.fit(x_train, y_train, epochs=1)\n",
    "        model, size = shrink_model(model, i, x_train, y_train, size, to_test, remove='always')\n",
    "    model.fit(x_train, y_train, epochs=1)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"#############################\")\n",
    "    best_models += [model]\n",
    "    scores += [(loss, acc)]\n",
    "    sizes +=[(len(model.get_weights()[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "sizes = [128, 122, 116, 110, 104, 98]\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_plain = [scores[0]]\n",
    "for i in range(1, len(scores)):\n",
    "    print(f\"Starting plain train of Dense size {sizes[i]}\")\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(sizes[i], activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=7)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    scores_plain += [(loss, acc)]\n",
    "    print(\"###############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_p = []\n",
    "sizes_p = []\n",
    "scores_p = []\n",
    "best_models_p += [model]\n",
    "scores_p += [(loss, acc)]\n",
    "sizes_p +=[128]\n",
    "for i in range(1, 6):\n",
    "    print(f\"Starting to shrinking the model by {i}\")\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.set_weights(start_weights)\n",
    "    size = 128\n",
    "    to_test = 15\n",
    "    for _ in range(rep):\n",
    "        model.fit(x_train, y_train, epochs=1)\n",
    "        model, size = shrink_model(model, i, x_train, y_train, size, to_test, remove='positive')\n",
    "    model.fit(x_train, y_train, epochs=1)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"#############################\")\n",
    "    best_models += [model]\n",
    "    scores_p += [(loss, acc)]\n",
    "    sizes_p +=[(len(model.get_weights()[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sizes_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "print(\"#############################\")\n",
    "print(scores_p)\n",
    "print(\"#############################\")\n",
    "print(scores_plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(scores)):\n",
    "    print(\"Loss change:\", (scores_plain[i][0] - scores[i][0])/scores_plain[i][0] *100, \"--- Acc change:\", -(scores_plain[i][1] - scores[i][1]) / scores_plain[i][1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_list = np.arange(1, 65)\n",
    "num_rep = 10\n",
    "loss_diff = np.zeros(num_rep)\n",
    "acc_diff = np.zeros(num_rep)\n",
    "loss_change = np.zeros(num_rep)\n",
    "acc_change = np.zeros(num_rep)\n",
    "nodes_removed_list = []\n",
    "num_nodes_removed = np.zeros(num_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_remove_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_rep):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    \n",
    "    n = np.random.choice(to_remove_list, 1)\n",
    "    \n",
    "    best_weights, _, nodes_removed = remove_random_nodes(acc, loss, model.get_weights(), n, 1, x_train, y_train, 0, remove='always')\n",
    "    \n",
    "    model.set_weights(best_weights)\n",
    "    print(n)\n",
    "    \n",
    "    loss_new, acc_new = model.evaluate(x_test, y_test, verbose=2)\n",
    "    \n",
    "    loss_diff[i] = loss - loss_new\n",
    "    acc_diff[i] = acc_new - acc\n",
    "    loss_change[i] = loss_diff[i] / loss * 100\n",
    "    acc_change[i] = acc_diff[i] / acc * 100\n",
    "    num_nodes_removed[i] = n\n",
    "    nodes_removed_list += [nodes_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 65):\n",
    "    print(f\"{i} nodes removed\")\n",
    "    print(\"Loss changes:\",loss_change[num_nodes_removed == i])\n",
    "    print(\"Accuracy changes:\",acc_change[num_nodes_removed == i])\n",
    "    print(\"#########################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
