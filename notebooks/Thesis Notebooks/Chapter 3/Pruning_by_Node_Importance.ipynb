{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tqdm\n",
    "from hfunc import models\n",
    "from hfunc import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-created functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_pruning(model, tester_model, x, y, layer_sizes, tol)\n",
    "\n",
    "    loss, acc = model.evaluate(x, y, verbose=0, batch_size=512)\n",
    "    original = model.get_weights()\n",
    "    bas = [acc]\n",
    "    bls = [loss]\n",
    "    best_weights = model.get_weights()\n",
    "    best_acc = 0\n",
    "    best_loss = 1e20\n",
    "    ol = loss\n",
    "    oa = acc\n",
    "    amounts = []\n",
    "    places = []\n",
    "\n",
    "    for layer, size in enumerate(layer_sizes):\n",
    "        end_not_reached = True\n",
    "        current_pos = 0\n",
    "        num_removed = 0\n",
    "        best_pos = -1\n",
    "        best_change = tol\n",
    "        nodes_removed = []\n",
    "        print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "        while end_not_reached or improved:\n",
    "            if not(end_not_reached):\n",
    "                end_not_reached = True\n",
    "                improved = False\n",
    "                current_pos = 0\n",
    "                size -= 1\n",
    "                layer_sizes[layer] -= 1\n",
    "                nodes_removed += [best_pos]\n",
    "                best_weights[0][...,best_pos] = 0\n",
    "                best_weights[1][best_pos] = 0\n",
    "                best_pos = -1\n",
    "                ol = best_loss\n",
    "                oa = best_acc\n",
    "                bas += [best_acc]\n",
    "                bls += [best_loss]\n",
    "                best_change = tol\n",
    "                num_removed += 1\n",
    "            if current_pos in nodes_removed:\n",
    "                current_pos += 1\n",
    "                if current_pos - num_removed >= size:\n",
    "                    end_not_reached = False\n",
    "                continue\n",
    "            w = copy.deepcopy(best_weights)\n",
    "            w[weight_len - (2*layer+1)][...,current_pos] = 0\n",
    "            w[weight_len - 2*layer][current_pos] = 0\n",
    "            tester_model.set_weights(w)\n",
    "            nl, na = tester_model.evaluate(x, y, verbose=0, batch_size=1024)\n",
    "            # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "            if 0.3*(na - oa) + 0.7*(ol - nl) >= best_change:\n",
    "                best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "                best_pos = current_pos\n",
    "                improved = True\n",
    "                best_acc = na\n",
    "                best_loss = nl\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "        amounts.append(num_removed)\n",
    "        places.append(nodes_removed)\n",
    "\n",
    "    return best_weights, bas, bls, amounts, places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 20\n",
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem)\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_mnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 20\n",
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem)\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_val_mnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_val_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_val_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 20\n",
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem)\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 20\n",
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem)\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_val_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_val_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_val_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [128]\n",
    "model, r = models.train_basic_ANN(x_train, y_train, 128, (x_test, y_test), epochs=5)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 10\n",
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_MLP_exh_pru_mnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_MLP_exh_pru_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_MLP_exh_pru_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_MLP_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_MLP_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_MLP_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_MLP_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 10\n",
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_MLP_exh_pru_val_mnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_MLP_exh_pru_val_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_MLP_exh_pru_val_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_MLP_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_MLP_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_MLP_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_MLP_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 10\n",
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_MLP_exh_pru_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_MLP_exh_pru_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_MLP_exh_pru_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_MLP_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_MLP_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_MLP_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_MLP_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 10\n",
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_MLP_exh_pru_val_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_MLP_exh_pru_val_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_MLP_exh_pru_val_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_MLP_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_MLP_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_MLP_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_MLP_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 5\n",
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 5\", \"Layer 4\", \"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_mnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_mnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 5\n",
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 5\", \"Layer 4\", \"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_val_mnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_val_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_val_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_val_mnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 5\n",
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 5\", \"Layer 4\", \"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 5\n",
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 5\", \"Layer 4\", \"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_val_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_val_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_val_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 5\n",
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 5\", \"Layer 4\", \"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_fmnist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [35:21<00:00, 42.42s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 5\n",
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "num_rem = []\n",
    "delta_accs = []\n",
    "delta_losses = []\n",
    "for trial in tqdm.trange(trials):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    shrinked_weights, _, _, tmp_a, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "    num_rem += tmp_a\n",
    "    a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "    tester_model.set_weights(shrinked_weights)\n",
    "    na, nl = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    delta_accs += [na - a]\n",
    "    delta_losses += [nl - l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = pd.Series(num_rem, columns=[\"Layer 5\", \"Layer 4\", \"Layer 3\", \"Layer 2\", \"Layer 1\"])\n",
    "NR.to_csv('../../../results/num_removed_ANN_exh_pru_val_fmnist.csv')\n",
    "NR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series(delta_accs)\n",
    "A.to_csv('../../../results/change_accuracy_ANN_exh_pru_val_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(delta_losses)\n",
    "L.to_csv('../../../results/chnage_loss_ANN_exh_pru_val_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rem = [sum(rem) for rem in num_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.scatter(x=num_rem, y=delta_accs, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in accuracy depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_accuracy_vs_nodes_removed_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_rem, y=delta_losses, marker='o', color='red', s=60)\n",
    "plt.title(\"Change in loss depending on number of nodes removed\", size=28)\n",
    "plt.xlabel(\"Number of nodes removed\", size=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../../../figures/change_loss_vs_nodes_removed_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-5\n",
    "layer_sizes = [64, 256, 128, 64, 32]\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "shrinked_weights, acc_ev, loss_ev, num_rem, _ = node_pruning(model, tester_model, x_train, y_train, layer_sizes, tol)\n",
    "a, l = model.evaluate(x_test, y_test, verbose=0)\n",
    "tester_model.set_weights(shrinked_weights)\n",
    "na, nl = tester_model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original loss: {l}, New loss: {nl}\")\n",
    "print(f\"Original accuracy: {a}, New accuracy: {na}\")\n",
    "print(f\"Number of nodes removed: {num_rem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = []\n",
    "tot_rem = 0\n",
    "for rem in num_rem:\n",
    "    tot_rem += rem\n",
    "    changes.append(tot_rem)\n",
    "colors = [\"black\", \"blue\", \"red\", \"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ev)\n",
    "plt.title(\"Evolution of accuracy as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Accuracy\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_acc_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ev)\n",
    "plt.title(\"Evolution of loss as nodes are removed\", size=28)\n",
    "plt.ylabel(\"Loss\", size=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "num_layers = len(changes)\n",
    "for i, pos in enumerate(changes[:-1]):\n",
    "    plt.axvline(x=pos, label=f\"Layer {num_layers-i} -> Layer {num_layer-(i+1)}\", color=colors[i])\n",
    "plt.legend(fontsize=20)\n",
    "plt.savefig(\"../../../figures/ev_loss_ANN_exh_pru_val_fmnist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
