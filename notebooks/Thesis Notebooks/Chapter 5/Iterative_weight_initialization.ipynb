{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import tqdm\n",
    "from hfunc import models\n",
    "from hfunc import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-created functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_node_importance(model, tester_model, layer_sizes, tol_low, tol_high, x, y, back=True, forward=True, single=False, layer=None):\n",
    "    \n",
    "    l, a = model.evaluate(x, y, verbose=0, batch_size=256)\n",
    "    or_weights = model.get_weights()\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    amounts = []\n",
    "    places = []\n",
    "    if single:\n",
    "        size = layer_sizes[layer]\n",
    "        num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "        z = []\n",
    "        wr = []\n",
    "        imp = []\n",
    "        for i in range(size):\n",
    "            w = copy.deepcopy(or_weights)\n",
    "            if back:\n",
    "                w[2*layer][...,i] = 0\n",
    "            w[2*layer+1][i] = 0\n",
    "            if forward:\n",
    "                w[2*layer+2][...,i,:] = 0\n",
    "            tester_model.set_weights(w)\n",
    "            nl, na = tester_model.evaluate(x, y, verbose=0, batch_size=256)\n",
    "            change = l - nl\n",
    "            if change <= tol_high and change >= tol_low:\n",
    "                num_zeros += 1\n",
    "                z += [i]\n",
    "            elif change > 0:\n",
    "                num_worse += 1\n",
    "                wr += [i]\n",
    "            else:\n",
    "                num_important += 1\n",
    "                imp += [i]\n",
    "        amounts.append((num_zeros, num_worse, num_important))\n",
    "        places.append((z, wr, imp))\n",
    "        \n",
    "        return amounts[0], places[0]\n",
    "    else:\n",
    "        for layer, size in enumerate(layer_sizes):\n",
    "            num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "            z = []\n",
    "            wr = []\n",
    "            imp = []\n",
    "            for i in range(size):\n",
    "                w = copy.deepcopy(or_weights)\n",
    "                if back:\n",
    "                    w[2*layer][...,i] = 0\n",
    "                w[2*layer+1][i] = 0\n",
    "                if forward:\n",
    "                    w[2*layer+2][...,i,:] = 0\n",
    "                tester_model.set_weights(w)\n",
    "                nl, na = tester_model.evaluate(x, y, verbose=0, batch_size=256)\n",
    "                change = l - nl\n",
    "                if change <= tol_high and change >= tol_low:\n",
    "                    num_zeros += 1\n",
    "                    z += [i]\n",
    "                elif change > 0:\n",
    "                    num_worse += 1\n",
    "                    wr += [i]\n",
    "                else:\n",
    "                    num_important += 1\n",
    "                    imp += [i]\n",
    "            amounts.append((num_zeros, num_worse, num_important))\n",
    "            places.append((z, wr, imp))\n",
    "    \n",
    "        return amounts, places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_weights(model, tester_model, layer_sizes, filter_sizes, tol_low, tol_high, x, y, input_size, output_size, min_imp_percentage=0.9, increasing=False, input_nodes=False):\n",
    "    \n",
    "    for l, size in enumerate(layer_sizes):\n",
    "        fil = filter_sizes[l]\n",
    "        if l < len(layer_sizes) - 1:\n",
    "            fil_out = filter_sizes[l+1]\n",
    "        else:\n",
    "            fil_out = None\n",
    "        #print(fil_out)\n",
    "        if l == 0:\n",
    "            #print('First time')\n",
    "            tmp_a, tmp_p = estimate_node_importance(model, tester_model, layer_sizes, low_tol, high_tol, x_val, y_val, layer=l, single=True)\n",
    "        else:\n",
    "            tmp_a, tmp_p = estimate_node_importance(model, tester_model, layer_sizes, low_tol, high_tol, x_val, y_val, back=False, layer=l, single=True)\n",
    "\n",
    "        imp_z_ratio = (tmp_a[2] + tmp_a[0]) / layer_sizes[l]\n",
    "        imp_ratio = (tmp_a[2]) / layer_sizes[l]\n",
    "        #print(imp_z_ratio)\n",
    "        #print(imp_ratio)\n",
    "        #print('#####################')\n",
    "        \n",
    "        if l < len(layer_sizes) - 1:\n",
    "            next_size = layer_sizes[l+1]\n",
    "        else:\n",
    "            next_size = output_size\n",
    "            \n",
    "        while imp_ratio <= min_imp_percentage:\n",
    "            \n",
    "            w = model.get_weights()\n",
    "            weight_len = len(w) - 3\n",
    "            if fil:\n",
    "                if fil_out:\n",
    "                    limit2 = np.sqrt(6 / ((size*fil*fil)+(next_size*fil_out*fil_out)))\n",
    "                else:\n",
    "                    limit2 = np.sqrt(6 / ((size*fil*fil)+(next_size)))\n",
    "            else:\n",
    "                limit2 = np.sqrt(6 / ((size)+(next_size)))\n",
    "            if l == 0:\n",
    "                if input_nodes:\n",
    "                    limit1 = np.sqrt(6 / (input_nodes+size))\n",
    "                else:\n",
    "                    limit1 = np.sqrt(6 / (input_size+size))\n",
    "                \n",
    "\n",
    "            if tmp_a[1]:\n",
    "                \n",
    "                if fil_out:\n",
    "                    size_out = (fil_out, fil_out, tmp_a[1], next_size)\n",
    "                else:\n",
    "                    size_out = (tmp_a[1], next_size)\n",
    "                \n",
    "                w[2*l+2][..., tmp_p[1], :] = list(np.random.uniform(-limit2, limit2, size_out))\n",
    "                if l == 0:\n",
    "                    if fil:\n",
    "                        size_in = (fil, fil, input_size, tmp_a[1])\n",
    "                    else:\n",
    "                        size_in = (input_size, tmp_a[1])\n",
    "                    w[2*l][..., tmp_p[1]] = list(np.random.uniform(-limit1, limit1, size_in))\n",
    "\n",
    "            if tmp_a[0]:\n",
    "                \n",
    "                if fil_out:\n",
    "                    size_out = (fil_out, fil_out, tmp_a[0], next_size)\n",
    "                else:\n",
    "                    size_out = (tmp_a[0], next_size)\n",
    "                    \n",
    "                w[2*l+2][..., tmp_p[0], :] = list(np.random.uniform(-limit2, limit2, size_out))\n",
    "                if l == 0:\n",
    "                    if fil:\n",
    "                        size_in = (fil, fil, input_size, tmp_a[0])\n",
    "                    else:\n",
    "                        size_in = (input_size, tmp_a[0])\n",
    "                    w[2*l][..., tmp_p[0]] = list(np.random.uniform(-limit1, limit1, size_in))\n",
    "            \n",
    "            model.set_weights(w)\n",
    "            if l == 0:\n",
    "                tmp_a, tmp_p = estimate_node_importance(model, tester_model, layer_sizes, low_tol, high_tol, x_val, y_val, layer=l, single=True)\n",
    "            else:\n",
    "                tmp_a, tmp_p = estimate_node_importance(model, tester_model, layer_sizes, low_tol, high_tol, x_val, y_val, back=False, layer=l, single=True)\n",
    "            imp_z_ratio = (tmp_a[2] + tmp_a[0]) / layer_sizes[l]\n",
    "            imp_ratio = (tmp_a[2]) / layer_sizes[l]\n",
    "            #print(imp_z_ratio)\n",
    "            #print(imp_ratio)\n",
    "            #print('#####################')\n",
    "            if increasing:\n",
    "                min_imp_percentage += increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                               | 1/25 [01:33<37:13, 93.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▋                                                                            | 2/25 [02:39<32:34, 84.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▉                                                                         | 3/25 [03:45<29:05, 79.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▎                                                                     | 4/25 [05:00<27:17, 77.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 5/25 [06:23<26:30, 79.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▉                                                               | 6/25 [07:54<26:19, 83.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████▏                                                           | 7/25 [09:00<23:22, 77.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▌                                                        | 8/25 [10:06<21:04, 74.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████▉                                                     | 9/25 [11:21<19:51, 74.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 10/25 [12:52<19:53, 79.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████                                              | 11/25 [14:15<18:48, 80.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████▎                                          | 12/25 [15:56<18:44, 86.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████▋                                       | 13/25 [17:27<17:35, 87.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████▉                                    | 14/25 [18:33<14:53, 81.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 15/25 [19:47<13:11, 79.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████▍                             | 16/25 [21:09<12:00, 80.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████████████▊                          | 17/25 [22:40<11:06, 83.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████                       | 18/25 [24:03<09:41, 83.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████▎                   | 19/25 [25:34<08:33, 85.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 20/25 [27:22<07:41, 92.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████▉             | 21/25 [28:36<05:47, 86.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████▏         | 22/25 [29:59<04:16, 85.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████▍      | 23/25 [31:22<02:49, 84.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████▋   | 24/25 [32:46<01:24, 84.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [34:09<00:00, 81.99s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 25\n",
    "low_tol = -1e-5\n",
    "high_tol = 1e-5\n",
    "filter_sizes = [None]\n",
    "layer_sizes = [128]\n",
    "accs_opt = np.zeros((trials))\n",
    "losses_opt = np.zeros((trials))\n",
    "accs = np.zeros((trials))\n",
    "losses = np.zeros((trials))\n",
    "for trial in tqdm.trange(trials):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    ) \n",
    "    \n",
    "    w = model.get_weights()\n",
    "    tester_model.set_weights(w)\n",
    "    tester_model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses[trial], accs[trial] = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    optimize_weights(model, tester_model, layer_sizes, filter_sizes, low_tol, high_tol, x_val, y_val, 784, 10)\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses_opt[trial], accs_opt[trial] = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unoptimized Weights</th>\n",
       "      <th>Optimized Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.975436</td>\n",
       "      <td>0.975128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.001529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.972700</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.974300</td>\n",
       "      <td>0.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.976800</td>\n",
       "      <td>0.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.977400</td>\n",
       "      <td>0.977300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unoptimized Weights  Optimized Weights\n",
       "count            25.000000          25.000000\n",
       "mean              0.975436           0.975128\n",
       "std               0.001496           0.001529\n",
       "min               0.972700           0.972400\n",
       "25%               0.974300           0.973700\n",
       "50%               0.975200           0.975600\n",
       "75%               0.976800           0.976500\n",
       "max               0.977400           0.977300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = pd.DataFrame(np.array([accs, accs_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "A.to_csv('../../../results/acc_ANN_iter_weights_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unoptimized Weights</th>\n",
       "      <th>Optimized Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.079142</td>\n",
       "      <td>0.081441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.071206</td>\n",
       "      <td>0.073149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.076419</td>\n",
       "      <td>0.076339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.078328</td>\n",
       "      <td>0.081898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.081304</td>\n",
       "      <td>0.084012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.086604</td>\n",
       "      <td>0.092926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unoptimized Weights  Optimized Weights\n",
       "count            25.000000          25.000000\n",
       "mean              0.079142           0.081441\n",
       "std               0.004107           0.005087\n",
       "min               0.071206           0.073149\n",
       "25%               0.076419           0.076339\n",
       "50%               0.078328           0.081898\n",
       "75%               0.081304           0.084012\n",
       "max               0.086604           0.092926"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = pd.DataFrame(np.array([losses, losses_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "L.to_csv('../../../results/loss_ANN_iter_weights_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [37:18<00:00, 89.54s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 25\n",
    "low_tol = -1e-5\n",
    "high_tol = 1e-5\n",
    "filter_sizes = [None]\n",
    "layer_sizes = [128]\n",
    "accs_opt = np.zeros((trials))\n",
    "losses_opt = np.zeros((trials))\n",
    "accs = np.zeros((trials))\n",
    "losses = np.zeros((trials))\n",
    "for trial in tqdm.trange(trials):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    w = model.get_weights()\n",
    "    tester_model.set_weights(w)\n",
    "    tester_model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses[trial], accs[trial] = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    optimize_weights(model, tester_model, layer_sizes, filter_sizes, low_tol, high_tol, x_val, y_val, 784, 10)\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses_opt[trial], accs_opt[trial] = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unoptimized Weights</th>\n",
       "      <th>Optimized Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.868888</td>\n",
       "      <td>0.869064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.006144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.856200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.864300</td>\n",
       "      <td>0.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.869100</td>\n",
       "      <td>0.869800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.878200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unoptimized Weights  Optimized Weights\n",
       "count            25.000000          25.000000\n",
       "mean              0.868888           0.869064\n",
       "std               0.006007           0.006144\n",
       "min               0.857100           0.856200\n",
       "25%               0.864300           0.866900\n",
       "50%               0.869100           0.869800\n",
       "75%               0.874500           0.873100\n",
       "max               0.878900           0.878200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = pd.DataFrame(np.array([accs, accs_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "A.to_csv('../../../results/acc_ANN_iter_weights_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unoptimized Weights</th>\n",
       "      <th>Optimized Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.364696</td>\n",
       "      <td>0.362768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.014066</td>\n",
       "      <td>0.016294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.342609</td>\n",
       "      <td>0.341724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.357195</td>\n",
       "      <td>0.350479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.361867</td>\n",
       "      <td>0.359648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.371032</td>\n",
       "      <td>0.366726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.402705</td>\n",
       "      <td>0.406200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unoptimized Weights  Optimized Weights\n",
       "count            25.000000          25.000000\n",
       "mean              0.364696           0.362768\n",
       "std               0.014066           0.016294\n",
       "min               0.342609           0.341724\n",
       "25%               0.357195           0.350479\n",
       "50%               0.361867           0.359648\n",
       "75%               0.371032           0.366726\n",
       "max               0.402705           0.406200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = pd.DataFrame(np.array([losses, losses_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "L.to_csv('../../../results/loss_ANN_iter_weights_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 25/25 [1:56:08<00:00, 278.75s/it]\n"
     ]
    }
   ],
   "source": [
    "trials = 25\n",
    "low_tol = -1e-5\n",
    "high_tol = 1e-5\n",
    "filter_sizes = [None, None, None]\n",
    "layer_sizes = [128, 64, 32]\n",
    "accs_opt = np.zeros((trials))\n",
    "losses_opt = np.zeros((trials))\n",
    "accs = np.zeros((trials))\n",
    "losses = np.zeros((trials))\n",
    "for trial in tqdm.trange(trials):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    w = model.get_weights()\n",
    "    tester_model.set_weights(w)\n",
    "    tester_model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses[trial], accs[trial] = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    optimize_weights(model, tester_model, layer_sizes, filter_sizes, low_tol, high_tol, x_val, y_val, 784, 10)\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses_opt[trial], accs_opt[trial] = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unoptimized Weights</th>\n",
       "      <th>Optimized Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.974096</td>\n",
       "      <td>0.973632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.968500</td>\n",
       "      <td>0.967300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.973400</td>\n",
       "      <td>0.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.974300</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.975700</td>\n",
       "      <td>0.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.977300</td>\n",
       "      <td>0.977800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unoptimized Weights  Optimized Weights\n",
       "count            25.000000          25.000000\n",
       "mean              0.974096           0.973632\n",
       "std               0.001981           0.002461\n",
       "min               0.968500           0.967300\n",
       "25%               0.973400           0.972600\n",
       "50%               0.974300           0.974100\n",
       "75%               0.975700           0.974800\n",
       "max               0.977300           0.977800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = pd.DataFrame(np.array([accs, accs_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "A.to_csv('../../../results/acc_MLP_iter_weights_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unoptimized Weights</th>\n",
       "      <th>Optimized Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.090750</td>\n",
       "      <td>0.090004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.009228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.077841</td>\n",
       "      <td>0.076328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.086862</td>\n",
       "      <td>0.084064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.090640</td>\n",
       "      <td>0.089402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.095472</td>\n",
       "      <td>0.093296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.108661</td>\n",
       "      <td>0.117765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unoptimized Weights  Optimized Weights\n",
       "count            25.000000          25.000000\n",
       "mean              0.090750           0.090004\n",
       "std               0.007703           0.009228\n",
       "min               0.077841           0.076328\n",
       "25%               0.086862           0.084064\n",
       "50%               0.090640           0.089402\n",
       "75%               0.095472           0.093296\n",
       "max               0.108661           0.117765"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = pd.DataFrame(np.array([losses, losses_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "L.to_csv('../../../results/loss_MLP_iter_weights_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████▊   | 24/25 [3:32:03<07:41, 461.68s/it]"
     ]
    }
   ],
   "source": [
    "trials = 25\n",
    "low_tol = -1e-5\n",
    "high_tol = 1e-5\n",
    "filter_sizes = [None, None, None]\n",
    "layer_sizes = [128, 64, 32]\n",
    "accs_opt = np.zeros((trials))\n",
    "losses_opt = np.zeros((trials))\n",
    "accs = np.zeros((trials))\n",
    "losses = np.zeros((trials))\n",
    "for trial in tqdm.trange(trials):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    w = model.get_weights()\n",
    "    tester_model.set_weights(w)\n",
    "    tester_model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses[trial], accs[trial] = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    optimize_weights(model, tester_model, layer_sizes, filter_sizes, low_tol, high_tol, x_val, y_val, 784, 10)\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses_opt[trial], accs_opt[trial] = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(np.array([accs, accs_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "A.to_csv('../../../results/acc_MLP_iter_weights_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.DataFrame(np.array([losses, losses_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "L.to_csv('../../../results/loss_MLP_iter_weights_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 10\n",
    "low_tol = -1e-5\n",
    "high_tol = 1e-5\n",
    "filter_sizes = [3, 3, 3, 3, None]\n",
    "layer_sizes = [32, 64, 128, 256, 64]\n",
    "accs_opt = np.zeros((trials))\n",
    "losses_opt = np.zeros((trials))\n",
    "accs = np.zeros((trials))\n",
    "losses = np.zeros((trials))\n",
    "for trial in tqdm.trange(trials):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    w = model.get_weights()\n",
    "    tester_model.set_weights(w)\n",
    "    tester_model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses[trial], accs[trial] = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    optimize_weights(model, tester_model, layer_sizes, filter_sizes, low_tol, high_tol, x_val, y_val, 1, 10, increasing=False, min_imp_percentage=0.6, input_nodes=784)\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses_opt[trial], accs_opt[trial] = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(np.array([accs, accs_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "A.to_csv('../../../results/acc_CNN_iter_weights_mnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.DataFrame(np.array([losses, losses_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "L.to_csv('../../../results/loss_CNN_iter_weights_mnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fmnist.load_data()\n",
    "x_train, x_test = x_train[..., np.newaxis], x_test[..., np.newaxis]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 10\n",
    "low_tol = -1e-5\n",
    "high_tol = 1e-5\n",
    "filter_sizes = [3, 3, 3, 3, None]\n",
    "layer_sizes = [32, 64, 128, 256, 64]\n",
    "accs_opt = np.zeros((trials))\n",
    "losses_opt = np.zeros((trials))\n",
    "accs = np.zeros((trials))\n",
    "losses = np.zeros((trials))\n",
    "for trial in tqdm.trange(trials):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    w = model.get_weights()\n",
    "    tester_model.set_weights(w)\n",
    "    tester_model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses[trial], accs[trial] = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    optimize_weights(model, tester_model, layer_sizes, filter_sizes, low_tol, high_tol, x_val, y_val, 1, 10, increasing=False, min_imp_percentage=0.6, input_nodes=784)\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses_opt[trial], accs_opt[trial] = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(np.array([accs, accs_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "A.to_csv('../../../results/acc_CNN_iter_weights_fmnist.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.DataFrame(np.array([losses, losses_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "L.to_csv('../../../results/loss_CNN_iter_weights_fmnist.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.85, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 10\n",
    "low_tol = -1e-5\n",
    "high_tol = 1e-5\n",
    "filter_sizes = [3, 3, 3, 3, None]\n",
    "layer_sizes = [32, 64, 128, 256, 64]\n",
    "accs_opt = np.zeros((trials))\n",
    "losses_opt = np.zeros((trials))\n",
    "accs = np.zeros((trials))\n",
    "losses = np.zeros((trials))\n",
    "for trial in tqdm.trange(trials):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    w = model.get_weights()\n",
    "    tester_model.set_weights(w)\n",
    "    tester_model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses[trial], accs[trial] = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    optimize_weights(model, tester_model, layer_sizes, filter_sizes, low_tol, high_tol, x_val, y_val, 3, 10, increasing=False, min_imp_percentage=0.6, input_nodes=3072)\n",
    "    \n",
    "    model.fit(x_train, y_train, verbose=0, epochs=5)\n",
    "    losses_opt[trial], accs_opt[trial] = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(np.array([accs, accs_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "A.to_csv('../../../results/acc_CNN_iter_weights_cifar.csv')\n",
    "A.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.DataFrame(np.array([losses, losses_opt]).T, columns=['Unoptimized Weights', 'Optimized Weights'])\n",
    "L.to_csv('../../../results/loss_CNN_iter_weights_cifar.csv')\n",
    "L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
