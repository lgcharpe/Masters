{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to remove from multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import tqdm\n",
    "import IProgress\n",
    "from hfunc import models\n",
    "from hfunc import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "class_accuracy = metrics.ClassAccuracy()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "y_train, y_test = tf.one_hot(y_train, 10), tf.one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "    \n",
    "tester_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "ow = np.array(model.get_weights())\n",
    "print(ow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4953 - accuracy: 0.8233 - auc: 0.9850\n",
      "Epoch 2/5\n",
      "1103/1875 [================>.............] - ETA: 2s - loss: 0.3729 - accuracy: 0.8637 - auc: 0.9908"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f799c55326df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \"\"\"\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    517\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \"\"\"\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 0.3920 - accuracy: 0.8633 - auc_2: 0.9891\n",
      "Node 0: -0.004817008972167969\n",
      "Node 1: -0.0034227371215820312\n",
      "Node 2: -9.521842002868652e-05\n",
      "Node 3: -0.0028116703033447266\n",
      "Node 4: -0.002624303102493286\n",
      "Node 5: 0.0\n",
      "Node 6: -7.203221321105957e-05\n",
      "Node 7: -0.003041297197341919\n",
      "Node 8: 0.003934323787689209\n",
      "Node 9: -0.02047094702720642\n",
      "Node 10: 9.429454803466797e-05\n",
      "Node 11: -0.004074901342391968\n",
      "Node 12: -0.014137089252471924\n",
      "Node 13: -0.017557114362716675\n",
      "Node 14: -0.00905466079711914\n",
      "Node 15: 2.327561378479004e-05\n",
      "Node 16: 0.011436671018600464\n",
      "Node 17: 0.0008215010166168213\n",
      "Node 18: 0.002939879894256592\n",
      "Node 19: -0.009530246257781982\n",
      "Node 20: -0.00716128945350647\n",
      "Node 21: -0.009292006492614746\n",
      "Node 22: -0.009544789791107178\n",
      "Node 23: -0.0009729862213134766\n",
      "Node 24: 0.0014873147010803223\n",
      "Node 25: -0.006831735372543335\n",
      "Node 26: -0.0012098848819732666\n",
      "Node 27: -0.003622770309448242\n",
      "Node 28: -0.006294578313827515\n",
      "Node 29: -0.003550410270690918\n",
      "Node 30: -0.001171410083770752\n",
      "Node 31: 0.009568721055984497\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-74f1124dfb40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweight_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtester_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mnl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtester_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Node {i}:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1055\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     dataset = dataset.map(\n\u001b[1;32m--> 397\u001b[1;33m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1626\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m           preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4018\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4019\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4020\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   4021\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4022\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"default\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3219\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3220\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3221\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2530\u001b[0m     \"\"\"\n\u001b[0;32m   2531\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 2532\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2534\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2497\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2671\u001b[0m         \u001b[1;31m# places (like Keras) where the FuncGraph lives longer than the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2672\u001b[0m         \u001b[1;31m# ConcreteFunction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2673\u001b[1;33m         shared_func_graph=False)\n\u001b[0m\u001b[0;32m   2674\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, signature, shared_func_graph)\u001b[0m\n\u001b[0;32m   1563\u001b[0m     \u001b[1;31m# FuncGraph directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[1;32m-> 1565\u001b[1;33m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[0;32m   1566\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[0;32m    652\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[0;32m    653\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[0;32m    655\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# control_output_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m         compat.as_str(\"\"))\n\u001b[0m\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l, a, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights = model.get_weights()\n",
    "weight_len = len(or_weights) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "layer_sizes = [64, 128]\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 12\n",
      "Worse Nodes: 40\n",
      "Important Nodes: 76\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 3\n",
      "Worse Nodes: 20\n",
      "Important Nodes: 41\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (12, 40, 76)\n",
      "1 (3, 20, 41)\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(reversed(amounts)):\n",
    "    print(i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 - 1s - loss: 0.3098 - accuracy: 0.8856 - auc_2: 0.9934\n",
      "Considering layer 2\n",
      "2.0861625671386717e-08\n",
      "Found something better\n",
      "0.0023130923509597774\n",
      "Found something better\n",
      "0.007125061750411986\n",
      "Found something better\n",
      "0.016171675920486447\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8918833136558533 --- Loss: 0.289421409368515 --- Change: 0.016171675920486447 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0019133180379867552\n",
      "Found something better\n",
      "0.0040796220302581785\n",
      "Found something better\n",
      "0.006046676635742187\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8943166732788086 --- Loss: 0.2818261682987213 --- Change: 0.006046676635742187 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0012735426425933838\n",
      "Found something better\n",
      "0.003202742338180542\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8964999914169312 --- Loss: 0.27818652987480164 --- Change: 0.003202742338180542 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0009826242923736573\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8971333503723145 --- Loss: 0.27705422043800354 --- Change: 0.0009826242923736573 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.000812336802482605\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8978333473205566 --- Loss: 0.2761937379837036 --- Change: 0.000812336802482605 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "2.0861625671386717e-08\n",
      "Found something better\n",
      "0.0006796360015869139\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8981999754905701 --- Loss: 0.2753799557685852 --- Change: 0.0006796360015869139 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "6.237626075744629e-06\n",
      "Found something better\n",
      "0.00032717883586883543\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8986333608627319 --- Loss: 0.2750982940196991 --- Change: 0.00032717883586883543 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "6.240606307983399e-06\n",
      "Found something better\n",
      "0.00011421442031860352\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8988666534423828 --- Loss: 0.2750351130962372 --- Change: 0.00011421442031860352 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "6.06924295425415e-05\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8989499807357788 --- Loss: 0.27498412132263184 --- Change: 6.06924295425415e-05 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "4.1723251342773435e-08\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8989499807357788 --- Loss: 0.27498406171798706 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8989499807357788 --- Loss: 0.27498406171798706 --- Change: 0.0 --- New tol: -1e-05\n",
      "Considering layer 1\n",
      "0.0\n",
      "Found something better\n",
      "6.849169731140136e-05\n",
      "Found something better\n",
      "7.58141279220581e-05\n",
      "Found something better\n",
      "0.00015327930450439453\n",
      "Found something better\n",
      "0.001385781168937683\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.899649977684021 --- Loss: 0.27330437302589417 --- Change: 0.001385781168937683 --- New tol: -1e-05\n",
      "7.76916742324829e-05\n",
      "Found something better\n",
      "0.0005418241024017334\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9003833532333374 --- Loss: 0.2728446424007416 --- Change: 0.0005418241024017334 --- New tol: -1e-05\n",
      "8.24153423309326e-05\n",
      "Found something better\n",
      "0.0002634733915328979\n",
      "Found something better\n",
      "0.0004230648279190063\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9008333086967468 --- Loss: 0.2724331021308899 --- Change: 0.0004230648279190063 --- New tol: -1e-05\n",
      "8.134841918945312e-05\n",
      "Found something better\n",
      "0.0002220809459686279\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9013000130653381 --- Loss: 0.2723158597946167 --- Change: 0.0002220809459686279 --- New tol: -1e-05\n",
      "9.384751319885254e-05\n",
      "Found something better\n",
      "0.000122109055519104\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017000198364258 --- Loss: 0.27231284976005554 --- Change: 0.000122109055519104 --- New tol: -1e-05\n",
      "9.441077709197997e-05\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9018666744232178 --- Loss: 0.27224940061569214 --- Change: 9.441077709197997e-05 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "3.173351287841797e-05\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9018833041191101 --- Loss: 0.2722111940383911 --- Change: 3.173351287841797e-05 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "2.253055572509766e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9018999934196472 --- Loss: 0.2722151279449463 --- Change: 2.253055572509766e-06 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "2.5570392608642565e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 2.5570392608642565e-06 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216148376464844 --- Change: 0.0 --- New tol: -1e-05\n",
      "-2.148747444152832e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27216455340385437 --- Change: -2.148747444152832e-06 --- New tol: -1e-05\n",
      "-8.031725883483887e-06\n",
      "Found something better\n",
      "-3.5047531127929686e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.2721695601940155 --- Change: -3.5047531127929686e-06 --- New tol: -1e-05\n",
      "-8.094310760498046e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.9017833471298218 --- Loss: 0.27218112349510193 --- Change: -8.094310760498046e-06 --- New tol: -1e-05\n",
      "313/313 - 1s - loss: 0.3400 - accuracy: 0.8771 - auc_3: 0.9922\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model.evaluate(x_train, y_train, verbose=2, batch_size=512)\n",
    "original2 = model.get_weights()\n",
    "tol = -1e-5\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights2 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    improved = False\n",
    "    current_pos = 0\n",
    "    best_pos = -1\n",
    "    best_change = tol\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached or improved:\n",
    "        if not(end_not_reached):\n",
    "            end_not_reached = True\n",
    "            improved = False\n",
    "            current_pos = 0\n",
    "            layer_sizes[layer] -= 1\n",
    "            size -= 1\n",
    "            nodes_removed2 += [best_pos]\n",
    "            best_weights2[weight_len - (2*layer+1)][:,best_pos] = 0\n",
    "            best_weights2[weight_len - 2*layer][best_pos] = 0\n",
    "            best_pos = -1\n",
    "            #tol -= best_change\n",
    "            ol = best_loss\n",
    "            oa = best_acc\n",
    "            bas2 += [best_acc]\n",
    "            bls2 += [best_loss]\n",
    "            print(\"Improvement has occured!! Accuracy:\", best_acc, \"--- Loss:\", best_loss, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            best_change = tol\n",
    "            num_removed2 += 1\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights2)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_train, y_train, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) > best_change:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            print(best_change)\n",
    "            best_pos = current_pos\n",
    "            improved = True\n",
    "            best_acc = na\n",
    "            best_loss = nl\n",
    "            print(\"Found something better\")\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            end_not_reached = False\n",
    "        if current_pos%200 == 0:\n",
    "            print(\"Did 200 iterations\")\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "tester_model.set_weights(best_weights2)\n",
    "loss2, acc2, auc2 = tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_flat = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([9 2 1 ... 8 1 5], shape=(10000,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.803, 0.986, 0.791, 0.745, 0.864, 0.94, 0.614, 0.951, 0.973, 0.966]\n"
     ]
    }
   ],
   "source": [
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.803, 0.968, 0.788, 0.896, 0.805, 0.949, 0.679, 0.957, 0.969, 0.957]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model2.set_weights(best_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 23]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 - 1s - loss: 0.2722 - accuracy: 0.9018 - auc_5: 0.9955\n",
      "Node 0: -0.0007068216800689697\n",
      "Node 1: -0.00951436161994934\n",
      "Node 2: -0.00010100007057189941\n",
      "Node 3: -0.0030525028705596924\n",
      "Node 4: -0.004624217748641968\n",
      "Node 5: 0.0\n",
      "Node 6: -0.0004836618900299072\n",
      "Node 7: -0.000919640064239502\n",
      "Node 8: 0.0\n",
      "Node 9: -0.014550328254699707\n",
      "Node 10: 0.0\n",
      "Node 11: -0.0030814409255981445\n",
      "Node 12: -0.006567031145095825\n",
      "Node 13: -0.004613637924194336\n",
      "Node 14: -0.0046923160552978516\n",
      "Node 15: -0.007797300815582275\n",
      "Node 16: 0.0\n",
      "Node 17: -0.003049790859222412\n",
      "Node 18: -0.008292704820632935\n",
      "Node 19: -0.0058487653732299805\n",
      "Node 20: -0.008133083581924438\n",
      "Node 21: -0.002508878707885742\n",
      "Node 22: -0.002284616231918335\n",
      "Node 23: -0.0005620419979095459\n",
      "Node 24: -0.0030630826950073242\n",
      "Node 25: -0.008069068193435669\n",
      "Node 26: -0.0029172301292419434\n",
      "Node 27: -0.002832561731338501\n",
      "Node 28: -0.004408299922943115\n",
      "Node 29: -0.0020445585250854492\n",
      "Node 30: -0.0010837316513061523\n",
      "Node 31: 0.0\n",
      "Node 32: -0.0050369203090667725\n",
      "Node 33: -0.010226279497146606\n",
      "Node 34: -0.001095801591873169\n",
      "Node 35: -0.002631634473800659\n",
      "Node 36: 0.0\n",
      "Node 37: -0.006977885961532593\n",
      "Node 38: -0.0002899765968322754\n",
      "Node 39: -0.010617971420288086\n",
      "Node 40: -0.00847393274307251\n",
      "Node 41: -0.0277215838432312\n",
      "Node 42: -0.0077793896198272705\n",
      "Node 43: 0.0\n",
      "Node 44: -0.0015724897384643555\n",
      "Node 45: -0.006340712308883667\n",
      "Node 46: -0.0033908188343048096\n",
      "Node 47: 0.0\n",
      "Node 48: -0.0011396408081054688\n",
      "Node 49: -0.03037133812904358\n",
      "Node 50: 0.0\n",
      "Node 51: -0.0019733309745788574\n",
      "Node 52: -0.0033124685287475586\n",
      "Node 53: -0.0044435858726501465\n",
      "Node 54: -0.0006185770034790039\n",
      "Node 55: -0.007940083742141724\n",
      "Node 56: 0.0\n",
      "Node 57: -0.003460109233856201\n",
      "Node 58: -0.00012022256851196289\n",
      "Node 59: -0.0014114975929260254\n",
      "Node 60: 0.0\n",
      "Node 61: -0.005777627229690552\n",
      "Node 62: -0.0011318325996398926\n",
      "Node 63: -0.0023572146892547607\n",
      "Node 0: -0.002093285322189331\n",
      "Node 1: 0.0\n",
      "Node 2: -0.001801908016204834\n",
      "Node 3: 0.0\n",
      "Node 4: -0.0012855231761932373\n",
      "Node 5: -0.004794001579284668\n",
      "Node 6: 0.0\n",
      "Node 7: -0.0020651817321777344\n",
      "Node 8: -0.003465712070465088\n",
      "Node 9: -0.02636474370956421\n",
      "Node 10: -0.0009329915046691895\n",
      "Node 11: -9.691715240478516e-05\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: -0.0011001229286193848\n",
      "Node 15: 0.0\n",
      "Node 16: -0.0019852519035339355\n",
      "Node 17: -0.013453304767608643\n",
      "Node 18: -0.00044402480125427246\n",
      "Node 19: 0.0\n",
      "Node 20: -0.0024059414863586426\n",
      "Node 21: -0.0001628100872039795\n",
      "Node 22: -0.007989734411239624\n",
      "Node 23: -0.0004158914089202881\n",
      "Node 24: 0.0\n",
      "Node 25: -0.0006421804428100586\n",
      "Node 26: 0.0\n",
      "Node 27: -0.0014461874961853027\n",
      "Node 28: -0.0011182129383087158\n",
      "Node 29: -0.009727180004119873\n",
      "Node 30: -0.002246379852294922\n",
      "Node 31: 0.0\n",
      "Node 32: -0.008841156959533691\n",
      "Node 33: -0.0005868971347808838\n",
      "Node 34: -0.004530787467956543\n",
      "Node 35: -7.4803829193115234e-06\n",
      "Node 36: 0.0\n",
      "Node 37: -0.0006826221942901611\n",
      "Node 38: -0.0015968084335327148\n",
      "Node 39: 0.0\n",
      "Node 40: -0.008526384830474854\n",
      "Node 41: -0.007239699363708496\n",
      "Node 42: -0.005503922700881958\n",
      "Node 43: 0.0\n",
      "Node 44: -0.0052120983600616455\n",
      "Node 45: -0.0005647540092468262\n",
      "Node 46: -0.004614502191543579\n",
      "Node 47: -0.0008571743965148926\n",
      "Node 48: 0.0\n",
      "Node 49: -8.505582809448242e-05\n",
      "Node 50: -0.0006643831729888916\n",
      "Node 51: -0.0015224814414978027\n",
      "Node 52: -0.0013710260391235352\n",
      "Node 53: -0.005246251821517944\n",
      "Node 54: -0.007490247488021851\n",
      "Node 55: -8.502602577209473e-05\n",
      "Node 56: -0.01147797703742981\n",
      "Node 57: -0.05534762144088745\n",
      "Node 58: 0.0\n",
      "Node 59: -0.0001468062400817871\n",
      "Node 60: -0.0035600662231445312\n",
      "Node 61: -0.007852554321289062\n",
      "Node 62: -0.0009981989860534668\n",
      "Node 63: -0.00138777494430542\n",
      "Node 64: -0.00818970799446106\n",
      "Node 65: -0.001548677682876587\n",
      "Node 66: -0.005624979734420776\n",
      "Node 67: -0.0017738938331604004\n",
      "Node 68: -0.00010117888450622559\n",
      "Node 69: -0.0013065636157989502\n",
      "Node 70: -0.0013165771961212158\n",
      "Node 71: 0.0\n",
      "Node 72: -3.287196159362793e-05\n",
      "Node 73: -0.0015665888786315918\n",
      "Node 74: -0.0033193230628967285\n",
      "Node 75: 0.0\n",
      "Node 76: -0.0009383261203765869\n",
      "Node 77: -0.0016218721866607666\n",
      "Node 78: -0.0022690892219543457\n",
      "Node 79: 0.0\n",
      "Node 80: -0.002125859260559082\n",
      "Node 81: -0.000645756721496582\n",
      "Node 82: -4.875659942626953e-05\n",
      "Node 83: 0.0\n",
      "Node 84: -0.0019897520542144775\n",
      "Node 85: -6.732344627380371e-05\n",
      "Node 86: -0.007034897804260254\n",
      "Node 87: -0.0020604729652404785\n",
      "Node 88: -0.0009419023990631104\n",
      "Node 89: 0.0\n",
      "Node 90: -0.00011715292930603027\n",
      "Node 91: 0.0\n",
      "Node 92: -0.0037602484226226807\n",
      "Node 93: -0.0008892714977264404\n",
      "Node 94: -0.001242130994796753\n",
      "Node 95: -0.0008288323879241943\n",
      "Node 96: -0.0008812844753265381\n",
      "Node 97: -0.0004464089870452881\n",
      "Node 98: -0.0016115307807922363\n",
      "Node 99: -0.00519561767578125\n",
      "Node 100: -0.004729181528091431\n",
      "Node 101: -0.00019723176956176758\n",
      "Node 102: -6.514787673950195e-05\n",
      "Node 103: -0.006949782371520996\n",
      "Node 104: -0.0008153617382049561\n",
      "Node 105: -0.006127595901489258\n",
      "Node 106: -0.0005711019039154053\n",
      "Node 107: -0.0028333663940429688\n",
      "Node 108: -0.0030369162559509277\n",
      "Node 109: -0.002617955207824707\n",
      "Node 110: -3.4928321838378906e-05\n",
      "Node 111: -0.0031246840953826904\n",
      "Node 112: -0.0036884844303131104\n",
      "Node 113: -0.008500009775161743\n",
      "Node 114: -0.0016534626483917236\n",
      "Node 115: 0.0\n",
      "Node 116: -0.0054028332233428955\n",
      "Node 117: -0.0015372037887573242\n",
      "Node 118: -0.0018575489521026611\n",
      "Node 119: -0.010031729936599731\n",
      "Node 120: -0.0003674328327178955\n",
      "Node 121: -0.0006220042705535889\n",
      "Node 122: -0.005650550127029419\n",
      "Node 123: -0.002957552671432495\n",
      "Node 124: -0.0010370314121246338\n",
      "Node 125: -0.008864641189575195\n",
      "Node 126: 0.0\n",
      "Node 127: -0.002480030059814453\n"
     ]
    }
   ],
   "source": [
    "l3, a3, auc3 = model2.evaluate(x_train, y_train, verbose=2, batch_size=256)\n",
    "or_weights2 = model2.get_weights()\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts2 = []\n",
    "places2 = []\n",
    "layer_sizes = [64, 128]\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights2)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_train, y_train, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a3) + 1.*(l3 - nl))\n",
    "        change = l3 - nl\n",
    "        if not i in places[layer]:\n",
    "            if change <= tol_high and change >= tol_low:\n",
    "                num_zeros += 1\n",
    "                z += [i]\n",
    "            elif change > 0:\n",
    "                num_worse += 1\n",
    "                wr += [i]\n",
    "            else:\n",
    "                num_important += 1\n",
    "                imp += [i]\n",
    "    amounts2.append((num_zeros, num_worse, num_important))\n",
    "    places2.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 1\n",
      "Worse Nodes: 0\n",
      "Important Nodes: 104\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 0\n",
      "Worse Nodes: 0\n",
      "Important Nodes: 53\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts2)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model.set_weights(original2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 - 0s - loss: 0.3098 - accuracy: 0.8856 - auc_7: 0.9934\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.8855500221252441 --- Loss: 0.30980950593948364 --- Change: 2.0861625671386717e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8860999941825867 --- Loss: 0.30674082040786743 --- Change: 0.002313071489334106 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8886666893959045 --- Loss: 0.2979861795902252 --- Change: 0.00689825713634491 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8897500038146973 --- Loss: 0.2968910336494446 --- Change: 0.001091596484184265 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8911666870117188 --- Loss: 0.293424516916275 --- Change: 0.002851566672325134 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8911666870117188 --- Loss: 0.293424516916275 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8916833400726318 --- Loss: 0.2924406826496124 --- Change: 0.000843679904937744 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8917666673660278 --- Loss: 0.2924532890319824 --- Change: 1.6173720359802247e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8924833536148071 --- Loss: 0.29188740253448486 --- Change: 0.0006111264228820801 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8962833285331726 --- Loss: 0.2851272225379944 --- Change: 0.005872118473052978 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8965333104133606 --- Loss: 0.28500834107398987 --- Change: 0.0001582115888595581 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.897266685962677 --- Loss: 0.283364474773407 --- Change: 0.0013707190752029418 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.8973333239555359 --- Loss: 0.28333574533462524 --- Change: 4.010200500488281e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8975833058357239 --- Loss: 0.2834426462650299 --- Change: 1.6391277313233777e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8975833058357239 --- Loss: 0.2834426462650299 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8975833058357239 --- Loss: 0.2834426462650299 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8975833058357239 --- Loss: 0.2834426462650299 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8985166549682617 --- Loss: 0.2818472683429718 --- Change: 0.0013967692852020262 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8984833359718323 --- Loss: 0.28180280327796936 --- Change: 2.1129846572875973e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8984833359718323 --- Loss: 0.2818056046962738 --- Change: -1.9609928131103515e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8984833359718323 --- Loss: 0.28181692957878113 --- Change: -7.927417755126953e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8984833359718323 --- Loss: 0.28182509541511536 --- Change: -5.7160854339599604e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8984833359718323 --- Loss: 0.28182509541511536 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8984833359718323 --- Loss: 0.281825989484787 --- Change: -6.258487701416016e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.899150013923645 --- Loss: 0.27973708510398865 --- Change: 0.001662236452102661 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.899150013923645 --- Loss: 0.27973708510398865 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8992000222206116 --- Loss: 0.27974992990493774 --- Change: 6.011128425598144e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8992000222206116 --- Loss: 0.27974992990493774 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8992000222206116 --- Loss: 0.27974992990493774 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8992000222206116 --- Loss: 0.27974992990493774 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.899316668510437 --- Loss: 0.27939078211784363 --- Change: 0.00028639733791351317 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8989999890327454 --- Loss: 0.2792088985443115 --- Change: 3.231465816497802e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8989999890327454 --- Loss: 0.27922067046165466 --- Change: -8.240342140197754e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8989999890327454 --- Loss: 0.27922067046165466 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8989999890327454 --- Loss: 0.27922067046165466 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8989999890327454 --- Loss: 0.27922067046165466 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "313/313 - 1s - loss: 0.3465 - accuracy: 0.8751 - auc_3: 0.9920\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model.evaluate(x_train, y_train, verbose=2, batch_size=512)\n",
    "original2 = model.get_weights()\n",
    "tol = -1e-5\n",
    "layer_sizes = [64, 128]\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights4 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts3 = []\n",
    "places3 = []\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights4)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_train, y_train, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights4[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights4[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts3.append(num_removed2)\n",
    "    places3.append(nodes_removed2)\n",
    "\n",
    "tester_model.set_weights(best_weights4)\n",
    "loss2, acc2, auc2 = tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "model3.set_weights(best_weights4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 8, 16, 18, 31, 36, 43, 10, 47, 50, 56, 60],\n",
       " [1,\n",
       "  3,\n",
       "  6,\n",
       "  12,\n",
       "  13,\n",
       "  15,\n",
       "  24,\n",
       "  26,\n",
       "  31,\n",
       "  35,\n",
       "  36,\n",
       "  39,\n",
       "  43,\n",
       "  48,\n",
       "  49,\n",
       "  58,\n",
       "  71,\n",
       "  75,\n",
       "  79,\n",
       "  81,\n",
       "  83,\n",
       "  91,\n",
       "  115,\n",
       "  126]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 - 1s - loss: 0.2792 - accuracy: 0.8990 - auc_9: 0.9952\n",
      "Node 0: -0.00027801692485809323\n",
      "Node 1: -0.010227012634277343\n",
      "Node 2: -9.000003337860108e-05\n",
      "Node 3: -0.0030906170606613157\n",
      "Node 4: -0.003475725650787353\n",
      "Node 5: 0.0\n",
      "Node 6: -0.0002203613519668579\n",
      "Node 7: -0.0007068723440170287\n",
      "Node 8: 0.0\n",
      "Node 9: -0.009521567821502683\n",
      "Node 10: 0.0\n",
      "Node 11: -0.001864698529243469\n",
      "Node 12: -0.0046282678842544545\n",
      "Node 13: -0.002602782845497131\n",
      "Node 14: -0.006760761141777038\n",
      "Node 15: -0.004790139198303222\n",
      "Node 16: 0.0\n",
      "Node 17: -0.0018842697143554687\n",
      "Node 18: 0.0\n",
      "Node 19: -0.00422491431236267\n",
      "Node 20: -0.005683809518814087\n",
      "Node 21: -0.0019903302192687987\n",
      "Node 22: -0.0012815326452255249\n",
      "Node 23: -0.00035728812217712403\n",
      "Node 24: -0.0016927331686019897\n",
      "Node 25: -0.005865535140037537\n",
      "Node 26: -0.002036064863204956\n",
      "Node 27: -0.002338472008705139\n",
      "Node 28: -0.005013236403465271\n",
      "Node 29: -0.0013986796140670777\n",
      "Node 30: -0.0008497655391693115\n",
      "Node 31: 0.0\n",
      "Node 32: -0.008186012506484985\n",
      "Node 33: -0.014611533284187316\n",
      "Node 34: -0.0010993987321853637\n",
      "Node 35: -0.0018965065479278564\n",
      "Node 36: 0.0\n",
      "Node 37: -0.006458371877670288\n",
      "Node 38: -0.00021642446517944336\n",
      "Node 39: -0.00651860237121582\n",
      "Node 40: -0.005833667516708374\n",
      "Node 41: -0.029944866895675656\n",
      "Node 42: -0.004714885354042053\n",
      "Node 43: 0.0\n",
      "Node 44: -0.0008773088455200195\n",
      "Node 45: -0.004094362258911133\n",
      "Node 46: -0.0047270268201828\n",
      "Node 47: 0.0\n",
      "Node 48: -0.0010097384452819823\n",
      "Node 49: -0.02553000748157501\n",
      "Node 50: 0.0\n",
      "Node 51: -0.0017854124307632444\n",
      "Node 52: -0.00511881411075592\n",
      "Node 53: -0.002877762913703918\n",
      "Node 54: -0.0003953754901885986\n",
      "Node 55: -0.006634193658828735\n",
      "Node 56: 0.0\n",
      "Node 57: -0.003309443593025207\n",
      "Node 58: -0.00011843442916870117\n",
      "Node 59: -0.002651301026344299\n",
      "Node 60: 0.0\n",
      "Node 61: -0.006334525346755981\n",
      "Node 62: -0.0015619486570358275\n",
      "Node 63: -0.0013624608516693113\n",
      "Node 0: -0.001695999503135681\n",
      "Node 1: 0.0\n",
      "Node 2: -0.0017124772071838378\n",
      "Node 3: 0.0\n",
      "Node 4: -0.0007826924324035644\n",
      "Node 5: -0.002855125069618225\n",
      "Node 6: 0.0\n",
      "Node 7: -0.003456491231918335\n",
      "Node 8: -0.0028037905693054197\n",
      "Node 9: -0.024004867672920226\n",
      "Node 10: -0.00043256282806396483\n",
      "Node 11: -9.759664535522461e-05\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: -0.000739648938179016\n",
      "Node 15: 0.0\n",
      "Node 16: -0.0012206524610519408\n",
      "Node 17: -0.009168028831481932\n",
      "Node 18: -0.0003145992755889892\n",
      "Node 19: -0.0004034310579299926\n",
      "Node 20: -0.0014701664447784423\n",
      "Node 21: -0.00014232099056243894\n",
      "Node 22: -0.010439738631248472\n",
      "Node 23: -0.0002735823392868042\n",
      "Node 24: 0.0\n",
      "Node 25: -0.00048109292984008787\n",
      "Node 26: 0.0\n",
      "Node 27: -0.0013679534196853638\n",
      "Node 28: -0.0006447523832321167\n",
      "Node 29: -0.010444936156272887\n",
      "Node 30: -0.0016872644424438476\n",
      "Node 31: 0.0\n",
      "Node 32: -0.009001502394676208\n",
      "Node 33: -0.00039757192134857176\n",
      "Node 34: -0.0031653612852096556\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: -0.0005186975002288818\n",
      "Node 38: -0.0011570155620574951\n",
      "Node 39: 0.0\n",
      "Node 40: -0.00973435640335083\n",
      "Node 41: -0.007210037112236022\n",
      "Node 42: -0.0043200522661209105\n",
      "Node 43: 0.0\n",
      "Node 44: -0.006744340062141418\n",
      "Node 45: -0.0002828538417816162\n",
      "Node 46: -0.002667012810707092\n",
      "Node 47: -0.0005390316247940064\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: -0.0004306942224502563\n",
      "Node 51: -0.0013786852359771726\n",
      "Node 52: -0.0009105592966079711\n",
      "Node 53: -0.003941363096237183\n",
      "Node 54: -0.005514881014823913\n",
      "Node 55: -0.000109100341796875\n",
      "Node 56: -0.012400043010711669\n",
      "Node 57: -0.038611647486686704\n",
      "Node 58: 0.0\n",
      "Node 59: -8.787214756011963e-05\n",
      "Node 60: -0.0036601483821868896\n",
      "Node 61: -0.00475485622882843\n",
      "Node 62: -0.0009294688701629638\n",
      "Node 63: -0.0014800548553466797\n",
      "Node 64: -0.010429975390434264\n",
      "Node 65: -0.0004487723112106323\n",
      "Node 66: -0.005805739760398864\n",
      "Node 67: -0.0009590238332748412\n",
      "Node 68: -1.059472560882568e-05\n",
      "Node 69: -0.0007821619510650634\n",
      "Node 70: -0.0011170923709869385\n",
      "Node 71: 0.0\n",
      "Node 72: -2.1466612815856932e-05\n",
      "Node 73: -0.002610054612159729\n",
      "Node 74: -0.0015681833028793335\n",
      "Node 75: 0.0\n",
      "Node 76: -0.0010599434375762938\n",
      "Node 77: -0.0007184058427810668\n",
      "Node 78: -0.0021017432212829586\n",
      "Node 79: 0.0\n",
      "Node 80: -0.001966005563735962\n",
      "Node 81: 0.0\n",
      "Node 82: -4.829764366149902e-05\n",
      "Node 83: 0.0\n",
      "Node 84: -0.0013653337955474851\n",
      "Node 85: -5.453228950500488e-05\n",
      "Node 86: -0.009472715854644775\n",
      "Node 87: -0.0008695751428604126\n",
      "Node 88: -0.0008442878723144531\n",
      "Node 89: -0.0007593631744384765\n",
      "Node 90: -9.880363941192626e-05\n",
      "Node 91: 0.0\n",
      "Node 92: -0.0026643365621566773\n",
      "Node 93: -0.0007680475711822509\n",
      "Node 94: -0.0007736653089523316\n",
      "Node 95: -0.0007636964321136474\n",
      "Node 96: -0.0005293846130371093\n",
      "Node 97: -0.0003963619470596313\n",
      "Node 98: -0.0014177560806274414\n",
      "Node 99: -0.0037756174802780146\n",
      "Node 100: -0.0038599282503128047\n",
      "Node 101: -0.00021567940711975098\n",
      "Node 102: -9.717941284179688e-05\n",
      "Node 103: -0.0066743075847625725\n",
      "Node 104: -0.0009331375360488891\n",
      "Node 105: -0.004348284006118774\n",
      "Node 106: -0.0004980266094207764\n",
      "Node 107: -0.0023888349533081055\n",
      "Node 108: -0.0020867973566055294\n",
      "Node 109: -0.0015649139881134032\n",
      "Node 110: -2.949833869934082e-05\n",
      "Node 111: -0.0033016651868820186\n",
      "Node 112: -0.0030546277761459347\n",
      "Node 113: -0.011951208114624022\n",
      "Node 114: -0.0014477014541625977\n",
      "Node 115: 0.0\n",
      "Node 116: -0.0035998612642288205\n",
      "Node 117: -0.0015317022800445557\n",
      "Node 118: -0.0014434158802032469\n",
      "Node 119: -0.01140851378440857\n",
      "Node 120: -0.0001998305320739746\n",
      "Node 121: -0.00042473375797271725\n",
      "Node 122: -0.003180435299873352\n",
      "Node 123: -0.0027137249708175655\n",
      "Node 124: -0.0009856104850769043\n",
      "Node 125: -0.008137202262878417\n",
      "Node 126: 0.0\n",
      "Node 127: -0.002280265092849731\n"
     ]
    }
   ],
   "source": [
    "l4, a4, auc4 = model3.evaluate(x_train, y_train, verbose=2, batch_size=256)\n",
    "or_weights3 = model3.get_weights()\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts4 = []\n",
    "places4 = []\n",
    "layer_sizes = [64, 128]\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights3)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_train, y_train, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.3*(na - a4) + 0.7*(l4 - nl))\n",
    "        change = l4 - nl\n",
    "        if not i in places[layer]:\n",
    "            if change <= tol_high and change >= tol_low:\n",
    "                num_zeros += 1\n",
    "                z += [i]\n",
    "            elif change > 0:\n",
    "                num_worse += 1\n",
    "                wr += [i]\n",
    "            else:\n",
    "                num_important += 1\n",
    "                imp += [i]\n",
    "    amounts4.append((num_zeros, num_worse, num_important))\n",
    "    places4.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 3\n",
      "Worse Nodes: 0\n",
      "Important Nodes: 102\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 1\n",
      "Worse Nodes: 0\n",
      "Important Nodes: 52\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts4)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.803, 0.968, 0.788, 0.896, 0.805, 0.949, 0.679, 0.957, 0.969, 0.957]\n"
     ]
    }
   ],
   "source": [
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.804, 0.959, 0.769, 0.882, 0.797, 0.955, 0.698, 0.953, 0.972, 0.959]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "    \n",
    "tester_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5026 - accuracy: 0.8207 - auc_2: 0.9846\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3689 - accuracy: 0.8644 - auc_2: 0.9911\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3334 - accuracy: 0.8767 - auc_2: 0.9925\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3151 - accuracy: 0.8819 - auc_2: 0.9934\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2954 - accuracy: 0.8900 - auc_2: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x128631f1388>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 0.3587 - accuracy: 0.8697 - auc_2: 0.9905\n",
      "Node 0: 0.005494028329849243\n",
      "Node 1: -0.0005543828010559082\n",
      "Node 2: -0.02857455611228943\n",
      "Node 3: -0.011651873588562012\n",
      "Node 4: -0.02465301752090454\n",
      "Node 5: -0.01774802803993225\n",
      "Node 6: -0.013079166412353516\n",
      "Node 7: 0.0017140507698059082\n",
      "Node 8: -2.9802322387695312e-08\n",
      "Node 9: -0.0050334632396698\n",
      "Node 10: 0.00021898746490478516\n",
      "Node 11: -0.008084803819656372\n",
      "Node 12: -0.03289198875427246\n",
      "Node 13: -0.03555145859718323\n",
      "Node 14: -0.005437731742858887\n",
      "Node 15: -0.02062740921974182\n",
      "Node 16: -0.00033780932426452637\n",
      "Node 17: -0.014653593301773071\n",
      "Node 18: -0.006222337484359741\n",
      "Node 19: -0.008284389972686768\n",
      "Node 20: -0.010763972997665405\n",
      "Node 21: 0.0006979405879974365\n",
      "Node 22: 0.00496634840965271\n",
      "Node 23: -0.018248677253723145\n",
      "Node 24: -0.027724236249923706\n",
      "Node 25: -0.0037917792797088623\n",
      "Node 26: -0.0014519691467285156\n",
      "Node 27: -0.007471948862075806\n",
      "Node 28: 0.0005661845207214355\n",
      "Node 29: -0.0015793442726135254\n",
      "Node 30: -0.0025542378425598145\n",
      "Node 31: -0.010666966438293457\n",
      "Node 0: -0.0006230175495147705\n",
      "Node 1: 0.000814974308013916\n",
      "Node 2: 0.0005907416343688965\n",
      "Node 3: -0.004765540361404419\n",
      "Node 4: 0.0007687211036682129\n",
      "Node 5: -0.0004298090934753418\n",
      "Node 6: 3.4570693969726562e-06\n",
      "Node 7: -0.0011479854583740234\n",
      "Node 8: -0.00029528141021728516\n",
      "Node 9: -0.0009639859199523926\n",
      "Node 10: 0.0068367719650268555\n",
      "Node 11: 0.0006163716316223145\n",
      "Node 12: -0.008022934198379517\n",
      "Node 13: -0.004890859127044678\n",
      "Node 14: 0.0016066431999206543\n",
      "Node 15: -0.008206337690353394\n",
      "Node 16: -0.0005822181701660156\n",
      "Node 17: -0.00020897388458251953\n",
      "Node 18: -0.002690255641937256\n",
      "Node 19: -0.0006708502769470215\n",
      "Node 20: -0.007884472608566284\n",
      "Node 21: -0.001736074686050415\n",
      "Node 22: 0.002046644687652588\n",
      "Node 23: 0.004838764667510986\n",
      "Node 24: -0.0005353987216949463\n",
      "Node 25: -0.004087328910827637\n",
      "Node 26: -0.0005684494972229004\n",
      "Node 27: -0.0046006739139556885\n",
      "Node 28: -0.012455731630325317\n",
      "Node 29: -0.0007897019386291504\n",
      "Node 30: 0.0003739595413208008\n",
      "Node 31: 0.0031211674213409424\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0021035373210906982\n",
      "Node 34: -0.002235978841781616\n",
      "Node 35: -0.045962125062942505\n",
      "Node 36: -0.0001118779182434082\n",
      "Node 37: 0.001604706048965454\n",
      "Node 38: 0.0009569525718688965\n",
      "Node 39: -0.0024560093879699707\n",
      "Node 40: -0.004440635442733765\n",
      "Node 41: -0.006200671195983887\n",
      "Node 42: -0.004328429698944092\n",
      "Node 43: -0.003068983554840088\n",
      "Node 44: 0.0014105737209320068\n",
      "Node 45: -0.005059719085693359\n",
      "Node 46: -0.0012913048267364502\n",
      "Node 47: -0.0027966201305389404\n",
      "Node 48: 0.0009352564811706543\n",
      "Node 49: -9.524822235107422e-05\n",
      "Node 50: 0.0069717466831207275\n",
      "Node 51: -0.002659231424331665\n",
      "Node 52: -0.0011546909809112549\n",
      "Node 53: -0.012311697006225586\n",
      "Node 54: -0.009524434804916382\n",
      "Node 55: -0.008382469415664673\n",
      "Node 56: -0.0019497871398925781\n",
      "Node 57: 0.0003827512264251709\n",
      "Node 58: -0.00196230411529541\n",
      "Node 59: -0.000716865062713623\n",
      "Node 60: -0.032732486724853516\n",
      "Node 61: 0.00480988621711731\n",
      "Node 62: -0.00016170740127563477\n",
      "Node 63: 0.00014075636863708496\n",
      "Node 0: -0.004242151975631714\n",
      "Node 1: -0.005415678024291992\n",
      "Node 2: -0.0006408095359802246\n",
      "Node 3: -0.0004050731658935547\n",
      "Node 4: 0.0026723742485046387\n",
      "Node 5: -0.00037488341331481934\n",
      "Node 6: 0.0038583576679229736\n",
      "Node 7: -0.003592759370803833\n",
      "Node 8: -0.0025048553943634033\n",
      "Node 9: 0.0\n",
      "Node 10: -0.0016422271728515625\n",
      "Node 11: -0.010745227336883545\n",
      "Node 12: -0.0011737644672393799\n",
      "Node 13: -0.014904946088790894\n",
      "Node 14: -0.0008452832698822021\n",
      "Node 15: 0.005199402570724487\n",
      "Node 16: 0.0015239119529724121\n",
      "Node 17: -0.0041030943393707275\n",
      "Node 18: 0.00014650821685791016\n",
      "Node 19: -0.002564162015914917\n",
      "Node 20: -0.005926579236984253\n",
      "Node 21: -0.0012207627296447754\n",
      "Node 22: 0.0\n",
      "Node 23: -7.957220077514648e-05\n",
      "Node 24: -0.0001685023307800293\n",
      "Node 25: -3.2335519790649414e-05\n",
      "Node 26: -0.000420302152633667\n",
      "Node 27: -0.0037182271480560303\n",
      "Node 28: -0.001054525375366211\n",
      "Node 29: 0.00014293193817138672\n",
      "Node 30: -9.819865226745605e-05\n",
      "Node 31: -0.00043067336082458496\n",
      "Node 32: 4.4673681259155273e-05\n",
      "Node 33: 0.0024715662002563477\n",
      "Node 34: -1.6361474990844727e-05\n",
      "Node 35: -0.0010549724102020264\n",
      "Node 36: 0.000387340784072876\n",
      "Node 37: -0.00265425443649292\n",
      "Node 38: 1.519918441772461e-05\n",
      "Node 39: -0.00351637601852417\n",
      "Node 40: -0.00019827485084533691\n",
      "Node 41: -2.115964889526367e-06\n",
      "Node 42: -0.0010396242141723633\n",
      "Node 43: 0.008718341588973999\n",
      "Node 44: 0.0009963512420654297\n",
      "Node 45: -0.004759609699249268\n",
      "Node 46: -0.0013796091079711914\n",
      "Node 47: -0.018492966890335083\n",
      "Node 48: -0.006518065929412842\n",
      "Node 49: -0.00040221214294433594\n",
      "Node 50: -3.018975257873535e-05\n",
      "Node 51: -0.0006731152534484863\n",
      "Node 52: 0.0005091428756713867\n",
      "Node 53: -0.002705216407775879\n",
      "Node 54: -8.502602577209473e-05\n",
      "Node 55: 0.001468569040298462\n",
      "Node 56: -0.0001525580883026123\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0008802115917205811\n",
      "Node 59: -0.004238814115524292\n",
      "Node 60: 0.0\n",
      "Node 61: 0.00017756223678588867\n",
      "Node 62: -0.008583247661590576\n",
      "Node 63: 0.000326305627822876\n",
      "Node 64: 0.0\n",
      "Node 65: -0.0005246400833129883\n",
      "Node 66: -0.0029987096786499023\n",
      "Node 67: -0.0014815032482147217\n",
      "Node 68: -1.1593103408813477e-05\n",
      "Node 69: -0.00016385316848754883\n",
      "Node 70: -0.0018647313117980957\n",
      "Node 71: 0.00040218234062194824\n",
      "Node 72: -0.0033213794231414795\n",
      "Node 73: -0.010937511920928955\n",
      "Node 74: -0.0010472536087036133\n",
      "Node 75: 0.00127333402633667\n",
      "Node 76: -0.001086413860321045\n",
      "Node 77: -8.463859558105469e-06\n",
      "Node 78: -0.00021088123321533203\n",
      "Node 79: -0.0033198893070220947\n",
      "Node 80: -0.0018480122089385986\n",
      "Node 81: -0.00038686394691467285\n",
      "Node 82: -0.02061524987220764\n",
      "Node 83: -9.53972339630127e-05\n",
      "Node 84: 0.004420757293701172\n",
      "Node 85: 0.0\n",
      "Node 86: 0.007380038499832153\n",
      "Node 87: -0.00040838122367858887\n",
      "Node 88: -5.1349401473999023e-05\n",
      "Node 89: 0.0006479918956756592\n",
      "Node 90: -5.030632019042969e-05\n",
      "Node 91: 0.00029531121253967285\n",
      "Node 92: -0.010342240333557129\n",
      "Node 93: -0.004878610372543335\n",
      "Node 94: 0.0033368468284606934\n",
      "Node 95: -0.004802346229553223\n",
      "Node 96: -0.008483916521072388\n",
      "Node 97: -0.00052604079246521\n",
      "Node 98: -0.00012767314910888672\n",
      "Node 99: 0.0033065080642700195\n",
      "Node 100: -0.0015461146831512451\n",
      "Node 101: -0.000361710786819458\n",
      "Node 102: -9.420514106750488e-05\n",
      "Node 103: 0.0007578432559967041\n",
      "Node 104: 0.0\n",
      "Node 105: -0.010690093040466309\n",
      "Node 106: -0.00200536847114563\n",
      "Node 107: 0.0\n",
      "Node 108: -0.0038337111473083496\n",
      "Node 109: -0.00758206844329834\n",
      "Node 110: 0.0\n",
      "Node 111: -0.002747058868408203\n",
      "Node 112: 0.00011137127876281738\n",
      "Node 113: 0.0061227381229400635\n",
      "Node 114: -0.007703185081481934\n",
      "Node 115: 0.00031894445419311523\n",
      "Node 116: 0.0005790293216705322\n",
      "Node 117: -0.0023036301136016846\n",
      "Node 118: 0.0012797713279724121\n",
      "Node 119: -0.0010563135147094727\n",
      "Node 120: -0.0013242065906524658\n",
      "Node 121: -0.0052928626537323\n",
      "Node 122: 1.963973045349121e-05\n",
      "Node 123: -0.0014942288398742676\n",
      "Node 124: -0.00089263916015625\n",
      "Node 125: -0.005652636289596558\n",
      "Node 126: 0.00019675493240356445\n",
      "Node 127: -0.002284497022628784\n"
     ]
    }
   ],
   "source": [
    "l, a, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights = model.get_weights()\n",
    "weight_len = len(or_weights) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "layer_sizes = [32, 64, 128]\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 11\n",
      "Worse Nodes: 33\n",
      "Important Nodes: 84\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 2\n",
      "Worse Nodes: 19\n",
      "Important Nodes: 43\n",
      "######### LAYER 2 #########\n",
      "Zero Nodes: 1\n",
      "Worse Nodes: 6\n",
      "Important Nodes: 25\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 - 0s - loss: 0.2792 - accuracy: 0.8946 - auc_2: 0.9946\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.8958666920661926 --- Loss: 0.27641811966896057 --- Change: 0.0023313969373703 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8958666920661926 --- Loss: 0.27642154693603516 --- Change: -2.3990869522094727e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.895883321762085 --- Loss: 0.2762441337108612 --- Change: 0.0001291781663894653 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8981833457946777 --- Loss: 0.2741703391075134 --- Change: 0.002141663432121277 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.8981999754905701 --- Loss: 0.27417001128196716 --- Change: 5.218386650085449e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.8994666934013367 --- Loss: 0.2740398347377777 --- Change: 0.00047113895416259763 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9003166556358337 --- Loss: 0.2716923952102661 --- Change: 0.0018981963396072386 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9005166888237 --- Loss: 0.27117523550987244 --- Change: 0.000422021746635437 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.901116669178009 --- Loss: 0.2698098123073578 --- Change: 0.0011357903480529784 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.901116669178009 --- Loss: 0.2698098123073578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9019666910171509 --- Loss: 0.2684048116207123 --- Change: 0.001238507032394409 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9022499918937683 --- Loss: 0.26826900243759155 --- Change: 0.00018005669116973874 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.9022499918937683 --- Loss: 0.26827266812324524 --- Change: -2.5659799575805662e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9028666615486145 --- Loss: 0.2678391635417938 --- Change: 0.0004884541034698486 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9028666615486145 --- Loss: 0.2678391635417938 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9028666615486145 --- Loss: 0.267849326133728 --- Change: -7.113814353942871e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9028833508491516 --- Loss: 0.2678694427013397 --- Change: -9.074807167053221e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9028833508491516 --- Loss: 0.2678694427013397 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9028833508491516 --- Loss: 0.2678694427013397 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9028833508491516 --- Loss: 0.2678694427013397 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9032999873161316 --- Loss: 0.2672174572944641 --- Change: 0.0005813807249069214 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9032999873161316 --- Loss: 0.2672174572944641 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9030666947364807 --- Loss: 0.2651543617248535 --- Change: 0.0013741791248321532 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9030500054359436 --- Loss: 0.2651612162590027 --- Change: -9.804964065551758e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9032999873161316 --- Loss: 0.2651299238204956 --- Change: 9.689927101135254e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9032999873161316 --- Loss: 0.2651299238204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9032999873161316 --- Loss: 0.2651299238204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9032999873161316 --- Loss: 0.26513057947158813 --- Change: -4.5895576477050777e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.9040666818618774 --- Loss: 0.2641812860965729 --- Change: 0.000894513726234436 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "313/313 - 1s - loss: 0.3321 - accuracy: 0.8781 - auc_3: 0.9927\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model.evaluate(x_train, y_train, verbose=2, batch_size=512)\n",
    "original2 = model.get_weights()\n",
    "tol = -1e-5\n",
    "layer_sizes = [32, 64, 128]\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights4 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts3 = []\n",
    "places3 = []\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights4)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_train, y_train, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights4[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights4[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts3.append(num_removed2)\n",
    "    places3.append(nodes_removed2)\n",
    "\n",
    "tester_model.set_weights(best_weights4)\n",
    "loss2, acc2, auc2 = tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 4s - loss: 0.2642 - accuracy: 0.9041 - auc_3: 0.9958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2641812562942505, 0.9040666818618774, 0.995792806148529]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester_model.evaluate(x_train, y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73, 0.965, 0.776, 0.907, 0.811, 0.888, 0.732, 0.966, 0.967, 0.955]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.797, 0.968, 0.777, 0.892, 0.84, 0.953, 0.679, 0.956, 0.965, 0.954]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 data - 3 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10\n",
    "class_accuracy = metrics.ClassAccuracy()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "y_train, y_test = tf.one_hot(y_train.flatten(), 10), tf.one_hot(y_test.flatten(), 10)\n",
    "y_test_flat = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "    \n",
    "tester_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0611 - accuracy: 0.2287 - auc_4: 0.7066\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.7864 - accuracy: 0.3545 - auc_4: 0.8051\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.7018 - accuracy: 0.3863 - auc_4: 0.8264\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.6400 - accuracy: 0.4099 - auc_4: 0.8403\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5874 - accuracy: 0.4295 - auc_4: 0.8513\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5505 - accuracy: 0.4434 - auc_4: 0.8590\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5182 - accuracy: 0.4566 - auc_4: 0.8653\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4895 - accuracy: 0.4649 - auc_4: 0.8708\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4711 - accuracy: 0.4725 - auc_4: 0.8741\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4504 - accuracy: 0.4809 - auc_4: 0.8779\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4340 - accuracy: 0.4878 - auc_4: 0.8808\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4167 - accuracy: 0.4926 - auc_4: 0.8839\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3993 - accuracy: 0.5007 - auc_4: 0.8869\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3833 - accuracy: 0.5060 - auc_4: 0.8897\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3672 - accuracy: 0.5100 - auc_4: 0.8923\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3533 - accuracy: 0.5161 - auc_4: 0.8945\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3389 - accuracy: 0.5205 - auc_4: 0.8969\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3296 - accuracy: 0.5226 - auc_4: 0.8985\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3145 - accuracy: 0.5294 - auc_4: 0.9008\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3026 - accuracy: 0.5349 - auc_4: 0.9027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x213d124e2c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 1.4547 - accuracy: 0.4852 - auc_4: 0.8793\n",
      "Node 0: 0.001023411750793457\n",
      "Node 1: -5.8650970458984375e-05\n",
      "Node 2: 2.658367156982422e-05\n",
      "Node 3: -0.37429845333099365\n",
      "Node 4: -0.4857388734817505\n",
      "Node 5: -0.3562051057815552\n",
      "Node 6: -0.02086508274078369\n",
      "Node 7: 0.00011420249938964844\n",
      "Node 8: -0.5088027715682983\n",
      "Node 9: 0.0\n",
      "Node 10: -0.29511559009552\n",
      "Node 11: 0.0007669925689697266\n",
      "Node 12: -0.062075018882751465\n",
      "Node 13: -0.018933534622192383\n",
      "Node 14: 0.0\n",
      "Node 15: -0.1759035587310791\n",
      "Node 0: -0.14026224613189697\n",
      "Node 1: 9.167194366455078e-05\n",
      "Node 2: -0.09598207473754883\n",
      "Node 3: -0.02030313014984131\n",
      "Node 4: -0.00032591819763183594\n",
      "Node 5: -0.019550204277038574\n",
      "Node 6: 0.0010339021682739258\n",
      "Node 7: -0.003211498260498047\n",
      "Node 8: -3.790855407714844e-05\n",
      "Node 9: 5.960464477539062e-07\n",
      "Node 10: -0.05502617359161377\n",
      "Node 11: -0.47560954093933105\n",
      "Node 12: 0.0\n",
      "Node 13: -0.061805009841918945\n",
      "Node 14: 0.00011658668518066406\n",
      "Node 15: -0.023238182067871094\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0002957582473754883\n",
      "Node 18: 0.0\n",
      "Node 19: -0.18632030487060547\n",
      "Node 20: 0.0007597208023071289\n",
      "Node 21: -0.04869961738586426\n",
      "Node 22: -3.325939178466797e-05\n",
      "Node 23: 0.0\n",
      "Node 24: -0.0021704435348510742\n",
      "Node 25: -0.00011205673217773438\n",
      "Node 26: 0.0\n",
      "Node 27: -0.0008124113082885742\n",
      "Node 28: -0.0004800558090209961\n",
      "Node 29: 0.006164193153381348\n",
      "Node 30: -0.10850894451141357\n",
      "Node 31: -3.540515899658203e-05\n",
      "Node 0: -0.01029348373413086\n",
      "Node 1: -0.010695934295654297\n",
      "Node 2: 0.0005183219909667969\n",
      "Node 3: 0.006629467010498047\n",
      "Node 4: 0.00017917156219482422\n",
      "Node 5: 0.0010036230087280273\n",
      "Node 6: -0.04571545124053955\n",
      "Node 7: 0.004874706268310547\n",
      "Node 8: -0.022914528846740723\n",
      "Node 9: -0.004799365997314453\n",
      "Node 10: -0.004372596740722656\n",
      "Node 11: -0.01724398136138916\n",
      "Node 12: 0.0006091594696044922\n",
      "Node 13: 0.0\n",
      "Node 14: -0.0011330842971801758\n",
      "Node 15: 0.0\n",
      "Node 16: -0.0003349781036376953\n",
      "Node 17: 0.0002645254135131836\n",
      "Node 18: -0.014690995216369629\n",
      "Node 19: 0.00015628337860107422\n",
      "Node 20: 0.0\n",
      "Node 21: 0.004401206970214844\n",
      "Node 22: -0.0010334253311157227\n",
      "Node 23: -0.00569462776184082\n",
      "Node 24: -0.03568840026855469\n",
      "Node 25: -0.0013327598571777344\n",
      "Node 26: -0.0006390810012817383\n",
      "Node 27: 0.0019311904907226562\n",
      "Node 28: -0.005591034889221191\n",
      "Node 29: -0.004105687141418457\n",
      "Node 30: -4.7206878662109375e-05\n",
      "Node 31: -0.000751495361328125\n",
      "Node 32: 0.0\n",
      "Node 33: -0.028091788291931152\n",
      "Node 34: -0.006221771240234375\n",
      "Node 35: -0.00021457672119140625\n",
      "Node 36: -0.020807862281799316\n",
      "Node 37: 0.009669780731201172\n",
      "Node 38: 0.00035631656646728516\n",
      "Node 39: -0.01934635639190674\n",
      "Node 40: -0.0071103572845458984\n",
      "Node 41: 0.0019986629486083984\n",
      "Node 42: -0.002164006233215332\n",
      "Node 43: -0.0008471012115478516\n",
      "Node 44: -0.007180452346801758\n",
      "Node 45: -0.00022327899932861328\n",
      "Node 46: -0.000225067138671875\n",
      "Node 47: -0.0010474920272827148\n",
      "Node 48: -0.002660512924194336\n",
      "Node 49: -0.0016491413116455078\n",
      "Node 50: -0.027614355087280273\n",
      "Node 51: -0.0002512931823730469\n",
      "Node 52: 0.0004767179489135742\n",
      "Node 53: -0.011944413185119629\n",
      "Node 54: -0.0024214982986450195\n",
      "Node 55: -0.0132826566696167\n",
      "Node 56: -0.007588505744934082\n",
      "Node 57: 0.0005400180816650391\n",
      "Node 58: -0.027600526809692383\n",
      "Node 59: 0.00012254714965820312\n",
      "Node 60: 0.00026988983154296875\n",
      "Node 61: -0.0015048980712890625\n",
      "Node 62: -0.00033402442932128906\n",
      "Node 63: -0.0009380578994750977\n",
      "Node 0: -0.05060863494873047\n",
      "Node 1: -0.013368487358093262\n",
      "Node 2: -0.007341146469116211\n",
      "Node 3: 0.0\n",
      "Node 4: -0.019667625427246094\n",
      "Node 5: 0.00016200542449951172\n",
      "Node 6: 0.00613248348236084\n",
      "Node 7: -0.004290938377380371\n",
      "Node 8: 0.0\n",
      "Node 9: -0.008418798446655273\n",
      "Node 10: 0.00010406970977783203\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: -0.0007202625274658203\n",
      "Node 14: -0.0018640756607055664\n",
      "Node 15: -0.004122018814086914\n",
      "Node 16: 0.0\n",
      "Node 17: -0.026286602020263672\n",
      "Node 18: -0.012257099151611328\n",
      "Node 19: 0.006950736045837402\n",
      "Node 20: -0.0010890960693359375\n",
      "Node 21: -0.0018351078033447266\n",
      "Node 22: -0.017318367958068848\n",
      "Node 23: -0.0021898746490478516\n",
      "Node 24: -0.008098602294921875\n",
      "Node 25: 0.0\n",
      "Node 26: -0.0022336244583129883\n",
      "Node 27: 0.0\n",
      "Node 28: 0.0008983612060546875\n",
      "Node 29: -0.003537893295288086\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: -0.006255626678466797\n",
      "Node 33: -0.005442380905151367\n",
      "Node 34: -0.0005123615264892578\n",
      "Node 35: 0.0\n",
      "Node 36: -0.03637874126434326\n",
      "Node 37: 0.00042057037353515625\n",
      "Node 38: 0.0\n",
      "Node 39: -0.0006402730941772461\n",
      "Node 40: 0.0009386539459228516\n",
      "Node 41: -0.0006711483001708984\n",
      "Node 42: 0.0\n",
      "Node 43: -0.0006688833236694336\n",
      "Node 44: 0.0\n",
      "Node 45: 0.0026301145553588867\n",
      "Node 46: -0.0012590885162353516\n",
      "Node 47: 0.0011135339736938477\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0023069381713867188\n",
      "Node 50: -0.009117603302001953\n",
      "Node 51: -0.001135110855102539\n",
      "Node 52: -0.0013409852981567383\n",
      "Node 53: -0.0007655620574951172\n",
      "Node 54: -0.0016143321990966797\n",
      "Node 55: 0.00341033935546875\n",
      "Node 56: -0.01030123233795166\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0\n",
      "Node 59: -0.0029670000076293945\n",
      "Node 60: 0.0\n",
      "Node 61: -0.0035756826400756836\n",
      "Node 62: -0.0028504133224487305\n",
      "Node 63: -0.015921473503112793\n",
      "Node 64: -0.00482022762298584\n",
      "Node 65: 0.0019701719284057617\n",
      "Node 66: -0.004695773124694824\n",
      "Node 67: 0.0\n",
      "Node 68: 0.0\n",
      "Node 69: 0.007837772369384766\n",
      "Node 70: -0.004021763801574707\n",
      "Node 71: -0.004464983940124512\n",
      "Node 72: 0.0\n",
      "Node 73: 0.0017033815383911133\n",
      "Node 74: -0.03478550910949707\n",
      "Node 75: -0.0029467344284057617\n",
      "Node 76: -0.00020706653594970703\n",
      "Node 77: 0.0\n",
      "Node 78: -0.014441490173339844\n",
      "Node 79: 0.0\n",
      "Node 80: -0.0021439790725708008\n",
      "Node 81: 0.009225964546203613\n",
      "Node 82: 1.1086463928222656e-05\n",
      "Node 83: -0.016967177391052246\n",
      "Node 84: -5.960464477539062e-07\n",
      "Node 85: -0.0010367631912231445\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0006445646286010742\n",
      "Node 88: -0.00013685226440429688\n",
      "Node 89: -0.0011571645736694336\n",
      "Node 90: 0.0\n",
      "Node 91: -0.0001882314682006836\n",
      "Node 92: -0.0004118680953979492\n",
      "Node 93: -0.015479803085327148\n",
      "Node 94: -0.005038857460021973\n",
      "Node 95: -0.003722667694091797\n",
      "Node 96: -0.000559687614440918\n",
      "Node 97: -0.0018222332000732422\n",
      "Node 98: -0.0024073123931884766\n",
      "Node 99: -0.0036301612854003906\n",
      "Node 100: 0.00020074844360351562\n",
      "Node 101: -0.00022971630096435547\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: -0.0019203424453735352\n",
      "Node 106: -0.04403829574584961\n",
      "Node 107: -0.003388524055480957\n",
      "Node 108: -0.004007458686828613\n",
      "Node 109: -0.04402732849121094\n",
      "Node 110: -0.002354741096496582\n",
      "Node 111: 0.0007939338684082031\n",
      "Node 112: -0.0005502700805664062\n",
      "Node 113: -0.006596684455871582\n",
      "Node 114: -0.0021740198135375977\n",
      "Node 115: -0.00493621826171875\n",
      "Node 116: 0.0\n",
      "Node 117: -0.0009046792984008789\n",
      "Node 118: 0.0\n",
      "Node 119: -0.015691757202148438\n",
      "Node 120: -0.007631421089172363\n",
      "Node 121: 5.602836608886719e-05\n",
      "Node 122: -0.00099945068359375\n",
      "Node 123: 0.0\n",
      "Node 124: -5.5789947509765625e-05\n",
      "Node 125: -0.01097571849822998\n",
      "Node 126: -0.0016251802444458008\n",
      "Node 127: 0.0004030466079711914\n",
      "Node 0: -3.2901763916015625e-05\n",
      "Node 1: 0.0\n",
      "Node 2: 6.318092346191406e-06\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: -0.06943261623382568\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 5.960464477539062e-07\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: -0.00761723518371582\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: 0.0\n",
      "Node 29: 0.0\n",
      "Node 30: -0.01646137237548828\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.0\n",
      "Node 38: -0.018315792083740234\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: -0.06959998607635498\n",
      "Node 45: 0.0\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: -0.005213737487792969\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: -0.055292606353759766\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0\n",
      "Node 59: 0.0\n",
      "Node 60: 0.0\n",
      "Node 61: -0.0008503198623657227\n",
      "Node 62: 0.0\n",
      "Node 63: -0.024993300437927246\n",
      "Node 64: -0.028795361518859863\n",
      "Node 65: 0.0\n",
      "Node 66: 0.0\n",
      "Node 67: -0.20509779453277588\n",
      "Node 68: 0.0\n",
      "Node 69: -1.3113021850585938e-06\n",
      "Node 70: 0.0\n",
      "Node 71: 0.0\n",
      "Node 72: -0.021297335624694824\n",
      "Node 73: -0.09897780418395996\n",
      "Node 74: -0.04372084140777588\n",
      "Node 75: 0.0\n",
      "Node 76: -0.022266149520874023\n",
      "Node 77: -0.0002499818801879883\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0\n",
      "Node 80: 0.0\n",
      "Node 81: 0.0\n",
      "Node 82: 0.0\n",
      "Node 83: 0.0\n",
      "Node 84: 0.0\n",
      "Node 85: -0.0006543397903442383\n",
      "Node 86: -0.03849136829376221\n",
      "Node 87: 0.0\n",
      "Node 88: -0.020520687103271484\n",
      "Node 89: 0.0\n",
      "Node 90: 0.0\n",
      "Node 91: 0.0\n",
      "Node 92: 0.0\n",
      "Node 93: 0.0\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: -0.000990748405456543\n",
      "Node 97: -0.05693256855010986\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: 0.0\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0\n",
      "Node 108: 0.0\n",
      "Node 109: -0.09901845455169678\n",
      "Node 110: 0.0\n",
      "Node 111: 0.0\n",
      "Node 112: 0.0009453296661376953\n",
      "Node 113: 0.0\n",
      "Node 114: 0.0\n",
      "Node 115: 0.0\n",
      "Node 116: 0.0\n",
      "Node 117: -0.053637146949768066\n",
      "Node 118: -0.10432493686676025\n",
      "Node 119: 0.0\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: 0.0\n",
      "Node 123: -0.0257265567779541\n",
      "Node 124: 0.0\n",
      "Node 125: -0.06928789615631104\n",
      "Node 126: -0.0037692785263061523\n",
      "Node 127: 0.0\n",
      "Node 128: 0.0\n",
      "Node 129: 0.0\n",
      "Node 130: 0.0\n",
      "Node 131: -0.008732318878173828\n",
      "Node 132: 0.0\n",
      "Node 133: 0.0\n",
      "Node 134: 0.0\n",
      "Node 135: 0.0\n",
      "Node 136: 0.0\n",
      "Node 137: 0.0\n",
      "Node 138: 0.0\n",
      "Node 139: 0.0\n",
      "Node 140: 0.0\n",
      "Node 141: 0.0\n",
      "Node 142: 0.0\n",
      "Node 143: 0.0\n",
      "Node 144: 0.0\n",
      "Node 145: 0.0\n",
      "Node 146: -0.044536709785461426\n",
      "Node 147: 0.0\n",
      "Node 148: 0.0\n",
      "Node 149: 0.0\n",
      "Node 150: 0.0\n",
      "Node 151: 0.0\n",
      "Node 152: 0.0\n",
      "Node 153: 0.0\n",
      "Node 154: 0.0\n",
      "Node 155: 0.0\n",
      "Node 156: 0.0\n",
      "Node 157: 0.0\n",
      "Node 158: 0.000286102294921875\n",
      "Node 159: 0.0\n",
      "Node 160: 0.0\n",
      "Node 161: 0.0\n",
      "Node 162: 0.0\n",
      "Node 163: 0.0\n",
      "Node 164: 0.0\n",
      "Node 165: 0.0\n",
      "Node 166: 0.0\n",
      "Node 167: 0.0\n",
      "Node 168: 0.0\n",
      "Node 169: 0.0\n",
      "Node 170: 0.0\n",
      "Node 171: -0.001671910285949707\n",
      "Node 172: 0.0\n",
      "Node 173: 0.0\n",
      "Node 174: 0.0\n",
      "Node 175: 0.0\n",
      "Node 176: -0.017418980598449707\n",
      "Node 177: 0.0\n",
      "Node 178: 0.0\n",
      "Node 179: 0.0\n",
      "Node 180: 0.0\n",
      "Node 181: 7.3909759521484375e-06\n",
      "Node 182: 0.0\n",
      "Node 183: 0.0\n",
      "Node 184: 0.0\n",
      "Node 185: 0.0\n",
      "Node 186: 0.0\n",
      "Node 187: 0.0\n",
      "Node 188: 0.0\n",
      "Node 189: 0.0\n",
      "Node 190: 0.0\n",
      "Node 191: 0.0\n",
      "Node 192: 0.0\n",
      "Node 193: 0.0\n",
      "Node 194: -0.049886226654052734\n",
      "Node 195: 0.0\n",
      "Node 196: 0.0\n",
      "Node 197: -0.025943875312805176\n",
      "Node 198: 0.0\n",
      "Node 199: 0.0\n",
      "Node 200: 0.0\n",
      "Node 201: 0.0\n",
      "Node 202: 0.0\n",
      "Node 203: 0.0\n",
      "Node 204: 0.0\n",
      "Node 205: 0.0\n",
      "Node 206: 0.0\n",
      "Node 207: 0.0\n",
      "Node 208: -5.0902366638183594e-05\n",
      "Node 209: 0.0\n",
      "Node 210: 0.0\n",
      "Node 211: -0.015407443046569824\n",
      "Node 212: 0.0\n",
      "Node 213: -0.013091564178466797\n",
      "Node 214: 0.0\n",
      "Node 215: 0.0\n",
      "Node 216: 0.0\n",
      "Node 217: 0.0\n",
      "Node 218: 0.0\n",
      "Node 219: 0.0\n",
      "Node 220: 0.0\n",
      "Node 221: 0.0\n",
      "Node 222: 0.0\n",
      "Node 223: 0.0\n",
      "Node 224: 0.0\n",
      "Node 225: 0.0\n",
      "Node 226: 0.0\n",
      "Node 227: -0.00013065338134765625\n",
      "Node 228: 0.0\n",
      "Node 229: 0.0\n",
      "Node 230: 0.0\n",
      "Node 231: 0.0\n",
      "Node 232: 0.0\n",
      "Node 233: 0.0\n",
      "Node 234: 0.0\n",
      "Node 235: 0.0\n",
      "Node 236: -4.5180320739746094e-05\n",
      "Node 237: 0.0\n",
      "Node 238: 0.0\n",
      "Node 239: 0.0\n",
      "Node 240: 0.0\n",
      "Node 241: 0.0\n",
      "Node 242: 0.0\n",
      "Node 243: 0.0\n",
      "Node 244: 0.0\n",
      "Node 245: 0.0\n",
      "Node 246: 0.0\n",
      "Node 247: 0.0\n",
      "Node 248: 0.0\n",
      "Node 249: 0.0\n",
      "Node 250: 0.0\n",
      "Node 251: -0.03731679916381836\n",
      "Node 252: 0.0\n",
      "Node 253: -0.11384105682373047\n",
      "Node 254: 0.0\n",
      "Node 255: 0.0\n"
     ]
    }
   ],
   "source": [
    "l, a, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights = model.get_weights()\n",
    "weight_len = len(or_weights) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "layer_sizes = [16, 32, 64, 128, 256]\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 213\n",
      "Worse Nodes: 2\n",
      "Important Nodes: 41\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 31\n",
      "Worse Nodes: 21\n",
      "Important Nodes: 76\n",
      "######### LAYER 2 #########\n",
      "Zero Nodes: 4\n",
      "Worse Nodes: 17\n",
      "Important Nodes: 43\n",
      "######### LAYER 3 #########\n",
      "Zero Nodes: 6\n",
      "Worse Nodes: 6\n",
      "Important Nodes: 20\n",
      "######### LAYER 4 #########\n",
      "Zero Nodes: 2\n",
      "Worse Nodes: 4\n",
      "Important Nodes: 10\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 1.4547 - accuracy: 0.4852 - auc_4: 0.8793\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.48500001430511475 --- Loss: 1.4536778926849365 --- Change: 0.0006563127040863036 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48500001430511475 --- Loss: 1.4536432027816772 --- Change: 2.428293228149414e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48500001430511475 --- Loss: 1.4535399675369263 --- Change: 7.226467132568359e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48500001430511475 --- Loss: 1.4535399675369263 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48489999771118164 --- Loss: 1.4529216289520264 --- Change: 0.00040283203125 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48489999771118164 --- Loss: 1.4529216289520264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.48510000109672546 --- Loss: 1.4528322219848633 --- Change: 0.00012258589267730713 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48570001125335693 --- Loss: 1.4507821798324585 --- Change: 0.0016150325536727905 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48570001125335693 --- Loss: 1.4507800340652466 --- Change: 1.5020370483398437e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48570001125335693 --- Loss: 1.4507800340652466 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4860000014305115 --- Loss: 1.4506022930145264 --- Change: 0.0002144157886505127 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4860000014305115 --- Loss: 1.4506022930145264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4860000014305115 --- Loss: 1.4506022930145264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4864000082015991 --- Loss: 1.4499921798706055 --- Change: 0.0005470812320709229 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48649999499320984 --- Loss: 1.4500329494476318 --- Change: 1.4573335647583035e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48649999499320984 --- Loss: 1.4500329494476318 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48649999499320984 --- Loss: 1.4500329494476318 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48660001158714294 --- Loss: 1.4500466585159302 --- Change: 2.040863037109375e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.48660001158714294 --- Loss: 1.4497734308242798 --- Change: 0.00019125938415527342 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.486299991607666 --- Loss: 1.448130488395691 --- Change: 0.0010600537061691284 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4860000014305115 --- Loss: 1.4478118419647217 --- Change: 0.0001330554485321045 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4880000054836273 --- Loss: 1.4419564008712769 --- Change: 0.00469880998134613 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4893999993801117 --- Loss: 1.4410558938980103 --- Change: 0.0010503530502319336 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49000000953674316 --- Loss: 1.4393671751022339 --- Change: 0.0013621062040328979 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49000000953674316 --- Loss: 1.4393671751022339 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.439268946647644 --- Change: 0.00015875697135925293 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.439268946647644 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.439268946647644 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4900999963283539 --- Loss: 1.4391169548034668 --- Change: 4.6393275260925294e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4900999963283539 --- Loss: 1.4391169548034668 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49000000953674316 --- Loss: 1.439024567604065 --- Change: 3.4675002098083496e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4900999963283539 --- Loss: 1.4387701749801636 --- Change: 0.00020807087421417236 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.490200012922287 --- Loss: 1.4387845993041992 --- Change: 1.990795135498047e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.490200012922287 --- Loss: 1.438722014427185 --- Change: 4.380941390991211e-05 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.490200012922287 --- Loss: 1.438722014427185 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.4383610486984253 --- Change: 0.00028267204761505126 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.4383610486984253 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.4383610486984253 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.4383610486984253 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.4383610486984253 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.4383610486984253 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4902999997138977 --- Loss: 1.4383610486984253 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4909999966621399 --- Loss: 1.4383481740951538 --- Change: 0.0002190113067626953 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4909999966621399 --- Loss: 1.4383481740951538 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4909999966621399 --- Loss: 1.4383481740951538 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4909999966621399 --- Loss: 1.4383481740951538 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4916999936103821 --- Loss: 1.4377822875976562 --- Change: 0.0006061196327209473 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.00022118091583251951 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4918000102043152 --- Loss: 1.4375091791152954 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49140000343322754 --- Loss: 1.436618447303772 --- Change: 0.0005035102367401123 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49140000343322754 --- Loss: 1.436618447303772 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49140000343322754 --- Loss: 1.436618447303772 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49149999022483826 --- Loss: 1.4366225004196167 --- Change: 2.7158856391906737e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49149999022483826 --- Loss: 1.4366228580474854 --- Change: -2.503395080566406e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49149999022483826 --- Loss: 1.4366228580474854 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4916999936103821 --- Loss: 1.4364280700683594 --- Change: 0.00019635260105133057 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.492000013589859 --- Loss: 1.4365121126174927 --- Change: 3.117620944976807e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.492000013589859 --- Loss: 1.4365121126174927 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.492000013589859 --- Loss: 1.4364066123962402 --- Change: 7.385015487670898e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.492000013589859 --- Loss: 1.4364066123962402 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.492000013589859 --- Loss: 1.4364066123962402 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.492000013589859 --- Loss: 1.4364066123962402 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49160000681877136 --- Loss: 1.4361218214035034 --- Change: 7.935166358947754e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4916999936103821 --- Loss: 1.4361231327056885 --- Change: 2.9078125953674318e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4916999936103821 --- Loss: 1.4361231327056885 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4916999936103821 --- Loss: 1.4361231327056885 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4912000000476837 --- Loss: 1.4358751773834229 --- Change: 2.357065677642824e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4927999973297119 --- Loss: 1.4365284442901611 --- Change: 2.271234989166261e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364463090896606 --- Change: -2.50637531280518e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364463090896606 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364463090896606 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 4.422664642333984e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364399909973145 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 1.6689300537109374e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364397525787354 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: -5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925999939441681 --- Loss: 1.4364404678344727 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0003016203641891479 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.4925000071525574 --- Loss: 1.4359667301177979 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.00022693574428558346 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.49300000071525574 --- Loss: 1.435856819152832 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "313/313 - 1s - loss: 1.4359 - accuracy: 0.4930 - auc_5: 0.8812\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model.get_weights()\n",
    "tol = -1e-5\n",
    "layer_sizes = [16, 32, 64, 128, 256]\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights4 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts3 = []\n",
    "places3 = []\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights4)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights4[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights4[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts3.append(num_removed2)\n",
    "    places3.append(nodes_removed2)\n",
    "\n",
    "tester_model.set_weights(best_weights4)\n",
    "loss2, acc2, auc2 = tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 - 4s - loss: 1.3094 - accuracy: 0.5359 - auc_5: 0.9035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3094489574432373, 0.5358800292015076, 0.9034848809242249]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester_model.evaluate(x_train, y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.474, 0.509, 0.252, 0.243, 0.403, 0.347, 0.745, 0.545, 0.745, 0.589]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.562, 0.61, 0.256, 0.308, 0.525, 0.36, 0.611, 0.528, 0.584, 0.586]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 13, 15, 45, 214]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 19, 49, 83, 42]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: Modified: 42 nodes\n",
      "         Original: 256 nodes\n",
      "Layer 1: Modified: 83 nodes\n",
      "         Original: 128 nodes\n",
      "Layer 2: Modified: 49 nodes\n",
      "         Original: 64 nodes\n",
      "Layer 3: Modified: 19 nodes\n",
      "         Original: 32 nodes\n",
      "Layer 4: Modified: 10 nodes\n",
      "         Original: 16 nodes\n"
     ]
    }
   ],
   "source": [
    "for i, size in enumerate(reversed(layer_sizes)):\n",
    "    print(f\"Layer {i}: Modified: {size} nodes\")\n",
    "    print(f\"         Original: {size + amounts3[4-i]} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old num parameters: 830618\n",
      "New num parameters: 138011\n",
      "Percentage reduction: 83%\n"
     ]
    }
   ],
   "source": [
    "old_param = 32*32*3*256+256*128+128*64+64*32+32*16+16*10 + 256+128+64+32+16+10\n",
    "new_param = 32*32*3*42+42*83+83*49+49*19+19*10+10*10 + 42+83+49+19+10+10\n",
    "\n",
    "print(f\"Old num parameters: {old_param}\")\n",
    "print(f\"New num parameters: {new_param}\")\n",
    "print(f\"Percentage reduction: {round((1 - new_param/old_param) * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 830,618\n",
      "Trainable params: 830,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
