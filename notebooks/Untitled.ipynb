{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset from the tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the NN model (in this case a simple ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the optimizer and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.2979 - accuracy: 0.9136\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.1421 - accuracy: 0.9575\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.1084 - accuracy: 0.9675\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0875 - accuracy: 0.9733\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0763 - accuracy: 0.9766\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0647 - accuracy: 0.9793\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0588 - accuracy: 0.9816\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0543 - accuracy: 0.9831\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0469 - accuracy: 0.9846\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0448 - accuracy: 0.9857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab2a37a0c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.0701 - accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing random number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "old = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nodes(acc, loss, weights, n, to_test, x_train, y_train, v=0):\n",
    "    check = 0\n",
    "    new_loss = loss\n",
    "    new_acc = acc\n",
    "    best_score = 1e20\n",
    "    best_model = copy.deepcopy(weights)\n",
    "    while check < to_test:   \n",
    "        new = copy.deepcopy(weights)\n",
    "        to_drop = np.random.choice(len(new[1]), n, replace=False)\n",
    "        for i in to_drop:\n",
    "            new[0][:,i] = 0\n",
    "            new[1][i] = 0\n",
    "            new[2][i,:] = 0\n",
    "        model.set_weights(new)\n",
    "        new_loss, new_acc = model.evaluate(x_train, y_train, verbose=v)\n",
    "        score = ((new_loss / loss) - 1) + ((new_acc / acc) - 1)\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_model = copy.deepcopy(new)\n",
    "            nodes_removed = to_drop.copy()\n",
    "        check = check + 1\n",
    "    return best_model, best_score, nodes_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.2113 - accuracy: 0.9519\n",
      "10000/10000 - 0s - loss: 0.2117 - accuracy: 0.9511\n",
      "10000/10000 - 0s - loss: 0.2398 - accuracy: 0.9410\n",
      "10000/10000 - 0s - loss: 0.2234 - accuracy: 0.9451\n",
      "10000/10000 - 0s - loss: 0.2492 - accuracy: 0.9394\n",
      "10000/10000 - 0s - loss: 0.2221 - accuracy: 0.9494\n",
      "10000/10000 - 0s - loss: 0.2384 - accuracy: 0.9431\n",
      "10000/10000 - 0s - loss: 0.2517 - accuracy: 0.9285\n",
      "10000/10000 - 0s - loss: 0.1937 - accuracy: 0.9526\n",
      "10000/10000 - 0s - loss: 0.2069 - accuracy: 0.9509\n",
      "10000/10000 - 0s - loss: 0.2534 - accuracy: 0.9355\n",
      "10000/10000 - 0s - loss: 0.1823 - accuracy: 0.9541\n",
      "10000/10000 - 0s - loss: 0.2461 - accuracy: 0.9292\n",
      "10000/10000 - 0s - loss: 0.2586 - accuracy: 0.9285\n",
      "10000/10000 - 0s - loss: 0.2883 - accuracy: 0.9278\n",
      "10000/10000 - 0s - loss: 0.3213 - accuracy: 0.9015\n",
      "10000/10000 - 0s - loss: 0.2768 - accuracy: 0.9268\n",
      "10000/10000 - 0s - loss: 0.2958 - accuracy: 0.9192\n",
      "10000/10000 - 0s - loss: 0.2661 - accuracy: 0.9284\n",
      "10000/10000 - 0s - loss: 0.2104 - accuracy: 0.9528\n",
      "10000/10000 - 1s - loss: 0.2560 - accuracy: 0.9390\n",
      "10000/10000 - 0s - loss: 0.2377 - accuracy: 0.9427\n",
      "10000/10000 - 0s - loss: 0.2064 - accuracy: 0.9490\n",
      "10000/10000 - 0s - loss: 0.2366 - accuracy: 0.9423\n",
      "10000/10000 - 0s - loss: 0.2077 - accuracy: 0.9509\n",
      "10000/10000 - 0s - loss: 0.2994 - accuracy: 0.9228\n",
      "10000/10000 - 0s - loss: 0.1917 - accuracy: 0.9599\n",
      "10000/10000 - 0s - loss: 0.3485 - accuracy: 0.8847\n",
      "10000/10000 - 0s - loss: 0.2581 - accuracy: 0.9377\n",
      "10000/10000 - 0s - loss: 0.2382 - accuracy: 0.9463\n",
      "10000/10000 - 0s - loss: 0.3200 - accuracy: 0.9096\n",
      "10000/10000 - 0s - loss: 0.2161 - accuracy: 0.9540\n",
      "10000/10000 - 0s - loss: 0.2378 - accuracy: 0.9375\n",
      "10000/10000 - 0s - loss: 0.2109 - accuracy: 0.9477\n",
      "10000/10000 - 0s - loss: 0.2729 - accuracy: 0.9299\n",
      "10000/10000 - 0s - loss: 0.2867 - accuracy: 0.9216\n",
      "10000/10000 - 0s - loss: 0.2032 - accuracy: 0.9517\n",
      "10000/10000 - 0s - loss: 0.2917 - accuracy: 0.9237\n",
      "10000/10000 - 0s - loss: 0.2949 - accuracy: 0.9174\n",
      "10000/10000 - 0s - loss: 0.2288 - accuracy: 0.9465\n",
      "10000/10000 - 0s - loss: 0.2049 - accuracy: 0.9497\n",
      "10000/10000 - 0s - loss: 0.2822 - accuracy: 0.9159\n",
      "10000/10000 - 0s - loss: 0.2026 - accuracy: 0.9479\n",
      "10000/10000 - 0s - loss: 0.2312 - accuracy: 0.9400\n",
      "10000/10000 - 0s - loss: 0.3268 - accuracy: 0.8913\n",
      "10000/10000 - 0s - loss: 0.2870 - accuracy: 0.9217\n",
      "10000/10000 - 0s - loss: 0.2074 - accuracy: 0.9501\n",
      "10000/10000 - 0s - loss: 0.3003 - accuracy: 0.9078\n",
      "10000/10000 - 0s - loss: 0.2568 - accuracy: 0.9300\n",
      "10000/10000 - 0s - loss: 0.3031 - accuracy: 0.9122\n",
      "10000/10000 - 0s - loss: 0.2508 - accuracy: 0.9322\n",
      "10000/10000 - 0s - loss: 0.2965 - accuracy: 0.9226\n",
      "10000/10000 - 0s - loss: 0.2654 - accuracy: 0.9407\n",
      "10000/10000 - 0s - loss: 0.2669 - accuracy: 0.9248\n",
      "10000/10000 - 0s - loss: 0.2400 - accuracy: 0.9353\n",
      "10000/10000 - 0s - loss: 0.2464 - accuracy: 0.9358\n",
      "10000/10000 - 0s - loss: 0.2567 - accuracy: 0.9379\n",
      "10000/10000 - 0s - loss: 0.2156 - accuracy: 0.9503\n",
      "10000/10000 - 0s - loss: 0.2314 - accuracy: 0.9414\n",
      "10000/10000 - 0s - loss: 0.1989 - accuracy: 0.9550\n",
      "10000/10000 - 0s - loss: 0.2729 - accuracy: 0.9219\n",
      "10000/10000 - 0s - loss: 0.4392 - accuracy: 0.8293\n",
      "10000/10000 - 0s - loss: 0.2155 - accuracy: 0.9491\n",
      "10000/10000 - 0s - loss: 0.2772 - accuracy: 0.9300\n",
      "10000/10000 - 0s - loss: 0.2904 - accuracy: 0.9263\n",
      "10000/10000 - 0s - loss: 0.2461 - accuracy: 0.9356\n",
      "10000/10000 - 0s - loss: 0.2478 - accuracy: 0.9315\n",
      "10000/10000 - 0s - loss: 0.2371 - accuracy: 0.9426\n",
      "10000/10000 - 0s - loss: 0.2287 - accuracy: 0.9452\n",
      "10000/10000 - 0s - loss: 0.2621 - accuracy: 0.9317\n",
      "10000/10000 - 0s - loss: 0.2033 - accuracy: 0.9479\n",
      "10000/10000 - 0s - loss: 0.2425 - accuracy: 0.9338\n",
      "10000/10000 - 0s - loss: 0.2208 - accuracy: 0.9467\n",
      "10000/10000 - 0s - loss: 0.2464 - accuracy: 0.9424\n",
      "10000/10000 - 0s - loss: 0.2283 - accuracy: 0.9462\n",
      "10000/10000 - 0s - loss: 0.2425 - accuracy: 0.9389\n",
      "10000/10000 - 1s - loss: 0.2321 - accuracy: 0.9414\n",
      "10000/10000 - 0s - loss: 0.2284 - accuracy: 0.9456\n",
      "10000/10000 - 0s - loss: 0.3194 - accuracy: 0.9056\n",
      "10000/10000 - 0s - loss: 0.2558 - accuracy: 0.9310\n",
      "10000/10000 - 0s - loss: 0.2723 - accuracy: 0.9315\n",
      "10000/10000 - 0s - loss: 0.2493 - accuracy: 0.9328\n",
      "10000/10000 - 0s - loss: 0.2318 - accuracy: 0.9477\n",
      "10000/10000 - 0s - loss: 0.2616 - accuracy: 0.9390\n",
      "10000/10000 - 0s - loss: 0.2326 - accuracy: 0.9439\n",
      "10000/10000 - 0s - loss: 0.2873 - accuracy: 0.9173\n",
      "10000/10000 - 0s - loss: 0.2338 - accuracy: 0.9400\n",
      "10000/10000 - 0s - loss: 0.2267 - accuracy: 0.9463\n",
      "10000/10000 - 0s - loss: 0.2731 - accuracy: 0.9303\n",
      "10000/10000 - 0s - loss: 0.2447 - accuracy: 0.9403\n",
      "10000/10000 - 0s - loss: 0.2653 - accuracy: 0.9337\n",
      "10000/10000 - 0s - loss: 0.2928 - accuracy: 0.9103\n",
      "10000/10000 - 0s - loss: 0.3578 - accuracy: 0.8819\n",
      "10000/10000 - 0s - loss: 0.2491 - accuracy: 0.9422\n",
      "10000/10000 - 0s - loss: 0.2420 - accuracy: 0.9467\n",
      "10000/10000 - 0s - loss: 0.3027 - accuracy: 0.9282\n",
      "10000/10000 - 0s - loss: 0.2201 - accuracy: 0.9523\n",
      "10000/10000 - 0s - loss: 0.2433 - accuracy: 0.9292\n",
      "10000/10000 - 0s - loss: 0.2563 - accuracy: 0.9322\n",
      "10000/10000 - 0s - loss: 0.2392 - accuracy: 0.9437\n"
     ]
    }
   ],
   "source": [
    "check = 0\n",
    "new_loss = loss\n",
    "new_acc = acc\n",
    "best_score = 1e20\n",
    "best_model = copy.deepcopy(old)\n",
    "while check < 100 and loss <= new_loss and acc >= new_acc:   \n",
    "    new = copy.deepcopy(old)\n",
    "    #for i in range(len(old)):\n",
    "    #    new[i] = old[i].copy()\n",
    "    to_drop = np.random.choice(len(new[1]), n, replace=False)\n",
    "    for i in to_drop:\n",
    "        new[0][:,i] = 0\n",
    "        new[1][i] = 0\n",
    "        new[2][i,:] = 0\n",
    "    model.set_weights(new)\n",
    "    new_loss, new_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    score = ((new_loss / loss) - 1) + ((new_acc / acc) - 1)\n",
    "    if best_score > score:\n",
    "        best_score = score\n",
    "        best_model = copy.deepcopy(new)\n",
    "    check = check + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.14561906e-02,  5.68875819e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.59633637e-03, -1.32929876e-01, -5.46002425e-02,\n",
       "       -5.79598919e-02, -7.24315643e-02,  4.81533911e-03,  0.00000000e+00,\n",
       "       -1.66753039e-01,  0.00000000e+00,  0.00000000e+00,  1.76903591e-01,\n",
       "        1.09480381e-01, -3.91760580e-02, -8.20288658e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  6.64555505e-02,  1.73894558e-02,\n",
       "       -6.39781430e-02,  0.00000000e+00,  1.12484261e-01, -8.43775086e-03,\n",
       "        6.02599643e-02,  0.00000000e+00, -2.02100664e-01,  1.28891826e-01,\n",
       "        0.00000000e+00,  2.36458983e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.03695244e-01,  4.80066426e-02, -1.01917274e-02,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -8.61475617e-02,\n",
       "       -5.66104650e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -2.18478031e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -6.79388121e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        5.86710162e-02,  0.00000000e+00,  1.09763965e-01,  0.00000000e+00,\n",
       "        1.22211762e-02,  0.00000000e+00,  3.19323689e-01,  0.00000000e+00,\n",
       "       -4.79063913e-02, -4.57470864e-03,  0.00000000e+00,  4.03490216e-02,\n",
       "        5.48387915e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  7.39403442e-02,\n",
       "        0.00000000e+00,  1.52514979e-01,  2.24502906e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  3.40433456e-02, -6.75965920e-02, -7.29739293e-02,\n",
       "        0.00000000e+00,  0.00000000e+00,  4.99943607e-02,  1.03981438e-05,\n",
       "        1.82316095e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.80668220e-01,  0.00000000e+00, -7.25767091e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.85106897e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  6.85754642e-02,  1.57847553e-01,\n",
       "        4.34255078e-02,  0.00000000e+00,  0.00000000e+00, -7.01223612e-02,\n",
       "        1.30412430e-01,  6.82475269e-02,  7.28340447e-02, -2.26635039e-01,\n",
       "        1.85175315e-01, -1.75631400e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.13766097e-01, -6.77282587e-02, -6.02602400e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.09204628e-01, -4.72155362e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02772735,  0.03969655, -0.04373507, -0.05227876, -0.01540127,\n",
       "        0.01939916, -0.12599927,  0.00782087, -0.28775257,  0.10967262,\n",
       "        0.01176385,  0.01894165,  0.03409952,  0.09057614, -0.00560972,\n",
       "        0.05311511, -0.10194276,  0.04806275, -0.0351391 , -0.06116132,\n",
       "        0.15312247,  0.04763832, -0.13207886, -0.06956799,  0.12679482,\n",
       "       -0.18160178, -0.14505222, -0.01961163, -0.09998371, -0.05521173,\n",
       "        0.10041504, -0.05807279, -0.17420274,  0.13425428,  0.11885659,\n",
       "        0.05859397,  0.05600267,  0.17821111, -0.01711084,  0.13274275,\n",
       "        0.01539186,  0.0376355 ,  0.09195647,  0.05777685,  0.06526637,\n",
       "       -0.06228716, -0.00629885, -0.07197394,  0.05084231, -0.08569249,\n",
       "       -0.28437993, -0.06442408,  0.04789071, -0.05077734, -0.03325139,\n",
       "        0.14767456, -0.07351412, -0.09289319, -0.22171189,  0.09691015,\n",
       "        0.04114012, -0.21430951,  0.06850304,  0.07310192,  0.00652712,\n",
       "        0.07554794,  0.11685141, -0.08168121,  0.08132137,  0.0442623 ,\n",
       "        0.02357966,  0.06455246, -0.05878138,  0.15651323, -0.00480317,\n",
       "       -0.0330253 ,  0.13871813, -0.23703693, -0.06636465,  0.0641122 ,\n",
       "       -0.15555902,  0.11855674, -0.07876124,  0.01095377, -0.10214404,\n",
       "        0.09586186, -0.05613307,  0.03719017,  0.03934513, -0.00884522,\n",
       "        0.04333698, -0.09226901,  0.10718799,  0.0472516 ,  0.11437607,\n",
       "        0.16179127, -0.13319118, -0.0401228 ,  0.12221295, -0.09920788,\n",
       "       -0.10585146,  0.01775449, -0.13854727, -0.0743935 , -0.06078023,\n",
       "        0.07726993, -0.05465837,  0.09297732, -0.09591851, -0.01870065,\n",
       "       -0.04317349, -0.09215351, -0.00730844, -0.33307117, -0.07316315,\n",
       "        0.03494161, -0.017581  ,  0.10192869,  0.15001072, -0.03647573,\n",
       "        0.12554386, -0.00715873, -0.04133457, -0.12653904,  0.06452513,\n",
       "       -0.03792397, -0.05036794,  0.15115803], dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old[0][130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.0713 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0712 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0707 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0702 - accuracy: 0.9792\n",
      "10000/10000 - 0s - loss: 0.0713 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0703 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0698 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0711 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0702 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0713 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0696 - accuracy: 0.9792\n",
      "10000/10000 - 0s - loss: 0.0709 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0696 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0708 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0700 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0707 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0699 - accuracy: 0.9792\n",
      "10000/10000 - 0s - loss: 0.0697 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0701 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0698 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0701 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0721 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0704 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0704 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0697 - accuracy: 0.9789\n",
      "Found new best model\n",
      "10000/10000 - 0s - loss: 0.0708 - accuracy: 0.9796\n",
      "10000/10000 - 0s - loss: 0.0731 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0708 - accuracy: 0.9784\n",
      "10000/10000 - 0s - loss: 0.0703 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0731 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0704 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0732 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0738 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0706 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0706 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0700 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0712 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0703 - accuracy: 0.9788\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0697 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0714 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0691 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0707 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0704 - accuracy: 0.9788\n",
      "10000/10000 - 0s - loss: 0.0704 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0705 - accuracy: 0.9797\n",
      "10000/10000 - 0s - loss: 0.0719 - accuracy: 0.9784\n",
      "Found new best model\n",
      "10000/10000 - 0s - loss: 0.0704 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0724 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0750 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0739 - accuracy: 0.9788\n",
      "10000/10000 - 0s - loss: 0.0711 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0703 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0733 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0722 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0729 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0714 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0722 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0729 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0731 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0705 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0706 - accuracy: 0.9788\n",
      "10000/10000 - 0s - loss: 0.0700 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0709 - accuracy: 0.9788\n",
      "10000/10000 - 0s - loss: 0.0708 - accuracy: 0.9788\n",
      "10000/10000 - 1s - loss: 0.0734 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0708 - accuracy: 0.9792\n",
      "10000/10000 - 0s - loss: 0.0729 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0702 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0706 - accuracy: 0.9788\n",
      "10000/10000 - 0s - loss: 0.0725 - accuracy: 0.9778\n",
      "10000/10000 - 0s - loss: 0.0714 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0711 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0720 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0714 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0765 - accuracy: 0.9757\n",
      "10000/10000 - 0s - loss: 0.0722 - accuracy: 0.9784\n",
      "10000/10000 - 0s - loss: 0.0731 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0744 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0756 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0746 - accuracy: 0.9784\n",
      "10000/10000 - 0s - loss: 0.0733 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0729 - accuracy: 0.9775\n",
      "10000/10000 - 0s - loss: 0.0733 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9788\n",
      "10000/10000 - 0s - loss: 0.0720 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0720 - accuracy: 0.9784\n",
      "10000/10000 - 0s - loss: 0.0720 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0728 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0722 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0763 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0742 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0719 - accuracy: 0.9789\n",
      "10000/10000 - 0s - loss: 0.0740 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0720 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0758 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0735 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0739 - accuracy: 0.9775\n",
      "10000/10000 - 1s - loss: 0.0765 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0730 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0771 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0716 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0721 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0718 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0736 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0724 - accuracy: 0.9795\n",
      "10000/10000 - 0s - loss: 0.0719 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0709 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0733 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0730 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0729 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0726 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0743 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0761 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0705 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0733 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0709 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0775 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0749 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0722 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0756 - accuracy: 0.9772\n",
      "10000/10000 - 0s - loss: 0.0716 - accuracy: 0.9784\n",
      "10000/10000 - 0s - loss: 0.0738 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0740 - accuracy: 0.9775\n",
      "10000/10000 - 1s - loss: 0.0749 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0736 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0737 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0739 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0730 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0727 - accuracy: 0.9775\n",
      "10000/10000 - 0s - loss: 0.0771 - accuracy: 0.9775\n",
      "10000/10000 - 0s - loss: 0.0737 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0738 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0796 - accuracy: 0.9759\n",
      "10000/10000 - 0s - loss: 0.0727 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0776 - accuracy: 0.9763\n",
      "10000/10000 - 0s - loss: 0.0756 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0721 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0767 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0768 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0781 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0749 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0748 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0730 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0747 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0773 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0762 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0768 - accuracy: 0.9752\n",
      "10000/10000 - 0s - loss: 0.0724 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0739 - accuracy: 0.9787\n",
      "10000/10000 - 0s - loss: 0.0710 - accuracy: 0.9799\n",
      "10000/10000 - 0s - loss: 0.0842 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0705 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0746 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0776 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0777 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0724 - accuracy: 0.9787\n",
      "10000/10000 - 1s - loss: 0.0758 - accuracy: 0.9772\n",
      "10000/10000 - 0s - loss: 0.0740 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0728 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0765 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0765 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0722 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0739 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0729 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0756 - accuracy: 0.9778\n",
      "10000/10000 - 0s - loss: 0.0746 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0771 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0728 - accuracy: 0.9778\n",
      "10000/10000 - 0s - loss: 0.0764 - accuracy: 0.9782\n",
      "10000/10000 - 0s - loss: 0.0751 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0747 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0737 - accuracy: 0.9778\n",
      "10000/10000 - 0s - loss: 0.0748 - accuracy: 0.9772\n",
      "10000/10000 - 0s - loss: 0.0749 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0737 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0754 - accuracy: 0.9778\n",
      "10000/10000 - 0s - loss: 0.0779 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0743 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0763 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0723 - accuracy: 0.9784\n",
      "10000/10000 - 0s - loss: 0.0743 - accuracy: 0.9784\n",
      "10000/10000 - 0s - loss: 0.0736 - accuracy: 0.9779\n",
      "10000/10000 - 0s - loss: 0.0804 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0743 - accuracy: 0.9763\n",
      "10000/10000 - 0s - loss: 0.0751 - accuracy: 0.9772\n",
      "10000/10000 - 0s - loss: 0.0741 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0726 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0745 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0748 - accuracy: 0.9786\n",
      "10000/10000 - 0s - loss: 0.0752 - accuracy: 0.9770\n",
      "10000/10000 - 0s - loss: 0.0786 - accuracy: 0.9746\n",
      "10000/10000 - 0s - loss: 0.0801 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0746 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0749 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0771 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0775 - accuracy: 0.9763\n",
      "10000/10000 - 0s - loss: 0.0766 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0844 - accuracy: 0.9743\n",
      "10000/10000 - 0s - loss: 0.0938 - accuracy: 0.9711\n",
      "10000/10000 - 0s - loss: 0.0802 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0750 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0735 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0783 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0738 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0763 - accuracy: 0.9773\n",
      "10000/10000 - 0s - loss: 0.0773 - accuracy: 0.9770\n",
      "10000/10000 - 0s - loss: 0.0784 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0759 - accuracy: 0.9772\n",
      "10000/10000 - 0s - loss: 0.0736 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0724 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0766 - accuracy: 0.9777\n",
      "10000/10000 - 1s - loss: 0.0736 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0790 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0744 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0751 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0736 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0826 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0779 - accuracy: 0.9773\n",
      "10000/10000 - 0s - loss: 0.0739 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0756 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0793 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0790 - accuracy: 0.9763\n",
      "10000/10000 - 0s - loss: 0.0784 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0746 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0766 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0782 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0765 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0756 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0760 - accuracy: 0.9773\n",
      "10000/10000 - 0s - loss: 0.0791 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0769 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0785 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0764 - accuracy: 0.9772\n",
      "10000/10000 - 0s - loss: 0.0762 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0802 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0781 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0770 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0747 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0829 - accuracy: 0.9758\n",
      "10000/10000 - 0s - loss: 0.0824 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0768 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0783 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0806 - accuracy: 0.9757\n",
      "10000/10000 - 0s - loss: 0.0791 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0731 - accuracy: 0.9778\n",
      "10000/10000 - 0s - loss: 0.0754 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0779 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0799 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0754 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0806 - accuracy: 0.9769\n",
      "10000/10000 - 0s - loss: 0.0783 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0763 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0782 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0777 - accuracy: 0.9773\n",
      "10000/10000 - 0s - loss: 0.0796 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0883 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.0840 - accuracy: 0.9752\n",
      "10000/10000 - 0s - loss: 0.0766 - accuracy: 0.9773\n",
      "10000/10000 - 0s - loss: 0.0806 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0870 - accuracy: 0.9751\n",
      "10000/10000 - 0s - loss: 0.0779 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0757 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0739 - accuracy: 0.9783\n",
      "10000/10000 - 0s - loss: 0.0854 - accuracy: 0.9749\n",
      "10000/10000 - 0s - loss: 0.0823 - accuracy: 0.9739\n",
      "10000/10000 - 0s - loss: 0.0812 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0803 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0790 - accuracy: 0.9763\n",
      "10000/10000 - 0s - loss: 0.0802 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0789 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0787 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0824 - accuracy: 0.9742\n",
      "10000/10000 - 0s - loss: 0.0844 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0835 - accuracy: 0.9749\n",
      "10000/10000 - 0s - loss: 0.0770 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0808 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0768 - accuracy: 0.9772\n",
      "10000/10000 - 0s - loss: 0.0775 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0767 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0806 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0757 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0782 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0747 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0781 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0816 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0821 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0831 - accuracy: 0.9749\n",
      "10000/10000 - 0s - loss: 0.0773 - accuracy: 0.9771\n",
      "10000/10000 - 0s - loss: 0.0791 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0799 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0779 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0830 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0811 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0786 - accuracy: 0.9775\n",
      "10000/10000 - 0s - loss: 0.0811 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0862 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0838 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0858 - accuracy: 0.9746\n",
      "10000/10000 - 0s - loss: 0.0803 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0814 - accuracy: 0.9758\n",
      "10000/10000 - 0s - loss: 0.0773 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0795 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0860 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0904 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0807 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0732 - accuracy: 0.9766\n",
      "10000/10000 - 0s - loss: 0.0814 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0797 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0755 - accuracy: 0.9780\n",
      "10000/10000 - 0s - loss: 0.0779 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0819 - accuracy: 0.9746\n",
      "10000/10000 - 0s - loss: 0.0842 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0791 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0801 - accuracy: 0.9751\n",
      "10000/10000 - 0s - loss: 0.0747 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0783 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0763 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0824 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0815 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0786 - accuracy: 0.9763\n",
      "10000/10000 - 0s - loss: 0.0753 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0881 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0807 - accuracy: 0.9744\n",
      "10000/10000 - 0s - loss: 0.0841 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0773 - accuracy: 0.9776\n",
      "10000/10000 - 0s - loss: 0.0790 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0823 - accuracy: 0.9770\n",
      "10000/10000 - 0s - loss: 0.0809 - accuracy: 0.9752\n",
      "10000/10000 - 0s - loss: 0.0822 - accuracy: 0.9751\n",
      "10000/10000 - 0s - loss: 0.0886 - accuracy: 0.9734\n",
      "10000/10000 - 0s - loss: 0.0796 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0909 - accuracy: 0.9726\n",
      "10000/10000 - 0s - loss: 0.0764 - accuracy: 0.9777\n",
      "10000/10000 - 0s - loss: 0.0799 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0919 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.0721 - accuracy: 0.9785\n",
      "10000/10000 - 0s - loss: 0.0785 - accuracy: 0.9774\n",
      "10000/10000 - 0s - loss: 0.0844 - accuracy: 0.9746\n",
      "10000/10000 - 0s - loss: 0.0846 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0852 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0805 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0837 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0840 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0823 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0815 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0815 - accuracy: 0.9746\n",
      "10000/10000 - 0s - loss: 0.0803 - accuracy: 0.9759\n",
      "10000/10000 - 0s - loss: 0.0847 - accuracy: 0.9757\n",
      "10000/10000 - 0s - loss: 0.0836 - accuracy: 0.9758\n",
      "10000/10000 - 0s - loss: 0.0882 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0807 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0836 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0836 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0791 - accuracy: 0.9755\n",
      "10000/10000 - 1s - loss: 0.0874 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0820 - accuracy: 0.9744\n",
      "10000/10000 - 1s - loss: 0.0845 - accuracy: 0.9756\n",
      "10000/10000 - 1s - loss: 0.0808 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0830 - accuracy: 0.9760\n",
      "10000/10000 - 1s - loss: 0.0807 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0801 - accuracy: 0.9752\n",
      "10000/10000 - 0s - loss: 0.0789 - accuracy: 0.9754\n",
      "10000/10000 - 0s - loss: 0.0792 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0821 - accuracy: 0.9754\n",
      "10000/10000 - 0s - loss: 0.0841 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0815 - accuracy: 0.9752\n",
      "10000/10000 - 0s - loss: 0.0858 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0824 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0852 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0809 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0817 - accuracy: 0.9758\n",
      "10000/10000 - 1s - loss: 0.0830 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0885 - accuracy: 0.9735\n",
      "10000/10000 - 0s - loss: 0.0886 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.0880 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.0867 - accuracy: 0.9737\n",
      "10000/10000 - 0s - loss: 0.0866 - accuracy: 0.9733\n",
      "10000/10000 - 0s - loss: 0.0957 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.0838 - accuracy: 0.9740\n",
      "10000/10000 - 0s - loss: 0.0827 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0861 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0799 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0821 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0833 - accuracy: 0.9743\n",
      "10000/10000 - 0s - loss: 0.0828 - accuracy: 0.9760\n",
      "10000/10000 - 0s - loss: 0.0852 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0844 - accuracy: 0.9739\n",
      "10000/10000 - 0s - loss: 0.0786 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0854 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0794 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0814 - accuracy: 0.9761\n",
      "10000/10000 - 1s - loss: 0.0864 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0844 - accuracy: 0.9739\n",
      "10000/10000 - 0s - loss: 0.0919 - accuracy: 0.9717\n",
      "10000/10000 - 0s - loss: 0.0829 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0934 - accuracy: 0.9710\n",
      "10000/10000 - 0s - loss: 0.0824 - accuracy: 0.9751\n",
      "10000/10000 - 0s - loss: 0.0847 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0857 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0892 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0834 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0798 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0840 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0815 - accuracy: 0.9751\n",
      "10000/10000 - 0s - loss: 0.0935 - accuracy: 0.9705\n",
      "10000/10000 - 0s - loss: 0.0848 - accuracy: 0.9758\n",
      "10000/10000 - 0s - loss: 0.0877 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0972 - accuracy: 0.9710\n",
      "10000/10000 - 0s - loss: 0.0907 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0865 - accuracy: 0.9743\n",
      "10000/10000 - 0s - loss: 0.0939 - accuracy: 0.9736\n",
      "10000/10000 - 0s - loss: 0.0866 - accuracy: 0.9739\n",
      "10000/10000 - 0s - loss: 0.0944 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.0806 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0889 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0841 - accuracy: 0.9743\n",
      "10000/10000 - 0s - loss: 0.0822 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0840 - accuracy: 0.9748\n",
      "10000/10000 - 0s - loss: 0.0876 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0980 - accuracy: 0.9697\n",
      "10000/10000 - 0s - loss: 0.0862 - accuracy: 0.9737\n",
      "10000/10000 - 0s - loss: 0.0874 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0857 - accuracy: 0.9744\n",
      "10000/10000 - 0s - loss: 0.0863 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0819 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0822 - accuracy: 0.9752\n",
      "10000/10000 - 0s - loss: 0.1018 - accuracy: 0.9687\n",
      "10000/10000 - 0s - loss: 0.0861 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0866 - accuracy: 0.9733\n",
      "10000/10000 - 0s - loss: 0.0761 - accuracy: 0.9781\n",
      "10000/10000 - 0s - loss: 0.0960 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.0884 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0845 - accuracy: 0.9754\n",
      "10000/10000 - 0s - loss: 0.0898 - accuracy: 0.9726\n",
      "10000/10000 - 0s - loss: 0.0834 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0821 - accuracy: 0.9749\n",
      "10000/10000 - 0s - loss: 0.0878 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0851 - accuracy: 0.9740\n",
      "10000/10000 - 0s - loss: 0.0821 - accuracy: 0.9737\n",
      "10000/10000 - 0s - loss: 0.0790 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0909 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.0798 - accuracy: 0.9764\n",
      "10000/10000 - 0s - loss: 0.0879 - accuracy: 0.9733\n",
      "10000/10000 - 0s - loss: 0.0858 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0991 - accuracy: 0.9692\n",
      "10000/10000 - 0s - loss: 0.0936 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0883 - accuracy: 0.9728\n",
      "10000/10000 - 0s - loss: 0.0905 - accuracy: 0.9735\n",
      "10000/10000 - 0s - loss: 0.0799 - accuracy: 0.9768\n",
      "10000/10000 - 0s - loss: 0.0850 - accuracy: 0.9749\n",
      "10000/10000 - 0s - loss: 0.0801 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0825 - accuracy: 0.9751\n",
      "10000/10000 - 0s - loss: 0.0836 - accuracy: 0.9757\n",
      "10000/10000 - 0s - loss: 0.0813 - accuracy: 0.9763\n",
      "10000/10000 - 0s - loss: 0.0817 - accuracy: 0.9755\n",
      "10000/10000 - 0s - loss: 0.0883 - accuracy: 0.9732\n",
      "10000/10000 - 0s - loss: 0.0995 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.0935 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.0822 - accuracy: 0.9743\n",
      "10000/10000 - 1s - loss: 0.0930 - accuracy: 0.9728\n",
      "10000/10000 - 0s - loss: 0.0848 - accuracy: 0.9749\n",
      "10000/10000 - 0s - loss: 0.1130 - accuracy: 0.9641\n",
      "10000/10000 - 0s - loss: 0.1061 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.0896 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0841 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0911 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0998 - accuracy: 0.9682\n",
      "10000/10000 - 0s - loss: 0.0834 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0898 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.0877 - accuracy: 0.9739\n",
      "10000/10000 - 0s - loss: 0.0843 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0863 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0942 - accuracy: 0.9718\n",
      "10000/10000 - 0s - loss: 0.0878 - accuracy: 0.9734\n",
      "10000/10000 - 0s - loss: 0.0867 - accuracy: 0.9744\n",
      "10000/10000 - 0s - loss: 0.0877 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.0981 - accuracy: 0.9718\n",
      "10000/10000 - 0s - loss: 0.0963 - accuracy: 0.9690\n",
      "10000/10000 - 0s - loss: 0.0856 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0882 - accuracy: 0.9743\n",
      "10000/10000 - 0s - loss: 0.0855 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0890 - accuracy: 0.9733\n",
      "10000/10000 - 0s - loss: 0.0915 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0962 - accuracy: 0.9713\n",
      "10000/10000 - 0s - loss: 0.0876 - accuracy: 0.9740\n",
      "10000/10000 - 0s - loss: 0.0919 - accuracy: 0.9711\n",
      "10000/10000 - 0s - loss: 0.0896 - accuracy: 0.9746\n",
      "10000/10000 - 1s - loss: 0.0884 - accuracy: 0.9743\n",
      "10000/10000 - 1s - loss: 0.0884 - accuracy: 0.9745\n",
      "10000/10000 - 1s - loss: 0.0921 - accuracy: 0.9716\n",
      "10000/10000 - 1s - loss: 0.0890 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0845 - accuracy: 0.9758\n",
      "10000/10000 - 0s - loss: 0.0969 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.0945 - accuracy: 0.9713\n",
      "10000/10000 - 0s - loss: 0.0849 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0836 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0871 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0857 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0863 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0906 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.0912 - accuracy: 0.9735\n",
      "10000/10000 - 0s - loss: 0.0937 - accuracy: 0.9732\n",
      "10000/10000 - 0s - loss: 0.0874 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.0934 - accuracy: 0.9727\n",
      "10000/10000 - 0s - loss: 0.0831 - accuracy: 0.9756\n",
      "10000/10000 - 0s - loss: 0.0866 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0840 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0855 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0897 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.0910 - accuracy: 0.9714\n",
      "10000/10000 - 0s - loss: 0.0880 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0934 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.1003 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.0862 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0871 - accuracy: 0.9740\n",
      "10000/10000 - 0s - loss: 0.0839 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0915 - accuracy: 0.9727\n",
      "10000/10000 - 0s - loss: 0.0910 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.0854 - accuracy: 0.9752\n",
      "10000/10000 - 0s - loss: 0.0918 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.0859 - accuracy: 0.9746\n",
      "10000/10000 - 0s - loss: 0.0804 - accuracy: 0.9767\n",
      "10000/10000 - 0s - loss: 0.0914 - accuracy: 0.9744\n",
      "10000/10000 - 0s - loss: 0.0877 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.0894 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0873 - accuracy: 0.9737\n",
      "10000/10000 - 0s - loss: 0.0851 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0879 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.0843 - accuracy: 0.9742\n",
      "10000/10000 - 0s - loss: 0.0871 - accuracy: 0.9747\n",
      "10000/10000 - 0s - loss: 0.0960 - accuracy: 0.9694\n",
      "10000/10000 - 0s - loss: 0.0860 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0955 - accuracy: 0.9713\n",
      "10000/10000 - 0s - loss: 0.1043 - accuracy: 0.9679\n",
      "10000/10000 - 0s - loss: 0.0941 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0873 - accuracy: 0.9736\n",
      "10000/10000 - 0s - loss: 0.0974 - accuracy: 0.9719\n",
      "10000/10000 - 0s - loss: 0.0891 - accuracy: 0.9749\n",
      "10000/10000 - 0s - loss: 0.0821 - accuracy: 0.9765\n",
      "10000/10000 - 0s - loss: 0.0865 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0955 - accuracy: 0.9714\n",
      "10000/10000 - 0s - loss: 0.0905 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0881 - accuracy: 0.9736\n",
      "10000/10000 - 0s - loss: 0.0832 - accuracy: 0.9761\n",
      "10000/10000 - 0s - loss: 0.0889 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.0851 - accuracy: 0.9762\n",
      "10000/10000 - 0s - loss: 0.0972 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.0987 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.1091 - accuracy: 0.9654\n",
      "10000/10000 - 0s - loss: 0.0866 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0904 - accuracy: 0.9740\n",
      "10000/10000 - 0s - loss: 0.0881 - accuracy: 0.9740\n",
      "10000/10000 - 0s - loss: 0.0981 - accuracy: 0.9699\n",
      "10000/10000 - 0s - loss: 0.0972 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.0846 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0997 - accuracy: 0.9713\n",
      "10000/10000 - 0s - loss: 0.0887 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.1096 - accuracy: 0.9670\n",
      "10000/10000 - 0s - loss: 0.0844 - accuracy: 0.9758\n",
      "10000/10000 - 0s - loss: 0.0939 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.0896 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.0829 - accuracy: 0.9753\n",
      "10000/10000 - 0s - loss: 0.0953 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0913 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.0858 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0909 - accuracy: 0.9733\n",
      "10000/10000 - 0s - loss: 0.0965 - accuracy: 0.9705\n",
      "10000/10000 - 0s - loss: 0.0920 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.0896 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.0894 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.0891 - accuracy: 0.9732\n",
      "10000/10000 - 0s - loss: 0.0931 - accuracy: 0.9703\n",
      "10000/10000 - 0s - loss: 0.0952 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.0975 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.1054 - accuracy: 0.9690\n",
      "10000/10000 - 0s - loss: 0.0926 - accuracy: 0.9726\n",
      "10000/10000 - 0s - loss: 0.0935 - accuracy: 0.9735\n",
      "10000/10000 - 0s - loss: 0.0986 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.0956 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.0946 - accuracy: 0.9713\n",
      "10000/10000 - 0s - loss: 0.0924 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.0857 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0918 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.0939 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.0852 - accuracy: 0.9750\n",
      "10000/10000 - 0s - loss: 0.0921 - accuracy: 0.9728\n",
      "10000/10000 - 0s - loss: 0.0929 - accuracy: 0.9734\n",
      "10000/10000 - 0s - loss: 0.0986 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0939 - accuracy: 0.9723\n",
      "10000/10000 - 0s - loss: 0.0941 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.0884 - accuracy: 0.9744\n",
      "10000/10000 - 0s - loss: 0.0898 - accuracy: 0.9736\n",
      "10000/10000 - 0s - loss: 0.0991 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.0946 - accuracy: 0.9701\n",
      "10000/10000 - 0s - loss: 0.0863 - accuracy: 0.9746\n",
      "10000/10000 - 0s - loss: 0.0885 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.1087 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.0902 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0983 - accuracy: 0.9723\n",
      "10000/10000 - 0s - loss: 0.0911 - accuracy: 0.9737\n",
      "10000/10000 - 0s - loss: 0.0908 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.1069 - accuracy: 0.9701\n",
      "10000/10000 - 0s - loss: 0.1012 - accuracy: 0.9695\n",
      "10000/10000 - 0s - loss: 0.1011 - accuracy: 0.9699\n",
      "10000/10000 - 0s - loss: 0.0993 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.0892 - accuracy: 0.9732\n",
      "10000/10000 - 0s - loss: 0.0963 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.0996 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.0894 - accuracy: 0.9737\n",
      "10000/10000 - 0s - loss: 0.1021 - accuracy: 0.9689\n",
      "10000/10000 - 0s - loss: 0.1020 - accuracy: 0.9680\n",
      "10000/10000 - 0s - loss: 0.0946 - accuracy: 0.9717\n",
      "10000/10000 - 0s - loss: 0.0874 - accuracy: 0.9742\n",
      "10000/10000 - 0s - loss: 0.1018 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.0912 - accuracy: 0.9717\n",
      "10000/10000 - 0s - loss: 0.1118 - accuracy: 0.9667\n",
      "10000/10000 - 1s - loss: 0.1056 - accuracy: 0.9690\n",
      "10000/10000 - 1s - loss: 0.0871 - accuracy: 0.9751\n",
      "10000/10000 - 1s - loss: 0.0933 - accuracy: 0.9721\n",
      "10000/10000 - 1s - loss: 0.0965 - accuracy: 0.9703\n",
      "10000/10000 - 0s - loss: 0.0944 - accuracy: 0.9735\n",
      "10000/10000 - 0s - loss: 0.1171 - accuracy: 0.9646\n",
      "10000/10000 - 0s - loss: 0.0968 - accuracy: 0.9710\n",
      "10000/10000 - 1s - loss: 0.1076 - accuracy: 0.9692\n",
      "10000/10000 - 1s - loss: 0.1018 - accuracy: 0.9700\n",
      "10000/10000 - 0s - loss: 0.0955 - accuracy: 0.9710\n",
      "10000/10000 - 1s - loss: 0.1001 - accuracy: 0.9708\n",
      "10000/10000 - 1s - loss: 0.0996 - accuracy: 0.9702\n",
      "10000/10000 - 0s - loss: 0.0922 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.0994 - accuracy: 0.9699\n",
      "10000/10000 - 0s - loss: 0.1081 - accuracy: 0.9664\n",
      "10000/10000 - 0s - loss: 0.0989 - accuracy: 0.9699\n",
      "10000/10000 - 0s - loss: 0.0953 - accuracy: 0.9709\n",
      "10000/10000 - 0s - loss: 0.0917 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0997 - accuracy: 0.9711\n",
      "10000/10000 - 0s - loss: 0.0945 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0936 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.1014 - accuracy: 0.9692\n",
      "10000/10000 - 0s - loss: 0.0952 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.1109 - accuracy: 0.9686\n",
      "10000/10000 - 0s - loss: 0.1000 - accuracy: 0.9710\n",
      "10000/10000 - 0s - loss: 0.0942 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.1027 - accuracy: 0.9702\n",
      "10000/10000 - 0s - loss: 0.0986 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.0963 - accuracy: 0.9728\n",
      "10000/10000 - 0s - loss: 0.0918 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.0925 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0922 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.0949 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.1012 - accuracy: 0.9714\n",
      "10000/10000 - 0s - loss: 0.0886 - accuracy: 0.9740\n",
      "10000/10000 - 0s - loss: 0.1038 - accuracy: 0.9672\n",
      "10000/10000 - 0s - loss: 0.0969 - accuracy: 0.9707\n",
      "10000/10000 - 0s - loss: 0.0920 - accuracy: 0.9754\n",
      "10000/10000 - 0s - loss: 0.1087 - accuracy: 0.9683\n",
      "10000/10000 - 0s - loss: 0.0881 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.0958 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.1125 - accuracy: 0.9665\n",
      "10000/10000 - 0s - loss: 0.0979 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.0906 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.1044 - accuracy: 0.9695\n",
      "10000/10000 - 0s - loss: 0.0978 - accuracy: 0.9693\n",
      "10000/10000 - 0s - loss: 0.0960 - accuracy: 0.9706\n",
      "10000/10000 - 0s - loss: 0.1022 - accuracy: 0.9719\n",
      "10000/10000 - 0s - loss: 0.1402 - accuracy: 0.9580\n",
      "10000/10000 - 0s - loss: 0.1021 - accuracy: 0.9693\n",
      "10000/10000 - 0s - loss: 0.0933 - accuracy: 0.9734\n",
      "10000/10000 - 0s - loss: 0.0974 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.0891 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.1039 - accuracy: 0.9693\n",
      "10000/10000 - 1s - loss: 0.0897 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.0963 - accuracy: 0.9726\n",
      "10000/10000 - 0s - loss: 0.1225 - accuracy: 0.9635\n",
      "10000/10000 - 0s - loss: 0.0944 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.0964 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.0983 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.1202 - accuracy: 0.9645\n",
      "10000/10000 - 0s - loss: 0.1112 - accuracy: 0.9682\n",
      "10000/10000 - 0s - loss: 0.1072 - accuracy: 0.9703\n",
      "10000/10000 - 0s - loss: 0.0958 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.0942 - accuracy: 0.9736\n",
      "10000/10000 - 0s - loss: 0.0934 - accuracy: 0.9735\n",
      "10000/10000 - 0s - loss: 0.1068 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.1084 - accuracy: 0.9693\n",
      "10000/10000 - 0s - loss: 0.0988 - accuracy: 0.9718\n",
      "10000/10000 - 0s - loss: 0.0956 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.0997 - accuracy: 0.9695\n",
      "10000/10000 - 0s - loss: 0.0884 - accuracy: 0.9751\n",
      "10000/10000 - 0s - loss: 0.0916 - accuracy: 0.9733\n",
      "10000/10000 - 0s - loss: 0.1173 - accuracy: 0.9633\n",
      "10000/10000 - 0s - loss: 0.0992 - accuracy: 0.9706\n",
      "10000/10000 - 0s - loss: 0.0994 - accuracy: 0.9725\n",
      "10000/10000 - 0s - loss: 0.1057 - accuracy: 0.9694\n",
      "10000/10000 - 0s - loss: 0.1001 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.1021 - accuracy: 0.9709\n",
      "10000/10000 - 0s - loss: 0.0974 - accuracy: 0.9701\n",
      "10000/10000 - 0s - loss: 0.1093 - accuracy: 0.9682\n",
      "10000/10000 - 0s - loss: 0.0965 - accuracy: 0.9709\n",
      "10000/10000 - 0s - loss: 0.0957 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.0910 - accuracy: 0.9738\n",
      "10000/10000 - 0s - loss: 0.1026 - accuracy: 0.9692\n",
      "10000/10000 - 0s - loss: 0.1055 - accuracy: 0.9700\n",
      "10000/10000 - 0s - loss: 0.0925 - accuracy: 0.9744\n",
      "10000/10000 - 0s - loss: 0.1259 - accuracy: 0.9604\n",
      "10000/10000 - 0s - loss: 0.1088 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.0926 - accuracy: 0.9741\n",
      "10000/10000 - 0s - loss: 0.0991 - accuracy: 0.9698\n",
      "10000/10000 - 0s - loss: 0.1012 - accuracy: 0.9700\n",
      "10000/10000 - 0s - loss: 0.1021 - accuracy: 0.9711\n",
      "10000/10000 - 0s - loss: 0.1112 - accuracy: 0.9687\n",
      "10000/10000 - 0s - loss: 0.0904 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.1060 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.1007 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.0905 - accuracy: 0.9733\n",
      "10000/10000 - 0s - loss: 0.0968 - accuracy: 0.9708\n",
      "10000/10000 - 0s - loss: 0.0965 - accuracy: 0.9717\n",
      "10000/10000 - 0s - loss: 0.1035 - accuracy: 0.9681\n",
      "10000/10000 - 0s - loss: 0.1041 - accuracy: 0.9702\n",
      "10000/10000 - 0s - loss: 0.1118 - accuracy: 0.9677\n",
      "10000/10000 - 0s - loss: 0.1111 - accuracy: 0.9680\n",
      "10000/10000 - 0s - loss: 0.1221 - accuracy: 0.9641\n",
      "10000/10000 - 0s - loss: 0.1080 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.1145 - accuracy: 0.9658\n",
      "10000/10000 - 0s - loss: 0.0982 - accuracy: 0.9714\n",
      "10000/10000 - 0s - loss: 0.1098 - accuracy: 0.9675\n",
      "10000/10000 - 0s - loss: 0.1096 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.0925 - accuracy: 0.9736\n",
      "10000/10000 - 0s - loss: 0.0955 - accuracy: 0.9706\n",
      "10000/10000 - 0s - loss: 0.1188 - accuracy: 0.9663\n",
      "10000/10000 - 0s - loss: 0.0971 - accuracy: 0.9710\n",
      "10000/10000 - 0s - loss: 0.1068 - accuracy: 0.9690\n",
      "10000/10000 - 0s - loss: 0.1041 - accuracy: 0.9688\n",
      "10000/10000 - 0s - loss: 0.1003 - accuracy: 0.9736\n",
      "10000/10000 - 0s - loss: 0.0868 - accuracy: 0.9745\n",
      "10000/10000 - 0s - loss: 0.0948 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.1271 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.0953 - accuracy: 0.9717\n",
      "10000/10000 - 0s - loss: 0.0962 - accuracy: 0.9729\n",
      "10000/10000 - 0s - loss: 0.0967 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.1079 - accuracy: 0.9662\n",
      "10000/10000 - 0s - loss: 0.0920 - accuracy: 0.9728\n",
      "10000/10000 - 0s - loss: 0.1124 - accuracy: 0.9663\n",
      "10000/10000 - 0s - loss: 0.1036 - accuracy: 0.9698\n",
      "10000/10000 - 0s - loss: 0.0995 - accuracy: 0.9721\n",
      "10000/10000 - 0s - loss: 0.0995 - accuracy: 0.9706\n",
      "10000/10000 - 0s - loss: 0.1016 - accuracy: 0.9691\n",
      "10000/10000 - 0s - loss: 0.1164 - accuracy: 0.9675\n",
      "10000/10000 - 0s - loss: 0.1126 - accuracy: 0.9667\n",
      "10000/10000 - 0s - loss: 0.1094 - accuracy: 0.9687\n",
      "10000/10000 - 0s - loss: 0.0960 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.1102 - accuracy: 0.9664\n",
      "10000/10000 - 0s - loss: 0.1086 - accuracy: 0.9675\n",
      "10000/10000 - 0s - loss: 0.0982 - accuracy: 0.9719\n",
      "10000/10000 - 0s - loss: 0.1010 - accuracy: 0.9713\n",
      "10000/10000 - 0s - loss: 0.1086 - accuracy: 0.9686\n",
      "10000/10000 - 0s - loss: 0.1261 - accuracy: 0.9609\n",
      "10000/10000 - 0s - loss: 0.1039 - accuracy: 0.9685\n",
      "10000/10000 - 0s - loss: 0.1395 - accuracy: 0.9599\n",
      "10000/10000 - 0s - loss: 0.1113 - accuracy: 0.9675\n",
      "10000/10000 - 0s - loss: 0.1007 - accuracy: 0.9701\n",
      "10000/10000 - 0s - loss: 0.1008 - accuracy: 0.9717\n",
      "10000/10000 - 0s - loss: 0.1097 - accuracy: 0.9670\n",
      "10000/10000 - 0s - loss: 0.1125 - accuracy: 0.9684\n",
      "10000/10000 - 0s - loss: 0.1103 - accuracy: 0.9677\n",
      "10000/10000 - 0s - loss: 0.0993 - accuracy: 0.9723\n",
      "10000/10000 - 0s - loss: 0.0995 - accuracy: 0.9718\n",
      "10000/10000 - 0s - loss: 0.1190 - accuracy: 0.9652\n",
      "10000/10000 - 0s - loss: 0.0997 - accuracy: 0.9702\n",
      "10000/10000 - 0s - loss: 0.1225 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1098 - accuracy: 0.9671\n",
      "10000/10000 - 0s - loss: 0.0940 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.1009 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.1100 - accuracy: 0.9680\n",
      "10000/10000 - 0s - loss: 0.1112 - accuracy: 0.9670\n",
      "10000/10000 - 0s - loss: 0.1067 - accuracy: 0.9694\n",
      "10000/10000 - 0s - loss: 0.1134 - accuracy: 0.9685\n",
      "10000/10000 - 0s - loss: 0.1333 - accuracy: 0.9589\n",
      "10000/10000 - 0s - loss: 0.1260 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.0981 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.1093 - accuracy: 0.9686\n",
      "10000/10000 - 0s - loss: 0.1058 - accuracy: 0.9702\n",
      "10000/10000 - 0s - loss: 0.1070 - accuracy: 0.9667\n",
      "10000/10000 - 0s - loss: 0.0983 - accuracy: 0.9700\n",
      "10000/10000 - 0s - loss: 0.1114 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.1135 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.1120 - accuracy: 0.9655\n",
      "10000/10000 - 0s - loss: 0.1146 - accuracy: 0.9653\n",
      "10000/10000 - 0s - loss: 0.0944 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.0962 - accuracy: 0.9730\n",
      "10000/10000 - 0s - loss: 0.1073 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.1247 - accuracy: 0.9635\n",
      "10000/10000 - 0s - loss: 0.1013 - accuracy: 0.9697\n",
      "10000/10000 - 0s - loss: 0.0997 - accuracy: 0.9708\n",
      "10000/10000 - 1s - loss: 0.1027 - accuracy: 0.9732\n",
      "10000/10000 - 1s - loss: 0.1200 - accuracy: 0.9665\n",
      "10000/10000 - 0s - loss: 0.1041 - accuracy: 0.9700\n",
      "10000/10000 - 0s - loss: 0.1373 - accuracy: 0.9629\n",
      "10000/10000 - 0s - loss: 0.1203 - accuracy: 0.9645\n",
      "10000/10000 - 0s - loss: 0.1251 - accuracy: 0.9645\n",
      "10000/10000 - 0s - loss: 0.1312 - accuracy: 0.9625\n",
      "10000/10000 - 0s - loss: 0.1303 - accuracy: 0.9640\n",
      "10000/10000 - 0s - loss: 0.1173 - accuracy: 0.9663\n",
      "10000/10000 - 0s - loss: 0.1278 - accuracy: 0.9587\n",
      "10000/10000 - 1s - loss: 0.1133 - accuracy: 0.9663\n",
      "10000/10000 - 1s - loss: 0.1311 - accuracy: 0.9626\n",
      "10000/10000 - 1s - loss: 0.1002 - accuracy: 0.9712\n",
      "10000/10000 - 1s - loss: 0.1049 - accuracy: 0.9685\n",
      "10000/10000 - 1s - loss: 0.1147 - accuracy: 0.9667\n",
      "10000/10000 - 1s - loss: 0.1448 - accuracy: 0.9568\n",
      "10000/10000 - 0s - loss: 0.1106 - accuracy: 0.9684\n",
      "10000/10000 - 0s - loss: 0.1019 - accuracy: 0.9697\n",
      "10000/10000 - 0s - loss: 0.1138 - accuracy: 0.9671\n",
      "10000/10000 - 0s - loss: 0.1026 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.1056 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.0973 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.1288 - accuracy: 0.9615\n",
      "10000/10000 - 0s - loss: 0.1138 - accuracy: 0.9671\n",
      "10000/10000 - 0s - loss: 0.0997 - accuracy: 0.9706\n",
      "10000/10000 - 0s - loss: 0.1174 - accuracy: 0.9657\n",
      "10000/10000 - 0s - loss: 0.1398 - accuracy: 0.9576\n",
      "10000/10000 - 0s - loss: 0.1267 - accuracy: 0.9618\n",
      "10000/10000 - 0s - loss: 0.1046 - accuracy: 0.9719\n",
      "10000/10000 - 0s - loss: 0.0993 - accuracy: 0.9710\n",
      "10000/10000 - 0s - loss: 0.1103 - accuracy: 0.9692\n",
      "10000/10000 - 0s - loss: 0.1108 - accuracy: 0.9679\n",
      "10000/10000 - 0s - loss: 0.1292 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1121 - accuracy: 0.9695\n",
      "10000/10000 - 0s - loss: 0.1282 - accuracy: 0.9611\n",
      "10000/10000 - 0s - loss: 0.1199 - accuracy: 0.9623\n",
      "10000/10000 - 0s - loss: 0.1053 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.0962 - accuracy: 0.9705\n",
      "10000/10000 - 0s - loss: 0.1285 - accuracy: 0.9628\n",
      "10000/10000 - 0s - loss: 0.1233 - accuracy: 0.9652\n",
      "10000/10000 - 0s - loss: 0.1033 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.1284 - accuracy: 0.9648\n",
      "10000/10000 - 0s - loss: 0.1040 - accuracy: 0.9710\n",
      "10000/10000 - 0s - loss: 0.1402 - accuracy: 0.9580\n",
      "10000/10000 - 0s - loss: 0.1031 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.0996 - accuracy: 0.9726\n",
      "10000/10000 - 0s - loss: 0.1169 - accuracy: 0.9689\n",
      "10000/10000 - 0s - loss: 0.1208 - accuracy: 0.9648\n",
      "10000/10000 - 0s - loss: 0.1225 - accuracy: 0.9650\n",
      "10000/10000 - 0s - loss: 0.1073 - accuracy: 0.9699\n",
      "10000/10000 - 0s - loss: 0.1025 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.1000 - accuracy: 0.9726\n",
      "10000/10000 - 0s - loss: 0.1080 - accuracy: 0.9675\n",
      "10000/10000 - 0s - loss: 0.1526 - accuracy: 0.9553\n",
      "10000/10000 - 0s - loss: 0.1322 - accuracy: 0.9626\n",
      "10000/10000 - 0s - loss: 0.1026 - accuracy: 0.9701\n",
      "10000/10000 - 0s - loss: 0.1364 - accuracy: 0.9588\n",
      "10000/10000 - 0s - loss: 0.1003 - accuracy: 0.9708\n",
      "10000/10000 - 0s - loss: 0.1023 - accuracy: 0.9722\n",
      "10000/10000 - 0s - loss: 0.1095 - accuracy: 0.9718\n",
      "10000/10000 - 0s - loss: 0.1166 - accuracy: 0.9656\n",
      "10000/10000 - 0s - loss: 0.1153 - accuracy: 0.9659\n",
      "10000/10000 - 0s - loss: 0.1024 - accuracy: 0.9715\n",
      "10000/10000 - 0s - loss: 0.1351 - accuracy: 0.9581\n",
      "10000/10000 - 0s - loss: 0.1314 - accuracy: 0.9627\n",
      "10000/10000 - 0s - loss: 0.1046 - accuracy: 0.9720\n",
      "10000/10000 - 0s - loss: 0.1099 - accuracy: 0.9660\n",
      "10000/10000 - 0s - loss: 0.0968 - accuracy: 0.9727\n",
      "10000/10000 - 0s - loss: 0.1250 - accuracy: 0.9634\n",
      "10000/10000 - 0s - loss: 0.1163 - accuracy: 0.9691\n",
      "10000/10000 - 0s - loss: 0.1160 - accuracy: 0.9688\n",
      "10000/10000 - 0s - loss: 0.1090 - accuracy: 0.9679\n",
      "10000/10000 - 0s - loss: 0.1022 - accuracy: 0.9711\n",
      "10000/10000 - 0s - loss: 0.1271 - accuracy: 0.9613\n",
      "10000/10000 - 0s - loss: 0.1799 - accuracy: 0.9402\n",
      "10000/10000 - 0s - loss: 0.1283 - accuracy: 0.9623\n",
      "10000/10000 - 0s - loss: 0.1135 - accuracy: 0.9687\n",
      "10000/10000 - 0s - loss: 0.1072 - accuracy: 0.9714\n",
      "10000/10000 - 0s - loss: 0.1123 - accuracy: 0.9700\n",
      "10000/10000 - 0s - loss: 0.1121 - accuracy: 0.9672\n",
      "10000/10000 - 0s - loss: 0.1117 - accuracy: 0.9695\n",
      "10000/10000 - 0s - loss: 0.1058 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.1050 - accuracy: 0.9701\n",
      "10000/10000 - 0s - loss: 0.1423 - accuracy: 0.9576\n",
      "10000/10000 - 0s - loss: 0.1185 - accuracy: 0.9671\n",
      "10000/10000 - 0s - loss: 0.1168 - accuracy: 0.9666\n",
      "10000/10000 - 0s - loss: 0.1112 - accuracy: 0.9680\n",
      "10000/10000 - 0s - loss: 0.1117 - accuracy: 0.9698\n",
      "10000/10000 - 0s - loss: 0.1019 - accuracy: 0.9702\n",
      "10000/10000 - 0s - loss: 0.1014 - accuracy: 0.9712\n",
      "10000/10000 - 0s - loss: 0.1079 - accuracy: 0.9684\n",
      "10000/10000 - 0s - loss: 0.1056 - accuracy: 0.9697\n",
      "10000/10000 - 0s - loss: 0.1106 - accuracy: 0.9682\n",
      "10000/10000 - 0s - loss: 0.1165 - accuracy: 0.9633\n",
      "10000/10000 - 0s - loss: 0.1154 - accuracy: 0.9679\n",
      "10000/10000 - 0s - loss: 0.1076 - accuracy: 0.9694\n",
      "10000/10000 - 0s - loss: 0.1640 - accuracy: 0.9493\n",
      "10000/10000 - 0s - loss: 0.1015 - accuracy: 0.9708\n",
      "10000/10000 - 0s - loss: 0.1076 - accuracy: 0.9693\n",
      "10000/10000 - 0s - loss: 0.1053 - accuracy: 0.9717\n",
      "10000/10000 - 0s - loss: 0.1163 - accuracy: 0.9686\n",
      "10000/10000 - 0s - loss: 0.1190 - accuracy: 0.9660\n",
      "10000/10000 - 0s - loss: 0.1257 - accuracy: 0.9619\n",
      "10000/10000 - 0s - loss: 0.1493 - accuracy: 0.9561\n",
      "10000/10000 - 0s - loss: 0.1208 - accuracy: 0.9672\n",
      "10000/10000 - 0s - loss: 0.1165 - accuracy: 0.9664\n",
      "10000/10000 - 0s - loss: 0.1249 - accuracy: 0.9659\n",
      "10000/10000 - 0s - loss: 0.1249 - accuracy: 0.9636\n",
      "10000/10000 - 0s - loss: 0.1100 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.1470 - accuracy: 0.9595\n",
      "10000/10000 - 0s - loss: 0.1203 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1156 - accuracy: 0.9665\n",
      "10000/10000 - 0s - loss: 0.1222 - accuracy: 0.9661\n",
      "10000/10000 - 0s - loss: 0.1114 - accuracy: 0.9697\n",
      "10000/10000 - 0s - loss: 0.1183 - accuracy: 0.9682\n",
      "10000/10000 - 0s - loss: 0.1344 - accuracy: 0.9592\n",
      "10000/10000 - 0s - loss: 0.1313 - accuracy: 0.9632\n",
      "10000/10000 - 0s - loss: 0.1361 - accuracy: 0.9592\n",
      "10000/10000 - 0s - loss: 0.1271 - accuracy: 0.9637\n",
      "10000/10000 - 0s - loss: 0.1917 - accuracy: 0.9302\n",
      "10000/10000 - 0s - loss: 0.1179 - accuracy: 0.9681\n",
      "10000/10000 - 0s - loss: 0.1331 - accuracy: 0.9638\n",
      "10000/10000 - 0s - loss: 0.1160 - accuracy: 0.9667\n",
      "10000/10000 - 0s - loss: 0.1149 - accuracy: 0.9689\n",
      "10000/10000 - 0s - loss: 0.1088 - accuracy: 0.9688\n",
      "10000/10000 - 0s - loss: 0.1168 - accuracy: 0.9664\n",
      "10000/10000 - 0s - loss: 0.1027 - accuracy: 0.9724\n",
      "10000/10000 - 0s - loss: 0.1157 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.1034 - accuracy: 0.9731\n",
      "10000/10000 - 0s - loss: 0.1249 - accuracy: 0.9660\n",
      "10000/10000 - 0s - loss: 0.1216 - accuracy: 0.9668\n",
      "10000/10000 - 0s - loss: 0.1336 - accuracy: 0.9604\n",
      "10000/10000 - 0s - loss: 0.1140 - accuracy: 0.9682\n",
      "10000/10000 - 0s - loss: 0.1082 - accuracy: 0.9694\n",
      "10000/10000 - 0s - loss: 0.1206 - accuracy: 0.9667\n",
      "10000/10000 - 0s - loss: 0.1300 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.1153 - accuracy: 0.9687\n",
      "10000/10000 - 0s - loss: 0.1166 - accuracy: 0.9680\n",
      "10000/10000 - 0s - loss: 0.1048 - accuracy: 0.9698\n",
      "10000/10000 - 0s - loss: 0.1066 - accuracy: 0.9703\n",
      "10000/10000 - 0s - loss: 0.1059 - accuracy: 0.9716\n",
      "10000/10000 - 0s - loss: 0.1236 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.1041 - accuracy: 0.9705\n",
      "10000/10000 - 0s - loss: 0.1210 - accuracy: 0.9650\n",
      "10000/10000 - 0s - loss: 0.1216 - accuracy: 0.9659\n",
      "10000/10000 - 0s - loss: 0.1297 - accuracy: 0.9631\n",
      "10000/10000 - 0s - loss: 0.1212 - accuracy: 0.9662\n",
      "10000/10000 - 0s - loss: 0.1250 - accuracy: 0.9621\n",
      "10000/10000 - 0s - loss: 0.1356 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.1274 - accuracy: 0.9648\n",
      "10000/10000 - 0s - loss: 0.1259 - accuracy: 0.9679\n",
      "10000/10000 - 0s - loss: 0.1518 - accuracy: 0.9579\n",
      "10000/10000 - 0s - loss: 0.1167 - accuracy: 0.9666\n",
      "10000/10000 - 0s - loss: 0.1347 - accuracy: 0.9648\n",
      "10000/10000 - 0s - loss: 0.1269 - accuracy: 0.9640\n",
      "10000/10000 - 0s - loss: 0.1445 - accuracy: 0.9588\n",
      "10000/10000 - 0s - loss: 0.1279 - accuracy: 0.9624\n",
      "10000/10000 - 0s - loss: 0.1313 - accuracy: 0.9589\n",
      "10000/10000 - 0s - loss: 0.1255 - accuracy: 0.9647\n",
      "10000/10000 - 0s - loss: 0.1489 - accuracy: 0.9542\n",
      "10000/10000 - 0s - loss: 0.1106 - accuracy: 0.9678\n",
      "10000/10000 - 0s - loss: 0.1220 - accuracy: 0.9652\n",
      "10000/10000 - 0s - loss: 0.1353 - accuracy: 0.9615\n",
      "10000/10000 - 0s - loss: 0.1244 - accuracy: 0.9648\n",
      "10000/10000 - 0s - loss: 0.1323 - accuracy: 0.9620\n",
      "10000/10000 - 0s - loss: 0.1113 - accuracy: 0.9699\n",
      "10000/10000 - 0s - loss: 0.1187 - accuracy: 0.9672\n",
      "10000/10000 - 0s - loss: 0.1388 - accuracy: 0.9586\n",
      "10000/10000 - 0s - loss: 0.1312 - accuracy: 0.9657\n",
      "10000/10000 - 0s - loss: 0.1183 - accuracy: 0.9673\n",
      "10000/10000 - 0s - loss: 0.1260 - accuracy: 0.9655\n",
      "10000/10000 - 0s - loss: 0.1220 - accuracy: 0.9665\n",
      "10000/10000 - 0s - loss: 0.1564 - accuracy: 0.9546\n",
      "10000/10000 - 0s - loss: 0.1158 - accuracy: 0.9666\n",
      "10000/10000 - 0s - loss: 0.1222 - accuracy: 0.9644\n",
      "10000/10000 - 0s - loss: 0.1533 - accuracy: 0.9555\n",
      "10000/10000 - 0s - loss: 0.1220 - accuracy: 0.9665\n",
      "10000/10000 - 0s - loss: 0.1347 - accuracy: 0.9639\n",
      "10000/10000 - 0s - loss: 0.1382 - accuracy: 0.9587\n",
      "10000/10000 - 0s - loss: 0.1307 - accuracy: 0.9659\n",
      "10000/10000 - 0s - loss: 0.1134 - accuracy: 0.9686\n",
      "10000/10000 - 0s - loss: 0.1271 - accuracy: 0.9658\n",
      "10000/10000 - 1s - loss: 0.1401 - accuracy: 0.9627\n",
      "10000/10000 - 0s - loss: 0.1273 - accuracy: 0.9639\n",
      "10000/10000 - 0s - loss: 0.1292 - accuracy: 0.9621\n",
      "10000/10000 - 0s - loss: 0.1251 - accuracy: 0.9656\n",
      "10000/10000 - 1s - loss: 0.1167 - accuracy: 0.9668\n",
      "10000/10000 - 0s - loss: 0.1162 - accuracy: 0.9656\n",
      "10000/10000 - 1s - loss: 0.1436 - accuracy: 0.9572\n",
      "10000/10000 - 0s - loss: 0.1230 - accuracy: 0.9636\n",
      "10000/10000 - 0s - loss: 0.1270 - accuracy: 0.9644\n",
      "10000/10000 - 0s - loss: 0.1074 - accuracy: 0.9714\n",
      "10000/10000 - 1s - loss: 0.1470 - accuracy: 0.9583\n",
      "10000/10000 - 1s - loss: 0.1298 - accuracy: 0.9643\n",
      "10000/10000 - 1s - loss: 0.1293 - accuracy: 0.9649\n",
      "10000/10000 - 1s - loss: 0.1276 - accuracy: 0.9637\n",
      "10000/10000 - 1s - loss: 0.1476 - accuracy: 0.9552\n",
      "10000/10000 - 1s - loss: 0.1401 - accuracy: 0.9590\n",
      "10000/10000 - 0s - loss: 0.1408 - accuracy: 0.9628\n",
      "10000/10000 - 1s - loss: 0.1220 - accuracy: 0.9697\n",
      "10000/10000 - 1s - loss: 0.1123 - accuracy: 0.9683\n",
      "10000/10000 - 1s - loss: 0.1312 - accuracy: 0.9617\n",
      "10000/10000 - 1s - loss: 0.1106 - accuracy: 0.9690\n",
      "10000/10000 - 0s - loss: 0.1089 - accuracy: 0.9703\n",
      "10000/10000 - 0s - loss: 0.1642 - accuracy: 0.9495\n",
      "10000/10000 - 0s - loss: 0.1203 - accuracy: 0.9665\n",
      "10000/10000 - 1s - loss: 0.1221 - accuracy: 0.9672\n",
      "10000/10000 - 1s - loss: 0.1245 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1187 - accuracy: 0.9694\n",
      "10000/10000 - 0s - loss: 0.1307 - accuracy: 0.9662\n",
      "10000/10000 - 0s - loss: 0.1376 - accuracy: 0.9618\n",
      "10000/10000 - 0s - loss: 0.1173 - accuracy: 0.9701\n",
      "10000/10000 - 0s - loss: 0.1671 - accuracy: 0.9506\n",
      "10000/10000 - 0s - loss: 0.1439 - accuracy: 0.9588\n",
      "10000/10000 - 0s - loss: 0.1415 - accuracy: 0.9575\n",
      "10000/10000 - 0s - loss: 0.1320 - accuracy: 0.9652\n",
      "10000/10000 - 0s - loss: 0.1366 - accuracy: 0.9564\n",
      "10000/10000 - 0s - loss: 0.1289 - accuracy: 0.9633\n",
      "10000/10000 - 0s - loss: 0.1416 - accuracy: 0.9587\n",
      "10000/10000 - 0s - loss: 0.1134 - accuracy: 0.9684\n",
      "10000/10000 - 0s - loss: 0.1218 - accuracy: 0.9659\n",
      "10000/10000 - 0s - loss: 0.1349 - accuracy: 0.9627\n",
      "10000/10000 - 0s - loss: 0.1310 - accuracy: 0.9607\n",
      "10000/10000 - 0s - loss: 0.1243 - accuracy: 0.9651\n",
      "10000/10000 - 0s - loss: 0.1324 - accuracy: 0.9616\n",
      "10000/10000 - 0s - loss: 0.1319 - accuracy: 0.9636\n",
      "10000/10000 - 0s - loss: 0.1267 - accuracy: 0.9636\n",
      "10000/10000 - 0s - loss: 0.1244 - accuracy: 0.9668\n",
      "10000/10000 - 0s - loss: 0.1343 - accuracy: 0.9624\n",
      "10000/10000 - 0s - loss: 0.1646 - accuracy: 0.9494\n",
      "10000/10000 - 0s - loss: 0.1270 - accuracy: 0.9633\n",
      "10000/10000 - 1s - loss: 0.1273 - accuracy: 0.9661\n",
      "10000/10000 - 1s - loss: 0.1319 - accuracy: 0.9590\n",
      "10000/10000 - 1s - loss: 0.1316 - accuracy: 0.9652\n",
      "10000/10000 - 0s - loss: 0.1173 - accuracy: 0.9690\n",
      "10000/10000 - 0s - loss: 0.1311 - accuracy: 0.9642\n",
      "10000/10000 - 0s - loss: 0.1171 - accuracy: 0.9670\n",
      "10000/10000 - 0s - loss: 0.1235 - accuracy: 0.9659\n",
      "10000/10000 - 0s - loss: 0.1439 - accuracy: 0.9573\n",
      "10000/10000 - 0s - loss: 0.1096 - accuracy: 0.9696\n",
      "10000/10000 - 0s - loss: 0.1311 - accuracy: 0.9636\n",
      "10000/10000 - 0s - loss: 0.1346 - accuracy: 0.9635\n",
      "10000/10000 - 0s - loss: 0.1192 - accuracy: 0.9667\n",
      "10000/10000 - 0s - loss: 0.1315 - accuracy: 0.9640\n",
      "10000/10000 - 0s - loss: 0.1455 - accuracy: 0.9577\n",
      "10000/10000 - 0s - loss: 0.1304 - accuracy: 0.9627\n",
      "10000/10000 - 0s - loss: 0.1200 - accuracy: 0.9681\n",
      "10000/10000 - 0s - loss: 0.1458 - accuracy: 0.9584\n",
      "10000/10000 - 0s - loss: 0.1148 - accuracy: 0.9677\n",
      "10000/10000 - 0s - loss: 0.1358 - accuracy: 0.9589\n",
      "10000/10000 - 0s - loss: 0.1194 - accuracy: 0.9682\n",
      "10000/10000 - 0s - loss: 0.1414 - accuracy: 0.9612\n",
      "10000/10000 - 0s - loss: 0.1187 - accuracy: 0.9665\n",
      "10000/10000 - 0s - loss: 0.1445 - accuracy: 0.9566\n",
      "10000/10000 - 0s - loss: 0.1309 - accuracy: 0.9607\n",
      "10000/10000 - 0s - loss: 0.1456 - accuracy: 0.9599\n",
      "10000/10000 - 0s - loss: 0.1195 - accuracy: 0.9639\n",
      "10000/10000 - 0s - loss: 0.1723 - accuracy: 0.9463\n",
      "10000/10000 - 0s - loss: 0.1418 - accuracy: 0.9626\n",
      "10000/10000 - 0s - loss: 0.1464 - accuracy: 0.9624\n",
      "10000/10000 - 0s - loss: 0.1508 - accuracy: 0.9570\n",
      "10000/10000 - 0s - loss: 0.1317 - accuracy: 0.9634\n",
      "10000/10000 - 0s - loss: 0.1227 - accuracy: 0.9679\n",
      "10000/10000 - 0s - loss: 0.1876 - accuracy: 0.9445\n",
      "10000/10000 - 0s - loss: 0.1255 - accuracy: 0.9637\n",
      "10000/10000 - 0s - loss: 0.1394 - accuracy: 0.9640\n",
      "10000/10000 - 0s - loss: 0.1391 - accuracy: 0.9614\n",
      "10000/10000 - 0s - loss: 0.1155 - accuracy: 0.9664\n",
      "10000/10000 - 0s - loss: 0.1752 - accuracy: 0.9462\n",
      "10000/10000 - 0s - loss: 0.1389 - accuracy: 0.9597\n",
      "10000/10000 - 0s - loss: 0.1551 - accuracy: 0.9581\n",
      "10000/10000 - 0s - loss: 0.1457 - accuracy: 0.9601\n",
      "10000/10000 - 0s - loss: 0.1321 - accuracy: 0.9646\n",
      "10000/10000 - 0s - loss: 0.1207 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1270 - accuracy: 0.9642\n",
      "10000/10000 - 0s - loss: 0.1328 - accuracy: 0.9640\n",
      "10000/10000 - 0s - loss: 0.1414 - accuracy: 0.9618\n",
      "10000/10000 - 0s - loss: 0.1411 - accuracy: 0.9603\n",
      "10000/10000 - 0s - loss: 0.1647 - accuracy: 0.9520\n",
      "10000/10000 - 0s - loss: 0.1365 - accuracy: 0.9631\n",
      "10000/10000 - 0s - loss: 0.1829 - accuracy: 0.9456\n",
      "10000/10000 - 0s - loss: 0.1769 - accuracy: 0.9488\n",
      "10000/10000 - 0s - loss: 0.1303 - accuracy: 0.9635\n",
      "10000/10000 - 0s - loss: 0.1323 - accuracy: 0.9630\n",
      "10000/10000 - 0s - loss: 0.1560 - accuracy: 0.9547\n",
      "10000/10000 - 0s - loss: 0.1435 - accuracy: 0.9605\n",
      "10000/10000 - 0s - loss: 0.1417 - accuracy: 0.9590\n",
      "10000/10000 - 0s - loss: 0.1964 - accuracy: 0.9376\n",
      "10000/10000 - 0s - loss: 0.1556 - accuracy: 0.9534\n",
      "10000/10000 - 0s - loss: 0.1550 - accuracy: 0.9504\n",
      "10000/10000 - 0s - loss: 0.1356 - accuracy: 0.9606\n",
      "10000/10000 - 0s - loss: 0.1464 - accuracy: 0.9605\n",
      "10000/10000 - 0s - loss: 0.1308 - accuracy: 0.9636\n",
      "10000/10000 - 0s - loss: 0.1864 - accuracy: 0.9392\n",
      "10000/10000 - 0s - loss: 0.1412 - accuracy: 0.9635\n",
      "10000/10000 - 0s - loss: 0.1193 - accuracy: 0.9654\n",
      "10000/10000 - 0s - loss: 0.1263 - accuracy: 0.9641\n",
      "10000/10000 - 0s - loss: 0.1727 - accuracy: 0.9525\n",
      "10000/10000 - 0s - loss: 0.1404 - accuracy: 0.9596\n",
      "10000/10000 - 0s - loss: 0.1370 - accuracy: 0.9633\n",
      "10000/10000 - 0s - loss: 0.1354 - accuracy: 0.9628\n",
      "10000/10000 - 0s - loss: 0.1375 - accuracy: 0.9609\n",
      "10000/10000 - 0s - loss: 0.1369 - accuracy: 0.9635\n",
      "10000/10000 - 0s - loss: 0.1346 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.1375 - accuracy: 0.9602\n",
      "10000/10000 - 0s - loss: 0.1368 - accuracy: 0.9643\n",
      "10000/10000 - 0s - loss: 0.1408 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.1447 - accuracy: 0.9566\n",
      "10000/10000 - 0s - loss: 0.1271 - accuracy: 0.9667\n",
      "10000/10000 - 0s - loss: 0.1735 - accuracy: 0.9514\n",
      "10000/10000 - 0s - loss: 0.1285 - accuracy: 0.9667\n",
      "10000/10000 - 0s - loss: 0.1572 - accuracy: 0.9559\n",
      "10000/10000 - 0s - loss: 0.1465 - accuracy: 0.9548\n",
      "10000/10000 - 0s - loss: 0.1121 - accuracy: 0.9704\n",
      "10000/10000 - 0s - loss: 0.1400 - accuracy: 0.9621\n",
      "10000/10000 - 0s - loss: 0.1255 - accuracy: 0.9666\n",
      "10000/10000 - 0s - loss: 0.1524 - accuracy: 0.9592\n",
      "10000/10000 - 0s - loss: 0.1490 - accuracy: 0.9620\n",
      "10000/10000 - 0s - loss: 0.1470 - accuracy: 0.9612\n",
      "10000/10000 - 0s - loss: 0.1442 - accuracy: 0.9580\n",
      "10000/10000 - 0s - loss: 0.1775 - accuracy: 0.9473\n",
      "10000/10000 - 0s - loss: 0.1410 - accuracy: 0.9631\n",
      "10000/10000 - 0s - loss: 0.1594 - accuracy: 0.9558\n",
      "10000/10000 - 0s - loss: 0.1391 - accuracy: 0.9604\n",
      "10000/10000 - 0s - loss: 0.1655 - accuracy: 0.9537\n",
      "10000/10000 - 0s - loss: 0.1385 - accuracy: 0.9641\n",
      "10000/10000 - 0s - loss: 0.1323 - accuracy: 0.9668\n",
      "10000/10000 - 0s - loss: 0.1351 - accuracy: 0.9634\n",
      "10000/10000 - 0s - loss: 0.1193 - accuracy: 0.9674\n",
      "10000/10000 - 0s - loss: 0.1411 - accuracy: 0.9576\n",
      "10000/10000 - 0s - loss: 0.1551 - accuracy: 0.9583\n",
      "10000/10000 - 0s - loss: 0.1258 - accuracy: 0.9659\n",
      "10000/10000 - 0s - loss: 0.1625 - accuracy: 0.9568\n",
      "10000/10000 - 0s - loss: 0.1595 - accuracy: 0.9578\n",
      "10000/10000 - 0s - loss: 0.1677 - accuracy: 0.9525\n",
      "10000/10000 - 0s - loss: 0.1359 - accuracy: 0.9643\n",
      "10000/10000 - 0s - loss: 0.1511 - accuracy: 0.9601\n",
      "10000/10000 - 0s - loss: 0.1802 - accuracy: 0.9483\n",
      "10000/10000 - 0s - loss: 0.1218 - accuracy: 0.9684\n",
      "10000/10000 - 0s - loss: 0.1237 - accuracy: 0.9654\n",
      "10000/10000 - 0s - loss: 0.1303 - accuracy: 0.9662\n",
      "10000/10000 - 0s - loss: 0.1311 - accuracy: 0.9616\n",
      "10000/10000 - 0s - loss: 0.1237 - accuracy: 0.9680\n",
      "10000/10000 - 0s - loss: 0.1525 - accuracy: 0.9616\n",
      "10000/10000 - 0s - loss: 0.1322 - accuracy: 0.9631\n",
      "10000/10000 - 0s - loss: 0.1456 - accuracy: 0.9625\n",
      "10000/10000 - 0s - loss: 0.1387 - accuracy: 0.9647\n",
      "10000/10000 - 0s - loss: 0.1539 - accuracy: 0.9592\n",
      "10000/10000 - 0s - loss: 0.1285 - accuracy: 0.9657\n",
      "10000/10000 - 0s - loss: 0.1372 - accuracy: 0.9581\n",
      "10000/10000 - 0s - loss: 0.1538 - accuracy: 0.9553\n",
      "10000/10000 - 0s - loss: 0.2131 - accuracy: 0.9345\n",
      "10000/10000 - 0s - loss: 0.1326 - accuracy: 0.9621\n",
      "10000/10000 - 0s - loss: 0.1362 - accuracy: 0.9664\n",
      "10000/10000 - 0s - loss: 0.1533 - accuracy: 0.9599\n",
      "10000/10000 - 1s - loss: 0.1382 - accuracy: 0.9621\n",
      "10000/10000 - 0s - loss: 0.1601 - accuracy: 0.9572\n",
      "10000/10000 - 0s - loss: 0.1591 - accuracy: 0.9567\n",
      "10000/10000 - 0s - loss: 0.1847 - accuracy: 0.9455\n",
      "10000/10000 - 0s - loss: 0.1633 - accuracy: 0.9559\n",
      "10000/10000 - 0s - loss: 0.1416 - accuracy: 0.9613\n",
      "10000/10000 - 0s - loss: 0.1924 - accuracy: 0.9429\n",
      "10000/10000 - 0s - loss: 0.1367 - accuracy: 0.9622\n",
      "10000/10000 - 0s - loss: 0.1511 - accuracy: 0.9583\n",
      "10000/10000 - 0s - loss: 0.1642 - accuracy: 0.9528\n",
      "10000/10000 - 0s - loss: 0.1393 - accuracy: 0.9623\n",
      "10000/10000 - 0s - loss: 0.1581 - accuracy: 0.9535\n",
      "10000/10000 - 0s - loss: 0.1415 - accuracy: 0.9576\n",
      "10000/10000 - 0s - loss: 0.1574 - accuracy: 0.9579\n",
      "10000/10000 - 0s - loss: 0.1560 - accuracy: 0.9542\n",
      "10000/10000 - 0s - loss: 0.1823 - accuracy: 0.9514\n",
      "10000/10000 - 0s - loss: 0.1772 - accuracy: 0.9485\n",
      "10000/10000 - 0s - loss: 0.1340 - accuracy: 0.9653\n",
      "10000/10000 - 0s - loss: 0.1299 - accuracy: 0.9645\n",
      "10000/10000 - 0s - loss: 0.1340 - accuracy: 0.9623\n",
      "10000/10000 - 0s - loss: 0.1540 - accuracy: 0.9556\n",
      "10000/10000 - 0s - loss: 0.1764 - accuracy: 0.9503\n",
      "10000/10000 - 0s - loss: 0.1480 - accuracy: 0.9602\n",
      "10000/10000 - 0s - loss: 0.1955 - accuracy: 0.9446\n",
      "10000/10000 - 0s - loss: 0.1551 - accuracy: 0.9613\n",
      "10000/10000 - 0s - loss: 0.2293 - accuracy: 0.9208\n",
      "10000/10000 - 0s - loss: 0.1575 - accuracy: 0.9590\n",
      "10000/10000 - 0s - loss: 0.1514 - accuracy: 0.9619\n",
      "10000/10000 - 0s - loss: 0.1794 - accuracy: 0.9443\n",
      "10000/10000 - 0s - loss: 0.1533 - accuracy: 0.9568\n",
      "10000/10000 - 0s - loss: 0.1387 - accuracy: 0.9625\n",
      "10000/10000 - 0s - loss: 0.1432 - accuracy: 0.9626\n",
      "10000/10000 - 0s - loss: 0.1495 - accuracy: 0.9619\n",
      "10000/10000 - 0s - loss: 0.1536 - accuracy: 0.9596\n",
      "10000/10000 - 0s - loss: 0.1412 - accuracy: 0.9624\n",
      "10000/10000 - 0s - loss: 0.1522 - accuracy: 0.9611\n",
      "10000/10000 - 0s - loss: 0.1294 - accuracy: 0.9658\n",
      "10000/10000 - 0s - loss: 0.1310 - accuracy: 0.9640\n",
      "10000/10000 - 0s - loss: 0.1801 - accuracy: 0.9485\n",
      "10000/10000 - 0s - loss: 0.1449 - accuracy: 0.9609\n",
      "10000/10000 - 0s - loss: 0.1656 - accuracy: 0.9479\n",
      "10000/10000 - 0s - loss: 0.1239 - accuracy: 0.9676\n",
      "10000/10000 - 0s - loss: 0.1421 - accuracy: 0.9608\n",
      "10000/10000 - 0s - loss: 0.1632 - accuracy: 0.9561\n",
      "10000/10000 - 0s - loss: 0.1483 - accuracy: 0.9602\n",
      "10000/10000 - 0s - loss: 0.1787 - accuracy: 0.9516\n",
      "10000/10000 - 0s - loss: 0.1501 - accuracy: 0.9605\n",
      "10000/10000 - 0s - loss: 0.1437 - accuracy: 0.9625\n",
      "10000/10000 - 0s - loss: 0.1903 - accuracy: 0.9409\n",
      "10000/10000 - 0s - loss: 0.1589 - accuracy: 0.9609\n",
      "10000/10000 - 0s - loss: 0.1366 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1529 - accuracy: 0.9611\n",
      "10000/10000 - 0s - loss: 0.1340 - accuracy: 0.9631\n",
      "10000/10000 - 0s - loss: 0.1699 - accuracy: 0.9537\n",
      "10000/10000 - 0s - loss: 0.1483 - accuracy: 0.9600\n",
      "10000/10000 - 0s - loss: 0.2086 - accuracy: 0.9336\n",
      "10000/10000 - 0s - loss: 0.1745 - accuracy: 0.9538\n",
      "10000/10000 - 0s - loss: 0.1690 - accuracy: 0.9523\n",
      "10000/10000 - 0s - loss: 0.1580 - accuracy: 0.9577\n",
      "10000/10000 - 0s - loss: 0.1672 - accuracy: 0.9522\n",
      "10000/10000 - 0s - loss: 0.1634 - accuracy: 0.9572\n",
      "10000/10000 - 0s - loss: 0.1600 - accuracy: 0.9574\n",
      "10000/10000 - 0s - loss: 0.1844 - accuracy: 0.9444\n",
      "10000/10000 - 0s - loss: 0.1727 - accuracy: 0.9520\n",
      "10000/10000 - 0s - loss: 0.1993 - accuracy: 0.9502\n",
      "10000/10000 - 0s - loss: 0.2090 - accuracy: 0.9378\n",
      "10000/10000 - 0s - loss: 0.1523 - accuracy: 0.9611\n",
      "10000/10000 - 0s - loss: 0.1428 - accuracy: 0.9615\n",
      "10000/10000 - 0s - loss: 0.2061 - accuracy: 0.9363\n",
      "10000/10000 - 0s - loss: 0.1786 - accuracy: 0.9526\n",
      "10000/10000 - 0s - loss: 0.1473 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1471 - accuracy: 0.9632\n",
      "10000/10000 - 0s - loss: 0.1612 - accuracy: 0.9567\n",
      "10000/10000 - 0s - loss: 0.1814 - accuracy: 0.9447\n",
      "10000/10000 - 0s - loss: 0.1547 - accuracy: 0.9620\n",
      "10000/10000 - 0s - loss: 0.2227 - accuracy: 0.9335\n",
      "10000/10000 - 0s - loss: 0.1688 - accuracy: 0.9540\n",
      "10000/10000 - 0s - loss: 0.1626 - accuracy: 0.9560\n",
      "10000/10000 - 0s - loss: 0.2337 - accuracy: 0.9358\n",
      "10000/10000 - 0s - loss: 0.1622 - accuracy: 0.9549\n",
      "10000/10000 - 0s - loss: 0.1749 - accuracy: 0.9529\n",
      "10000/10000 - 0s - loss: 0.1337 - accuracy: 0.9640\n",
      "10000/10000 - 0s - loss: 0.1571 - accuracy: 0.9595\n",
      "10000/10000 - 0s - loss: 0.1875 - accuracy: 0.9486\n",
      "10000/10000 - 0s - loss: 0.1610 - accuracy: 0.9551\n",
      "10000/10000 - 0s - loss: 0.1532 - accuracy: 0.9625\n",
      "10000/10000 - 0s - loss: 0.1962 - accuracy: 0.9434\n",
      "10000/10000 - 0s - loss: 0.1375 - accuracy: 0.9579\n",
      "10000/10000 - 0s - loss: 0.1755 - accuracy: 0.9550\n",
      "10000/10000 - 0s - loss: 0.1570 - accuracy: 0.9602\n",
      "10000/10000 - 0s - loss: 0.1441 - accuracy: 0.9616\n",
      "10000/10000 - 0s - loss: 0.1696 - accuracy: 0.9576\n",
      "10000/10000 - 0s - loss: 0.1479 - accuracy: 0.9612\n",
      "10000/10000 - 0s - loss: 0.1724 - accuracy: 0.9549\n",
      "10000/10000 - 0s - loss: 0.1800 - accuracy: 0.9523\n",
      "10000/10000 - 0s - loss: 0.1516 - accuracy: 0.9604\n",
      "10000/10000 - 0s - loss: 0.1503 - accuracy: 0.9624\n",
      "10000/10000 - 0s - loss: 0.1702 - accuracy: 0.9563\n",
      "10000/10000 - 0s - loss: 0.1634 - accuracy: 0.9604\n",
      "10000/10000 - 0s - loss: 0.1674 - accuracy: 0.9592\n",
      "10000/10000 - 0s - loss: 0.1873 - accuracy: 0.9523\n",
      "10000/10000 - 0s - loss: 0.1831 - accuracy: 0.9426\n",
      "10000/10000 - 0s - loss: 0.1494 - accuracy: 0.9617\n",
      "10000/10000 - 0s - loss: 0.1561 - accuracy: 0.9588\n",
      "10000/10000 - 0s - loss: 0.1627 - accuracy: 0.9558\n",
      "10000/10000 - 0s - loss: 0.1532 - accuracy: 0.9628\n",
      "10000/10000 - 0s - loss: 0.1682 - accuracy: 0.9564\n",
      "10000/10000 - 0s - loss: 0.1704 - accuracy: 0.9500\n",
      "10000/10000 - 0s - loss: 0.1527 - accuracy: 0.9628\n",
      "10000/10000 - 0s - loss: 0.1657 - accuracy: 0.9599\n",
      "10000/10000 - 0s - loss: 0.1578 - accuracy: 0.9572\n",
      "10000/10000 - 0s - loss: 0.1763 - accuracy: 0.9501\n",
      "10000/10000 - 0s - loss: 0.1688 - accuracy: 0.9568\n",
      "10000/10000 - 0s - loss: 0.2251 - accuracy: 0.9332\n",
      "10000/10000 - 0s - loss: 0.1829 - accuracy: 0.9531\n",
      "10000/10000 - 0s - loss: 0.1565 - accuracy: 0.9634\n",
      "10000/10000 - 0s - loss: 0.1638 - accuracy: 0.9559\n",
      "10000/10000 - 0s - loss: 0.2417 - accuracy: 0.9296\n",
      "10000/10000 - 0s - loss: 0.1996 - accuracy: 0.9394\n",
      "10000/10000 - 0s - loss: 0.1797 - accuracy: 0.9476\n",
      "10000/10000 - 0s - loss: 0.1736 - accuracy: 0.9517\n",
      "10000/10000 - 0s - loss: 0.1450 - accuracy: 0.9649\n",
      "10000/10000 - 0s - loss: 0.1682 - accuracy: 0.9565\n",
      "10000/10000 - 0s - loss: 0.1808 - accuracy: 0.9519\n",
      "10000/10000 - 0s - loss: 0.1747 - accuracy: 0.9526\n",
      "10000/10000 - 0s - loss: 0.1519 - accuracy: 0.9616\n",
      "10000/10000 - 0s - loss: 0.1572 - accuracy: 0.9583\n",
      "10000/10000 - 0s - loss: 0.1601 - accuracy: 0.9606\n",
      "10000/10000 - 0s - loss: 0.1607 - accuracy: 0.9565\n",
      "10000/10000 - 0s - loss: 0.2027 - accuracy: 0.9490\n",
      "10000/10000 - 0s - loss: 0.1383 - accuracy: 0.9631\n",
      "10000/10000 - 0s - loss: 0.1669 - accuracy: 0.9589\n",
      "10000/10000 - 0s - loss: 0.2073 - accuracy: 0.9431\n",
      "10000/10000 - 0s - loss: 0.1795 - accuracy: 0.9486\n",
      "10000/10000 - 0s - loss: 0.2166 - accuracy: 0.9360\n",
      "10000/10000 - 0s - loss: 0.1630 - accuracy: 0.9581\n",
      "10000/10000 - 0s - loss: 0.1866 - accuracy: 0.9517\n",
      "10000/10000 - 1s - loss: 0.3416 - accuracy: 0.8822\n",
      "10000/10000 - 0s - loss: 0.2074 - accuracy: 0.9426\n",
      "10000/10000 - 0s - loss: 0.1724 - accuracy: 0.9495\n",
      "10000/10000 - 0s - loss: 0.1480 - accuracy: 0.9612\n",
      "10000/10000 - 0s - loss: 0.1683 - accuracy: 0.9552\n",
      "10000/10000 - 0s - loss: 0.1860 - accuracy: 0.9511\n",
      "10000/10000 - 0s - loss: 0.1794 - accuracy: 0.9534\n",
      "10000/10000 - 0s - loss: 0.1569 - accuracy: 0.9554\n",
      "10000/10000 - 0s - loss: 0.1646 - accuracy: 0.9521\n",
      "10000/10000 - 0s - loss: 0.1981 - accuracy: 0.9480\n",
      "10000/10000 - 0s - loss: 0.1645 - accuracy: 0.9557\n",
      "10000/10000 - 0s - loss: 0.1698 - accuracy: 0.9555\n",
      "10000/10000 - 0s - loss: 0.1651 - accuracy: 0.9564\n",
      "10000/10000 - 0s - loss: 0.1888 - accuracy: 0.9451\n",
      "10000/10000 - 0s - loss: 0.1892 - accuracy: 0.9493\n",
      "10000/10000 - 0s - loss: 0.1864 - accuracy: 0.9527\n",
      "10000/10000 - 0s - loss: 0.1439 - accuracy: 0.9622\n",
      "10000/10000 - 0s - loss: 0.2211 - accuracy: 0.9333\n",
      "10000/10000 - 0s - loss: 0.2063 - accuracy: 0.9384\n",
      "10000/10000 - 0s - loss: 0.2372 - accuracy: 0.9356\n",
      "10000/10000 - 0s - loss: 0.2254 - accuracy: 0.9345\n",
      "10000/10000 - 0s - loss: 0.1743 - accuracy: 0.9532\n",
      "10000/10000 - 0s - loss: 0.2765 - accuracy: 0.9095\n",
      "10000/10000 - 0s - loss: 0.2023 - accuracy: 0.9438\n",
      "10000/10000 - 0s - loss: 0.1697 - accuracy: 0.9571\n",
      "10000/10000 - 0s - loss: 0.1495 - accuracy: 0.9602\n",
      "10000/10000 - 0s - loss: 0.1645 - accuracy: 0.9557\n",
      "10000/10000 - 0s - loss: 0.2028 - accuracy: 0.9494\n",
      "10000/10000 - 0s - loss: 0.2083 - accuracy: 0.9440\n",
      "10000/10000 - 0s - loss: 0.1713 - accuracy: 0.9543\n",
      "10000/10000 - 0s - loss: 0.2036 - accuracy: 0.9410\n",
      "10000/10000 - 0s - loss: 0.1500 - accuracy: 0.9622\n",
      "10000/10000 - 0s - loss: 0.1572 - accuracy: 0.9625\n",
      "10000/10000 - 0s - loss: 0.1644 - accuracy: 0.9567\n",
      "10000/10000 - 0s - loss: 0.1627 - accuracy: 0.9553\n",
      "10000/10000 - 0s - loss: 0.2203 - accuracy: 0.9422\n",
      "10000/10000 - 0s - loss: 0.2131 - accuracy: 0.9361\n",
      "10000/10000 - 0s - loss: 0.1661 - accuracy: 0.9570\n",
      "10000/10000 - 0s - loss: 0.1800 - accuracy: 0.9593\n",
      "10000/10000 - 0s - loss: 0.1558 - accuracy: 0.9583\n",
      "10000/10000 - 0s - loss: 0.2179 - accuracy: 0.9368\n",
      "10000/10000 - 0s - loss: 0.3158 - accuracy: 0.8946\n",
      "10000/10000 - 0s - loss: 0.1767 - accuracy: 0.9558\n",
      "10000/10000 - 0s - loss: 0.2059 - accuracy: 0.9440\n",
      "10000/10000 - 0s - loss: 0.2394 - accuracy: 0.9293\n",
      "10000/10000 - 0s - loss: 0.2147 - accuracy: 0.9418\n",
      "10000/10000 - 0s - loss: 0.1544 - accuracy: 0.9621\n",
      "10000/10000 - 0s - loss: 0.1920 - accuracy: 0.9490\n",
      "10000/10000 - 0s - loss: 0.2041 - accuracy: 0.9406\n",
      "10000/10000 - 0s - loss: 0.1649 - accuracy: 0.9603\n",
      "10000/10000 - 0s - loss: 0.2175 - accuracy: 0.9366\n",
      "10000/10000 - 0s - loss: 0.1604 - accuracy: 0.9547\n",
      "10000/10000 - 0s - loss: 0.1750 - accuracy: 0.9520\n",
      "10000/10000 - 0s - loss: 0.1821 - accuracy: 0.9559\n",
      "10000/10000 - 0s - loss: 0.2349 - accuracy: 0.9261\n",
      "10000/10000 - 0s - loss: 0.1663 - accuracy: 0.9582\n",
      "10000/10000 - 0s - loss: 0.2016 - accuracy: 0.9425\n",
      "10000/10000 - 0s - loss: 0.1692 - accuracy: 0.9533\n",
      "10000/10000 - 0s - loss: 0.1649 - accuracy: 0.9557\n",
      "10000/10000 - 0s - loss: 0.1640 - accuracy: 0.9599\n",
      "10000/10000 - 0s - loss: 0.1551 - accuracy: 0.9590\n",
      "10000/10000 - 0s - loss: 0.1696 - accuracy: 0.9573\n",
      "10000/10000 - 0s - loss: 0.1645 - accuracy: 0.9545\n",
      "10000/10000 - 0s - loss: 0.1740 - accuracy: 0.9565\n",
      "10000/10000 - 0s - loss: 0.1824 - accuracy: 0.9479\n",
      "10000/10000 - 0s - loss: 0.1784 - accuracy: 0.9539\n",
      "10000/10000 - 0s - loss: 0.2158 - accuracy: 0.9450\n",
      "10000/10000 - 0s - loss: 0.1749 - accuracy: 0.9544\n",
      "10000/10000 - 0s - loss: 0.1993 - accuracy: 0.9388\n",
      "10000/10000 - 0s - loss: 0.1999 - accuracy: 0.9500\n",
      "10000/10000 - 0s - loss: 0.2152 - accuracy: 0.9432\n",
      "10000/10000 - 0s - loss: 0.1748 - accuracy: 0.9518\n",
      "10000/10000 - 0s - loss: 0.2109 - accuracy: 0.9386\n",
      "10000/10000 - 0s - loss: 0.1530 - accuracy: 0.9651\n",
      "10000/10000 - 0s - loss: 0.1817 - accuracy: 0.9496\n",
      "10000/10000 - 0s - loss: 0.1756 - accuracy: 0.9560\n",
      "10000/10000 - 0s - loss: 0.1715 - accuracy: 0.9607\n",
      "10000/10000 - 0s - loss: 0.2015 - accuracy: 0.9489\n",
      "10000/10000 - 0s - loss: 0.1607 - accuracy: 0.9596\n",
      "10000/10000 - 0s - loss: 0.1855 - accuracy: 0.9505\n",
      "10000/10000 - 0s - loss: 0.2367 - accuracy: 0.9285\n",
      "10000/10000 - 0s - loss: 0.2437 - accuracy: 0.9229\n",
      "10000/10000 - 0s - loss: 0.2041 - accuracy: 0.9516\n",
      "10000/10000 - 0s - loss: 0.2242 - accuracy: 0.9469\n",
      "10000/10000 - 0s - loss: 0.1676 - accuracy: 0.9585\n",
      "10000/10000 - 0s - loss: 0.2109 - accuracy: 0.9515\n",
      "10000/10000 - 0s - loss: 0.1628 - accuracy: 0.9583\n",
      "10000/10000 - 0s - loss: 0.1900 - accuracy: 0.9529\n",
      "10000/10000 - 0s - loss: 0.1937 - accuracy: 0.9525\n",
      "10000/10000 - 0s - loss: 0.1879 - accuracy: 0.9529\n",
      "10000/10000 - 0s - loss: 0.1758 - accuracy: 0.9590\n",
      "10000/10000 - 0s - loss: 0.2312 - accuracy: 0.9371\n",
      "10000/10000 - 0s - loss: 0.2076 - accuracy: 0.9457\n",
      "10000/10000 - 0s - loss: 0.2086 - accuracy: 0.9407\n",
      "10000/10000 - 0s - loss: 0.2249 - accuracy: 0.9320\n",
      "10000/10000 - 0s - loss: 0.2067 - accuracy: 0.9473\n",
      "10000/10000 - 0s - loss: 0.2554 - accuracy: 0.9277\n",
      "10000/10000 - 0s - loss: 0.1578 - accuracy: 0.9571\n",
      "10000/10000 - 0s - loss: 0.1932 - accuracy: 0.9445\n",
      "10000/10000 - 0s - loss: 0.1692 - accuracy: 0.9587\n",
      "10000/10000 - 0s - loss: 0.1922 - accuracy: 0.9452\n",
      "10000/10000 - 0s - loss: 0.2163 - accuracy: 0.9422\n",
      "10000/10000 - 0s - loss: 0.1946 - accuracy: 0.9544\n",
      "10000/10000 - 0s - loss: 0.1868 - accuracy: 0.9532\n",
      "10000/10000 - 0s - loss: 0.1917 - accuracy: 0.9535\n",
      "10000/10000 - 0s - loss: 0.1641 - accuracy: 0.9629\n",
      "10000/10000 - 0s - loss: 0.1643 - accuracy: 0.9585\n",
      "10000/10000 - 0s - loss: 0.2733 - accuracy: 0.9163\n",
      "10000/10000 - 0s - loss: 0.2101 - accuracy: 0.9466\n",
      "10000/10000 - 0s - loss: 0.1710 - accuracy: 0.9538\n",
      "10000/10000 - 0s - loss: 0.2134 - accuracy: 0.9418\n",
      "10000/10000 - 0s - loss: 0.1931 - accuracy: 0.9465\n",
      "10000/10000 - 0s - loss: 0.1645 - accuracy: 0.9588\n",
      "10000/10000 - 0s - loss: 0.1899 - accuracy: 0.9543\n",
      "10000/10000 - 0s - loss: 0.1799 - accuracy: 0.9499\n",
      "10000/10000 - 0s - loss: 0.2181 - accuracy: 0.9452\n",
      "10000/10000 - 0s - loss: 0.2194 - accuracy: 0.9389\n",
      "10000/10000 - 0s - loss: 0.1623 - accuracy: 0.9612\n",
      "10000/10000 - 0s - loss: 0.2002 - accuracy: 0.9511\n",
      "10000/10000 - 0s - loss: 0.1738 - accuracy: 0.9558\n",
      "10000/10000 - 0s - loss: 0.1923 - accuracy: 0.9481\n",
      "10000/10000 - 0s - loss: 0.2567 - accuracy: 0.9312\n",
      "10000/10000 - 0s - loss: 0.1988 - accuracy: 0.9477\n",
      "10000/10000 - 0s - loss: 0.1898 - accuracy: 0.9522\n",
      "10000/10000 - 0s - loss: 0.1848 - accuracy: 0.9460\n",
      "10000/10000 - 0s - loss: 0.2023 - accuracy: 0.9474\n",
      "10000/10000 - 0s - loss: 0.2066 - accuracy: 0.9479\n",
      "10000/10000 - 0s - loss: 0.2079 - accuracy: 0.9444\n",
      "10000/10000 - 0s - loss: 0.1920 - accuracy: 0.9513\n",
      "10000/10000 - 0s - loss: 0.1715 - accuracy: 0.9565\n",
      "10000/10000 - 0s - loss: 0.1616 - accuracy: 0.9601\n",
      "10000/10000 - 0s - loss: 0.2302 - accuracy: 0.9383\n",
      "10000/10000 - 0s - loss: 0.1688 - accuracy: 0.9569\n",
      "10000/10000 - 0s - loss: 0.2233 - accuracy: 0.9459\n",
      "10000/10000 - 0s - loss: 0.2395 - accuracy: 0.9311\n",
      "10000/10000 - 0s - loss: 0.2330 - accuracy: 0.9394\n",
      "10000/10000 - 0s - loss: 0.2149 - accuracy: 0.9474\n",
      "10000/10000 - 0s - loss: 0.2151 - accuracy: 0.9332\n",
      "10000/10000 - 0s - loss: 0.2113 - accuracy: 0.9331\n",
      "10000/10000 - 0s - loss: 0.2009 - accuracy: 0.9474\n",
      "10000/10000 - 0s - loss: 0.2132 - accuracy: 0.9440\n",
      "10000/10000 - 0s - loss: 0.1973 - accuracy: 0.9502\n",
      "10000/10000 - 0s - loss: 0.1912 - accuracy: 0.9499\n",
      "10000/10000 - 0s - loss: 0.2194 - accuracy: 0.9415\n",
      "10000/10000 - 0s - loss: 0.1993 - accuracy: 0.9432\n",
      "10000/10000 - 0s - loss: 0.1892 - accuracy: 0.9550\n",
      "10000/10000 - 0s - loss: 0.1985 - accuracy: 0.9490\n",
      "10000/10000 - 0s - loss: 0.1829 - accuracy: 0.9546\n",
      "10000/10000 - 0s - loss: 0.2009 - accuracy: 0.9481\n",
      "10000/10000 - 0s - loss: 0.2005 - accuracy: 0.9539\n",
      "10000/10000 - 0s - loss: 0.1789 - accuracy: 0.9595\n",
      "10000/10000 - 0s - loss: 0.1995 - accuracy: 0.9540\n",
      "10000/10000 - 0s - loss: 0.1985 - accuracy: 0.9515\n",
      "10000/10000 - 0s - loss: 0.1908 - accuracy: 0.9505\n",
      "10000/10000 - 0s - loss: 0.1860 - accuracy: 0.9532\n",
      "10000/10000 - 0s - loss: 0.2025 - accuracy: 0.9485\n",
      "10000/10000 - 0s - loss: 0.2038 - accuracy: 0.9516\n",
      "10000/10000 - 0s - loss: 0.2354 - accuracy: 0.9403\n",
      "10000/10000 - 0s - loss: 0.1972 - accuracy: 0.9546\n",
      "10000/10000 - 0s - loss: 0.1604 - accuracy: 0.9627\n",
      "10000/10000 - 0s - loss: 0.2028 - accuracy: 0.9508\n",
      "10000/10000 - 0s - loss: 0.3072 - accuracy: 0.9033\n",
      "10000/10000 - 0s - loss: 0.1926 - accuracy: 0.9515\n",
      "10000/10000 - 0s - loss: 0.1914 - accuracy: 0.9469\n",
      "10000/10000 - 0s - loss: 0.2266 - accuracy: 0.9436\n",
      "10000/10000 - 0s - loss: 0.1683 - accuracy: 0.9597\n",
      "10000/10000 - 0s - loss: 0.2228 - accuracy: 0.9469\n",
      "10000/10000 - 0s - loss: 0.2318 - accuracy: 0.9365\n",
      "10000/10000 - 0s - loss: 0.2178 - accuracy: 0.9406\n",
      "10000/10000 - 0s - loss: 0.2348 - accuracy: 0.9305\n",
      "10000/10000 - 0s - loss: 0.1959 - accuracy: 0.9557\n",
      "10000/10000 - 0s - loss: 0.1729 - accuracy: 0.9581\n",
      "10000/10000 - 0s - loss: 0.1692 - accuracy: 0.9585\n",
      "10000/10000 - 0s - loss: 0.1804 - accuracy: 0.9573\n",
      "10000/10000 - 0s - loss: 0.2029 - accuracy: 0.9431\n",
      "10000/10000 - 0s - loss: 0.2283 - accuracy: 0.9398\n",
      "10000/10000 - 0s - loss: 0.3061 - accuracy: 0.9099\n",
      "10000/10000 - 0s - loss: 0.2732 - accuracy: 0.9117\n",
      "10000/10000 - 0s - loss: 0.2183 - accuracy: 0.9403\n",
      "10000/10000 - 0s - loss: 0.1986 - accuracy: 0.9548\n",
      "10000/10000 - 0s - loss: 0.2028 - accuracy: 0.9487\n",
      "10000/10000 - 0s - loss: 0.2352 - accuracy: 0.9327\n",
      "10000/10000 - 0s - loss: 0.2209 - accuracy: 0.9380\n",
      "10000/10000 - 0s - loss: 0.2475 - accuracy: 0.9262\n",
      "10000/10000 - 0s - loss: 0.2070 - accuracy: 0.9458\n",
      "10000/10000 - 0s - loss: 0.2331 - accuracy: 0.9425\n",
      "10000/10000 - 0s - loss: 0.2290 - accuracy: 0.9390\n",
      "10000/10000 - 0s - loss: 0.2420 - accuracy: 0.9388\n",
      "10000/10000 - 0s - loss: 0.1949 - accuracy: 0.9495\n",
      "10000/10000 - 0s - loss: 0.2494 - accuracy: 0.9251\n",
      "10000/10000 - 0s - loss: 0.2341 - accuracy: 0.9352\n",
      "10000/10000 - 0s - loss: 0.2009 - accuracy: 0.9498\n",
      "10000/10000 - 0s - loss: 0.2311 - accuracy: 0.9428\n",
      "10000/10000 - 0s - loss: 0.1754 - accuracy: 0.9575\n",
      "10000/10000 - 0s - loss: 0.1885 - accuracy: 0.9504\n",
      "10000/10000 - 0s - loss: 0.2651 - accuracy: 0.9279\n",
      "10000/10000 - 0s - loss: 0.3239 - accuracy: 0.8974\n",
      "10000/10000 - 0s - loss: 0.2283 - accuracy: 0.9417\n",
      "10000/10000 - 0s - loss: 0.1919 - accuracy: 0.9531\n",
      "10000/10000 - 0s - loss: 0.1651 - accuracy: 0.9563\n",
      "10000/10000 - 0s - loss: 0.2414 - accuracy: 0.9349\n",
      "10000/10000 - 0s - loss: 0.1867 - accuracy: 0.9562\n",
      "10000/10000 - 0s - loss: 0.4172 - accuracy: 0.8556\n",
      "10000/10000 - 0s - loss: 0.1915 - accuracy: 0.9491\n",
      "10000/10000 - 0s - loss: 0.2412 - accuracy: 0.9289\n",
      "10000/10000 - 0s - loss: 0.2267 - accuracy: 0.9418\n",
      "10000/10000 - 0s - loss: 0.2633 - accuracy: 0.9245\n",
      "10000/10000 - 0s - loss: 0.1776 - accuracy: 0.9548\n",
      "10000/10000 - 0s - loss: 0.3252 - accuracy: 0.9085\n",
      "10000/10000 - 0s - loss: 0.1896 - accuracy: 0.9487\n",
      "10000/10000 - 0s - loss: 0.2015 - accuracy: 0.9515\n",
      "10000/10000 - 0s - loss: 0.1788 - accuracy: 0.9585\n",
      "10000/10000 - 0s - loss: 0.2120 - accuracy: 0.9454\n",
      "10000/10000 - 0s - loss: 0.2056 - accuracy: 0.9468\n",
      "10000/10000 - 0s - loss: 0.2174 - accuracy: 0.9404\n",
      "10000/10000 - 0s - loss: 0.2076 - accuracy: 0.9466\n",
      "10000/10000 - 0s - loss: 0.2681 - accuracy: 0.9250\n",
      "10000/10000 - 0s - loss: 0.2095 - accuracy: 0.9526\n",
      "10000/10000 - 0s - loss: 0.2121 - accuracy: 0.9445\n",
      "10000/10000 - 0s - loss: 0.2466 - accuracy: 0.9330\n",
      "10000/10000 - 0s - loss: 0.2378 - accuracy: 0.9395\n",
      "10000/10000 - 0s - loss: 0.2301 - accuracy: 0.9430\n",
      "10000/10000 - 0s - loss: 0.2897 - accuracy: 0.9122\n",
      "10000/10000 - 0s - loss: 0.2451 - accuracy: 0.9400\n",
      "10000/10000 - 0s - loss: 0.1954 - accuracy: 0.9486\n",
      "10000/10000 - 0s - loss: 0.2113 - accuracy: 0.9461\n",
      "10000/10000 - 0s - loss: 0.2738 - accuracy: 0.9145\n",
      "10000/10000 - 0s - loss: 0.2609 - accuracy: 0.9221\n",
      "10000/10000 - 0s - loss: 0.2093 - accuracy: 0.9550\n",
      "10000/10000 - 0s - loss: 0.2288 - accuracy: 0.9490\n",
      "10000/10000 - 0s - loss: 0.3095 - accuracy: 0.9109\n",
      "10000/10000 - 0s - loss: 0.2195 - accuracy: 0.9446\n",
      "10000/10000 - 0s - loss: 0.2028 - accuracy: 0.9443\n",
      "10000/10000 - 0s - loss: 0.1802 - accuracy: 0.9582\n",
      "10000/10000 - 0s - loss: 0.2560 - accuracy: 0.9322\n",
      "10000/10000 - 0s - loss: 0.2754 - accuracy: 0.9290\n",
      "10000/10000 - 0s - loss: 0.2429 - accuracy: 0.9421\n",
      "10000/10000 - 0s - loss: 0.2202 - accuracy: 0.9400\n",
      "10000/10000 - 0s - loss: 0.2004 - accuracy: 0.9533\n",
      "10000/10000 - 0s - loss: 0.2417 - accuracy: 0.9389\n",
      "10000/10000 - 0s - loss: 0.1982 - accuracy: 0.9539\n",
      "10000/10000 - 0s - loss: 0.2022 - accuracy: 0.9504\n",
      "10000/10000 - 0s - loss: 0.2577 - accuracy: 0.9262\n",
      "10000/10000 - 0s - loss: 0.2048 - accuracy: 0.9517\n",
      "10000/10000 - 0s - loss: 0.2240 - accuracy: 0.9424\n",
      "10000/10000 - 0s - loss: 0.2137 - accuracy: 0.9431\n",
      "10000/10000 - 0s - loss: 0.2409 - accuracy: 0.9377\n",
      "10000/10000 - 0s - loss: 0.2360 - accuracy: 0.9340\n",
      "10000/10000 - 0s - loss: 0.2062 - accuracy: 0.9491\n",
      "10000/10000 - 0s - loss: 0.2171 - accuracy: 0.9406\n",
      "10000/10000 - 0s - loss: 0.2125 - accuracy: 0.9510\n",
      "10000/10000 - 0s - loss: 0.2384 - accuracy: 0.9348\n",
      "10000/10000 - 0s - loss: 0.2448 - accuracy: 0.9475\n",
      "10000/10000 - 0s - loss: 0.2059 - accuracy: 0.9566\n",
      "10000/10000 - 0s - loss: 0.2099 - accuracy: 0.9484\n",
      "10000/10000 - 0s - loss: 0.2599 - accuracy: 0.9322\n",
      "10000/10000 - 0s - loss: 0.2211 - accuracy: 0.9489\n",
      "10000/10000 - 0s - loss: 0.3605 - accuracy: 0.8785\n",
      "10000/10000 - 0s - loss: 0.1957 - accuracy: 0.9550\n",
      "10000/10000 - 0s - loss: 0.1739 - accuracy: 0.9571\n",
      "10000/10000 - 0s - loss: 0.2032 - accuracy: 0.9489\n",
      "10000/10000 - 0s - loss: 0.2474 - accuracy: 0.9372\n",
      "10000/10000 - 0s - loss: 0.2199 - accuracy: 0.9443\n",
      "10000/10000 - 0s - loss: 0.2538 - accuracy: 0.9370\n",
      "10000/10000 - 0s - loss: 0.2074 - accuracy: 0.9546\n",
      "10000/10000 - 0s - loss: 0.2171 - accuracy: 0.9529\n",
      "10000/10000 - 0s - loss: 0.2419 - accuracy: 0.9378\n",
      "10000/10000 - 0s - loss: 0.2547 - accuracy: 0.9346\n",
      "10000/10000 - 0s - loss: 0.2395 - accuracy: 0.9402\n",
      "10000/10000 - 0s - loss: 0.2366 - accuracy: 0.9457\n",
      "10000/10000 - 0s - loss: 0.2364 - accuracy: 0.9373\n",
      "10000/10000 - 0s - loss: 0.2264 - accuracy: 0.9440\n",
      "10000/10000 - 0s - loss: 0.2443 - accuracy: 0.9356\n",
      "10000/10000 - 0s - loss: 0.2810 - accuracy: 0.9248\n",
      "10000/10000 - 0s - loss: 0.2339 - accuracy: 0.9467\n",
      "10000/10000 - 0s - loss: 0.3319 - accuracy: 0.8941\n",
      "10000/10000 - 0s - loss: 0.2269 - accuracy: 0.9379\n",
      "10000/10000 - 0s - loss: 0.2073 - accuracy: 0.9447\n",
      "10000/10000 - 0s - loss: 0.2678 - accuracy: 0.9313\n",
      "10000/10000 - 0s - loss: 0.2462 - accuracy: 0.9427\n",
      "10000/10000 - 0s - loss: 0.3226 - accuracy: 0.9094\n",
      "10000/10000 - 0s - loss: 0.2220 - accuracy: 0.9492\n",
      "10000/10000 - 0s - loss: 0.2108 - accuracy: 0.9488\n",
      "10000/10000 - 0s - loss: 0.2423 - accuracy: 0.9400\n",
      "10000/10000 - 0s - loss: 0.2744 - accuracy: 0.9280\n",
      "10000/10000 - 0s - loss: 0.2514 - accuracy: 0.9478\n",
      "10000/10000 - 0s - loss: 0.2589 - accuracy: 0.9295\n",
      "10000/10000 - 0s - loss: 0.2481 - accuracy: 0.9306\n",
      "10000/10000 - 0s - loss: 0.2084 - accuracy: 0.9483\n",
      "10000/10000 - 0s - loss: 0.2636 - accuracy: 0.9225\n",
      "10000/10000 - 0s - loss: 0.2780 - accuracy: 0.9231\n",
      "10000/10000 - 0s - loss: 0.2336 - accuracy: 0.9399\n",
      "10000/10000 - 0s - loss: 0.2614 - accuracy: 0.9405\n",
      "10000/10000 - 0s - loss: 0.2163 - accuracy: 0.9461\n",
      "10000/10000 - 0s - loss: 0.2205 - accuracy: 0.9522\n",
      "10000/10000 - 0s - loss: 0.3445 - accuracy: 0.9008\n",
      "10000/10000 - 0s - loss: 0.2681 - accuracy: 0.9258\n",
      "10000/10000 - 0s - loss: 0.2998 - accuracy: 0.9227\n",
      "10000/10000 - 0s - loss: 0.2636 - accuracy: 0.9274\n",
      "10000/10000 - 0s - loss: 0.2490 - accuracy: 0.9344\n",
      "10000/10000 - 0s - loss: 0.2138 - accuracy: 0.9455\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_model = copy.deepcopy(old)\n",
    "to_test = 25\n",
    "for i in range(1, 65):\n",
    "    temp_model, temp_score = remove_nodes(acc, loss, old, i, to_test)\n",
    "    if temp_score < best_score:\n",
    "        best_model = temp_model\n",
    "        best_score = temp_score\n",
    "        print(\"Found new best model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new restricted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.0666 - accuracy: 0.9799\n",
      "10000/10000 - 0s - loss: 0.0670 - accuracy: 0.9804\n",
      "10000/10000 - 0s - loss: 0.0660 - accuracy: 0.9798\n",
      "10000/10000 - 0s - loss: 0.0656 - accuracy: 0.9798\n",
      "10000/10000 - 0s - loss: 0.0661 - accuracy: 0.9800\n",
      "10000/10000 - 0s - loss: 0.0668 - accuracy: 0.9797\n",
      "10000/10000 - 0s - loss: 0.0692 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0647 - accuracy: 0.9802\n",
      "10000/10000 - 0s - loss: 0.0669 - accuracy: 0.9802\n",
      "10000/10000 - 0s - loss: 0.0663 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0653 - accuracy: 0.9806\n",
      "10000/10000 - 0s - loss: 0.0669 - accuracy: 0.9796\n",
      "10000/10000 - 0s - loss: 0.0655 - accuracy: 0.9799\n",
      "10000/10000 - 0s - loss: 0.0665 - accuracy: 0.9801\n",
      "10000/10000 - 0s - loss: 0.0675 - accuracy: 0.9796\n",
      "10000/10000 - 0s - loss: 0.0661 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0659 - accuracy: 0.9801\n",
      "10000/10000 - 0s - loss: 0.0667 - accuracy: 0.9797\n",
      "10000/10000 - 0s - loss: 0.0661 - accuracy: 0.9802\n",
      "10000/10000 - 0s - loss: 0.0649 - accuracy: 0.9808\n",
      "10000/10000 - 0s - loss: 0.0662 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0656 - accuracy: 0.9799\n",
      "10000/10000 - 0s - loss: 0.0667 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0674 - accuracy: 0.9797\n",
      "10000/10000 - 0s - loss: 0.0650 - accuracy: 0.9800\n",
      "10000/10000 - 0s - loss: 0.0656 - accuracy: 0.9797\n",
      "10000/10000 - 0s - loss: 0.0657 - accuracy: 0.9797\n",
      "10000/10000 - 0s - loss: 0.0650 - accuracy: 0.9795\n",
      "10000/10000 - 0s - loss: 0.0654 - accuracy: 0.9800\n",
      "10000/10000 - 0s - loss: 0.0644 - accuracy: 0.9791\n",
      "10000/10000 - 0s - loss: 0.0662 - accuracy: 0.9802\n",
      "10000/10000 - 0s - loss: 0.0663 - accuracy: 0.9793\n",
      "10000/10000 - 0s - loss: 0.0660 - accuracy: 0.9802\n",
      "10000/10000 - 0s - loss: 0.0659 - accuracy: 0.9800\n",
      "10000/10000 - 0s - loss: 0.0669 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0643 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0679 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0658 - accuracy: 0.9800\n",
      "10000/10000 - 0s - loss: 0.0671 - accuracy: 0.9803\n",
      "10000/10000 - 0s - loss: 0.0665 - accuracy: 0.9803\n",
      "10000/10000 - 0s - loss: 0.0670 - accuracy: 0.9800\n",
      "10000/10000 - 0s - loss: 0.0668 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0661 - accuracy: 0.9800\n",
      "10000/10000 - 0s - loss: 0.0667 - accuracy: 0.9796\n",
      "10000/10000 - 0s - loss: 0.0680 - accuracy: 0.9790\n",
      "10000/10000 - 0s - loss: 0.0649 - accuracy: 0.9804\n",
      "10000/10000 - 0s - loss: 0.0669 - accuracy: 0.9798\n",
      "10000/10000 - 0s - loss: 0.0662 - accuracy: 0.9803\n",
      "10000/10000 - 0s - loss: 0.0654 - accuracy: 0.9794\n",
      "10000/10000 - 0s - loss: 0.0656 - accuracy: 0.9797\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "best_weights, _, nodes_removed = remove_nodes(acc, loss, old, n, 50)\n",
    "\n",
    "new_weights = [np.zeros((best_weights[0].shape[0], best_weights[0].shape[1] - n)), np.zeros((best_weights[1].shape[0] - n)), np.zeros((best_weights[2].shape[0] - n, best_weights[2].shape[1])), best_weights[3]]\n",
    "\n",
    "j = 0\n",
    "for i in range(len(best_weights[1])):\n",
    "    if i not in nodes_removed:\n",
    "        new_weights[0][:, j] = best_weights[0][:, i]\n",
    "        new_weights[1][j] = best_weights[1][i]\n",
    "        new_weights[2][j, :] = best_weights[2][i, :]\n",
    "        j = j + 1\n",
    "    \n",
    "new_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128 - n, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "new_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_model(model, shrinkage_factor, x_train, y_train, size, to_test, v=0):\n",
    "    \n",
    "    n = shrinkage_factor\n",
    "    loss, acc = model.evaluate(x_train, y_train, verbose=2)\n",
    "    old = model.get_weights()\n",
    "    best_weights, _, nodes_removed = remove_nodes(acc, loss, old, n, to_test, x_train, y_train, v)\n",
    "\n",
    "    new_weights = [np.zeros((best_weights[0].shape[0], best_weights[0].shape[1] - n)), np.zeros((best_weights[1].shape[0] - n)), np.zeros((best_weights[2].shape[0] - n, best_weights[2].shape[1])), best_weights[3]]\n",
    "\n",
    "    j = 0\n",
    "    for i in range(len(best_weights[1])):\n",
    "        if i not in nodes_removed:\n",
    "            new_weights[0][:, j] = best_weights[0][:, i]\n",
    "            new_weights[1][j] = best_weights[1][i]\n",
    "            new_weights[2][j, :] = best_weights[2][i, :]\n",
    "            j = j + 1\n",
    "\n",
    "    new_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(size - n, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    new_model.set_weights(new_weights)\n",
    "    return new_model, size-n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.0643 - accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06433191793382867, 0.9794]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0429 - accuracy: 0.9860\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0383 - accuracy: 0.9873\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0356 - accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a950764408>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.0758 - accuracy: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07577427882198827, 0.9802]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2990 - accuracy: 0.9146\n",
      "60000/60000 - 2s - loss: 0.1337 - accuracy: 0.9624\n",
      "60000/60000 - 2s - loss: 0.1440 - accuracy: 0.9610\n",
      "60000/60000 - 2s - loss: 0.1446 - accuracy: 0.9606\n",
      "60000/60000 - 2s - loss: 0.1446 - accuracy: 0.9596\n",
      "60000/60000 - 2s - loss: 0.1464 - accuracy: 0.9599\n",
      "60000/60000 - 2s - loss: 0.1412 - accuracy: 0.9625\n",
      "60000/60000 - 2s - loss: 0.1460 - accuracy: 0.9597\n",
      "60000/60000 - 2s - loss: 0.1465 - accuracy: 0.9599\n",
      "60000/60000 - 2s - loss: 0.1433 - accuracy: 0.9606\n",
      "60000/60000 - 2s - loss: 0.1484 - accuracy: 0.9600\n",
      "60000/60000 - 2s - loss: 0.1432 - accuracy: 0.9612\n",
      "60000/60000 - 2s - loss: 0.1429 - accuracy: 0.9603\n",
      "60000/60000 - 2s - loss: 0.1427 - accuracy: 0.9614\n",
      "60000/60000 - 2s - loss: 0.1453 - accuracy: 0.9600\n",
      "60000/60000 - 2s - loss: 0.1454 - accuracy: 0.9611\n",
      "60000/60000 - 2s - loss: 0.1398 - accuracy: 0.9624\n",
      "60000/60000 - 2s - loss: 0.1470 - accuracy: 0.9610\n",
      "60000/60000 - 2s - loss: 0.1482 - accuracy: 0.9590\n",
      "60000/60000 - 2s - loss: 0.1446 - accuracy: 0.9614\n",
      "60000/60000 - 2s - loss: 0.1432 - accuracy: 0.9601\n",
      "60000/60000 - 3s - loss: 0.1468 - accuracy: 0.9597\n",
      "60000/60000 - 2s - loss: 0.1470 - accuracy: 0.9586\n",
      "60000/60000 - 2s - loss: 0.1425 - accuracy: 0.9618\n",
      "60000/60000 - 2s - loss: 0.1419 - accuracy: 0.9609\n",
      "60000/60000 - 2s - loss: 0.1404 - accuracy: 0.9618\n",
      "60000/60000 - 2s - loss: 0.1404 - accuracy: 0.9614\n",
      "120\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.1496 - accuracy: 0.9562\n",
      "60000/60000 - 2s - loss: 0.0879 - accuracy: 0.9739\n",
      "60000/60000 - 2s - loss: 0.0983 - accuracy: 0.9715\n",
      "60000/60000 - 2s - loss: 0.0971 - accuracy: 0.9719\n",
      "60000/60000 - 2s - loss: 0.0966 - accuracy: 0.9717\n",
      "60000/60000 - 2s - loss: 0.0930 - accuracy: 0.9732\n",
      "60000/60000 - 2s - loss: 0.1021 - accuracy: 0.9697\n",
      "60000/60000 - 2s - loss: 0.0954 - accuracy: 0.9724\n",
      "60000/60000 - 2s - loss: 0.0926 - accuracy: 0.9731\n",
      "60000/60000 - 2s - loss: 0.0948 - accuracy: 0.9722\n",
      "60000/60000 - 2s - loss: 0.0970 - accuracy: 0.9718\n",
      "60000/60000 - 2s - loss: 0.0983 - accuracy: 0.9707\n",
      "60000/60000 - 2s - loss: 0.0926 - accuracy: 0.9734\n",
      "60000/60000 - 2s - loss: 0.1031 - accuracy: 0.9700\n",
      "60000/60000 - 2s - loss: 0.0969 - accuracy: 0.9721\n",
      "60000/60000 - 3s - loss: 0.1034 - accuracy: 0.9701\n",
      "60000/60000 - 2s - loss: 0.0930 - accuracy: 0.9729\n",
      "60000/60000 - 2s - loss: 0.0978 - accuracy: 0.9714\n",
      "60000/60000 - 3s - loss: 0.0962 - accuracy: 0.9723\n",
      "60000/60000 - 2s - loss: 0.0974 - accuracy: 0.9713\n",
      "60000/60000 - 2s - loss: 0.1023 - accuracy: 0.9699\n",
      "60000/60000 - 2s - loss: 0.0971 - accuracy: 0.9717\n",
      "60000/60000 - 2s - loss: 0.0962 - accuracy: 0.9718\n",
      "60000/60000 - 2s - loss: 0.0974 - accuracy: 0.9715\n",
      "60000/60000 - 2s - loss: 0.0986 - accuracy: 0.9716\n",
      "60000/60000 - 2s - loss: 0.0951 - accuracy: 0.9720\n",
      "60000/60000 - 2s - loss: 0.0973 - accuracy: 0.9722\n",
      "112\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1183 - accuracy: 0.9641\n",
      "60000/60000 - 2s - loss: 0.0652 - accuracy: 0.9808\n",
      "60000/60000 - 2s - loss: 0.0747 - accuracy: 0.9783\n",
      "60000/60000 - 2s - loss: 0.0740 - accuracy: 0.9788\n",
      "60000/60000 - 2s - loss: 0.0789 - accuracy: 0.9773\n",
      "60000/60000 - 2s - loss: 0.0778 - accuracy: 0.9778\n",
      "60000/60000 - 2s - loss: 0.0755 - accuracy: 0.9776\n",
      "60000/60000 - 2s - loss: 0.0765 - accuracy: 0.9780\n",
      "60000/60000 - 2s - loss: 0.0732 - accuracy: 0.9789\n",
      "60000/60000 - 2s - loss: 0.0724 - accuracy: 0.9792\n",
      "60000/60000 - 2s - loss: 0.0738 - accuracy: 0.9791\n",
      "60000/60000 - 2s - loss: 0.0718 - accuracy: 0.9794\n",
      "60000/60000 - 2s - loss: 0.0762 - accuracy: 0.9786\n",
      "60000/60000 - 2s - loss: 0.0757 - accuracy: 0.9777\n",
      "60000/60000 - 2s - loss: 0.0789 - accuracy: 0.9771\n",
      "60000/60000 - 2s - loss: 0.0738 - accuracy: 0.9779\n",
      "60000/60000 - 2s - loss: 0.0728 - accuracy: 0.9788\n",
      "60000/60000 - 2s - loss: 0.0753 - accuracy: 0.9784\n",
      "60000/60000 - 2s - loss: 0.0806 - accuracy: 0.9767\n",
      "60000/60000 - 2s - loss: 0.0773 - accuracy: 0.9775\n",
      "60000/60000 - 2s - loss: 0.0758 - accuracy: 0.9783\n",
      "60000/60000 - 2s - loss: 0.0751 - accuracy: 0.9775\n",
      "60000/60000 - 2s - loss: 0.0720 - accuracy: 0.9789\n",
      "60000/60000 - 2s - loss: 0.0773 - accuracy: 0.9770\n",
      "60000/60000 - 2s - loss: 0.0716 - accuracy: 0.9790\n",
      "60000/60000 - 2s - loss: 0.0731 - accuracy: 0.9787\n",
      "60000/60000 - 3s - loss: 0.0762 - accuracy: 0.9780\n",
      "104\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1007 - accuracy: 0.9696\n",
      "60000/60000 - 3s - loss: 0.0590 - accuracy: 0.9826\n",
      "60000/60000 - 3s - loss: 0.0672 - accuracy: 0.9808\n",
      "60000/60000 - 3s - loss: 0.0698 - accuracy: 0.9801\n",
      "60000/60000 - 2s - loss: 0.0675 - accuracy: 0.9811\n",
      "60000/60000 - 3s - loss: 0.0750 - accuracy: 0.9790\n",
      "60000/60000 - 3s - loss: 0.0696 - accuracy: 0.9804\n",
      "60000/60000 - 3s - loss: 0.0694 - accuracy: 0.9805\n",
      "60000/60000 - 2s - loss: 0.0694 - accuracy: 0.9806\n",
      "60000/60000 - 2s - loss: 0.0708 - accuracy: 0.9794\n",
      "60000/60000 - 3s - loss: 0.0634 - accuracy: 0.9822\n",
      "60000/60000 - 3s - loss: 0.0703 - accuracy: 0.9807\n",
      "60000/60000 - 3s - loss: 0.0812 - accuracy: 0.9766\n",
      "60000/60000 - 3s - loss: 0.0673 - accuracy: 0.9815\n",
      "60000/60000 - 3s - loss: 0.0680 - accuracy: 0.9804\n",
      "60000/60000 - 2s - loss: 0.0705 - accuracy: 0.9793\n",
      "60000/60000 - 2s - loss: 0.0667 - accuracy: 0.9812\n",
      "60000/60000 - 2s - loss: 0.0717 - accuracy: 0.9793\n",
      "60000/60000 - 2s - loss: 0.0677 - accuracy: 0.9810\n",
      "60000/60000 - 2s - loss: 0.0719 - accuracy: 0.9800\n",
      "60000/60000 - 2s - loss: 0.0728 - accuracy: 0.9794\n",
      "60000/60000 - 2s - loss: 0.0682 - accuracy: 0.9808\n",
      "60000/60000 - 2s - loss: 0.0684 - accuracy: 0.9809\n",
      "60000/60000 - 2s - loss: 0.0668 - accuracy: 0.9813\n",
      "60000/60000 - 2s - loss: 0.0747 - accuracy: 0.9790\n",
      "60000/60000 - 2s - loss: 0.0782 - accuracy: 0.9769\n",
      "60000/60000 - 2s - loss: 0.0664 - accuracy: 0.9809\n",
      "96\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0935 - accuracy: 0.9708\n",
      "10000/10000 - 0s - loss: 0.0837 - accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08368590022437275, 0.975]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "size = 128\n",
    "to_test = 25\n",
    "for _ in range(4):\n",
    "    model.fit(x_train, y_train, epochs=1)\n",
    "    model, size = shrink_model(model, 8, x_train, y_train, size, to_test)\n",
    "    print(len(model.get_weights()[1]))\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 2.4551 - accuracy: 0.5997\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.9119 - accuracy: 0.6414\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.8475 - accuracy: 0.6664\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.8432 - accuracy: 0.6703\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.8211 - accuracy: 0.6807\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.8123 - accuracy: 0.6885\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.7956 - accuracy: 0.6978\n",
      "10000/10000 - 0s - loss: 0.7238 - accuracy: 0.7112\n",
      "#############################\n",
      "Starting to shrinking the model by 1\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 2.5753 - accuracy: 0.5378\n",
      "60000/60000 - 2s - loss: 0.9136 - accuracy: 0.6790\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 1.0827 - accuracy: 0.5969\n",
      "60000/60000 - 2s - loss: 0.8217 - accuracy: 0.6990\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.9670 - accuracy: 0.6482\n",
      "60000/60000 - 2s - loss: 0.7798 - accuracy: 0.7017\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.9243 - accuracy: 0.6608\n",
      "60000/60000 - 3s - loss: 0.7434 - accuracy: 0.7127\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.8968 - accuracy: 0.6636\n",
      "60000/60000 - 4s - loss: 0.6480 - accuracy: 0.7321\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.8676 - accuracy: 0.6751\n",
      "60000/60000 - 2s - loss: 0.8912 - accuracy: 0.6470\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.8854 - accuracy: 0.6726\n",
      "10000/10000 - 0s - loss: 0.7619 - accuracy: 0.7253\n",
      "#############################\n",
      "Starting to shrinking the model by 2\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 2.3665 - accuracy: 0.5820\n",
      "60000/60000 - 3s - loss: 0.8591 - accuracy: 0.6884\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 1.0340 - accuracy: 0.6113\n",
      "60000/60000 - 2s - loss: 0.7847 - accuracy: 0.6908\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.9204 - accuracy: 0.6451\n",
      "60000/60000 - 3s - loss: 0.6563 - accuracy: 0.7562\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.8946 - accuracy: 0.6651\n",
      "60000/60000 - 3s - loss: 0.7487 - accuracy: 0.6710\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.8821 - accuracy: 0.6855\n",
      "60000/60000 - 3s - loss: 0.6190 - accuracy: 0.7771\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.9266 - accuracy: 0.6639\n",
      "60000/60000 - 3s - loss: 0.8015 - accuracy: 0.6984\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.8828 - accuracy: 0.6775\n",
      "10000/10000 - 1s - loss: 0.6737 - accuracy: 0.7450\n",
      "#############################\n",
      "Starting to shrinking the model by 3\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 2.5920 - accuracy: 0.5774\n",
      "60000/60000 - 3s - loss: 0.8340 - accuracy: 0.6730\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 1.0592 - accuracy: 0.6067\n",
      "60000/60000 - 2s - loss: 0.8184 - accuracy: 0.7046\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 1.0178 - accuracy: 0.6095\n",
      "60000/60000 - 3s - loss: 0.7930 - accuracy: 0.6565\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.9854 - accuracy: 0.6266\n",
      "60000/60000 - 3s - loss: 0.7389 - accuracy: 0.7000\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.9471 - accuracy: 0.6416\n",
      "60000/60000 - 3s - loss: 0.6865 - accuracy: 0.7301\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.9128 - accuracy: 0.6574\n",
      "60000/60000 - 3s - loss: 0.7027 - accuracy: 0.7208\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.9257 - accuracy: 0.6554\n",
      "10000/10000 - 1s - loss: 0.7662 - accuracy: 0.7151\n",
      "#############################\n",
      "Starting to shrinking the model by 4\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 2.4207 - accuracy: 0.5734\n",
      "60000/60000 - 3s - loss: 0.9617 - accuracy: 0.6660\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1.0446 - accuracy: 0.6216\n",
      "60000/60000 - 3s - loss: 0.7444 - accuracy: 0.7126\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 0.9458 - accuracy: 0.6494\n",
      "60000/60000 - 3s - loss: 0.7947 - accuracy: 0.6885\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.9765 - accuracy: 0.6335\n",
      "60000/60000 - 3s - loss: 0.7141 - accuracy: 0.7215\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.9618 - accuracy: 0.6343\n",
      "60000/60000 - 3s - loss: 0.7072 - accuracy: 0.7176\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.9744 - accuracy: 0.6276\n",
      "60000/60000 - 3s - loss: 0.7230 - accuracy: 0.7098\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.9588 - accuracy: 0.6400\n",
      "10000/10000 - 1s - loss: 0.7450 - accuracy: 0.7132\n",
      "#############################\n",
      "Starting to shrinking the model by 5\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 2.4976 - accuracy: 0.5958\n",
      "60000/60000 - 3s - loss: 0.9288 - accuracy: 0.6477\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.9810 - accuracy: 0.6240\n",
      "60000/60000 - 6s - loss: 0.7197 - accuracy: 0.6964\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.9219 - accuracy: 0.6442\n",
      "60000/60000 - 3s - loss: 0.7700 - accuracy: 0.7247\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.9004 - accuracy: 0.6516\n",
      "60000/60000 - 3s - loss: 0.7289 - accuracy: 0.6794\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.9048 - accuracy: 0.6535\n",
      "60000/60000 - 3s - loss: 0.7681 - accuracy: 0.7433\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.9004 - accuracy: 0.6580\n",
      "60000/60000 - 3s - loss: 0.6603 - accuracy: 0.7577\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.8764 - accuracy: 0.6653\n",
      "10000/10000 - 1s - loss: 0.6963 - accuracy: 0.7402\n",
      "#############################\n",
      "Starting to shrinking the model by 6\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 2.5069 - accuracy: 0.5731\n",
      "60000/60000 - 3s - loss: 0.9169 - accuracy: 0.6684\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 1.0300 - accuracy: 0.6266\n",
      "60000/60000 - 3s - loss: 0.7825 - accuracy: 0.6975\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.9192 - accuracy: 0.6561\n",
      "60000/60000 - 3s - loss: 0.7142 - accuracy: 0.7171\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.8963 - accuracy: 0.6592\n",
      "60000/60000 - 3s - loss: 0.7267 - accuracy: 0.7228\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.9005 - accuracy: 0.6626\n",
      "60000/60000 - 3s - loss: 0.7104 - accuracy: 0.7181\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.8911 - accuracy: 0.6629\n",
      "60000/60000 - 3s - loss: 0.6887 - accuracy: 0.7312\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.8883 - accuracy: 0.6669\n",
      "10000/10000 - 1s - loss: 0.7659 - accuracy: 0.7014\n",
      "#############################\n",
      "Starting to shrinking the model by 7\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 2.6198 - accuracy: 0.5819\n",
      "60000/60000 - 3s - loss: 0.8212 - accuracy: 0.6798\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.9991 - accuracy: 0.6237\n",
      "60000/60000 - 3s - loss: 0.7578 - accuracy: 0.7079\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.9376 - accuracy: 0.6477\n",
      "60000/60000 - 3s - loss: 0.8716 - accuracy: 0.6660\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.9687 - accuracy: 0.6328\n",
      "60000/60000 - 3s - loss: 0.7366 - accuracy: 0.7537\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.9175 - accuracy: 0.6532\n",
      "60000/60000 - 3s - loss: 0.6334 - accuracy: 0.7542\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.8736 - accuracy: 0.6654\n",
      "60000/60000 - 2s - loss: 0.6412 - accuracy: 0.7569\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.9244 - accuracy: 0.6527\n",
      "10000/10000 - 0s - loss: 0.7515 - accuracy: 0.7133\n",
      "#############################\n",
      "Starting to shrinking the model by 8\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 2.4462 - accuracy: 0.5967\n",
      "60000/60000 - 3s - loss: 0.9403 - accuracy: 0.6618\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 1.0845 - accuracy: 0.6103\n",
      "60000/60000 - 3s - loss: 0.8274 - accuracy: 0.6901\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 0.9835 - accuracy: 0.6365\n",
      "60000/60000 - 3s - loss: 0.7474 - accuracy: 0.7146\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.9820 - accuracy: 0.6294\n",
      "60000/60000 - 3s - loss: 0.7981 - accuracy: 0.7029\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.9737 - accuracy: 0.6362\n",
      "60000/60000 - 3s - loss: 0.7466 - accuracy: 0.6966\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.9946 - accuracy: 0.6292\n",
      "60000/60000 - 2s - loss: 0.7271 - accuracy: 0.7203\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.9582 - accuracy: 0.6448\n",
      "10000/10000 - 1s - loss: 0.7926 - accuracy: 0.6945\n",
      "#############################\n",
      "Starting to shrinking the model by 9\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 2.5055 - accuracy: 0.5744\n",
      "60000/60000 - 5s - loss: 0.8755 - accuracy: 0.6880\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 1.0491 - accuracy: 0.6180\n",
      "60000/60000 - 3s - loss: 0.7787 - accuracy: 0.6894\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.9490 - accuracy: 0.6396\n",
      "60000/60000 - 3s - loss: 0.7055 - accuracy: 0.7117\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.9642 - accuracy: 0.6408\n",
      "60000/60000 - 3s - loss: 0.7195 - accuracy: 0.7199\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.9232 - accuracy: 0.6564\n",
      "60000/60000 - 2s - loss: 0.7172 - accuracy: 0.7242\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.9463 - accuracy: 0.6482\n",
      "60000/60000 - 2s - loss: 0.7902 - accuracy: 0.7179\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.9269 - accuracy: 0.6541\n",
      "10000/10000 - 0s - loss: 0.7093 - accuracy: 0.7216\n",
      "#############################\n",
      "Starting to shrinking the model by 10\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 2.4792 - accuracy: 0.5649\n",
      "60000/60000 - 3s - loss: 0.8107 - accuracy: 0.6640\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 1.0136 - accuracy: 0.6296\n",
      "60000/60000 - 3s - loss: 0.7999 - accuracy: 0.7078\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.9358 - accuracy: 0.6517\n",
      "60000/60000 - 3s - loss: 0.7319 - accuracy: 0.7150\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.9104 - accuracy: 0.6573\n",
      "60000/60000 - 3s - loss: 0.6988 - accuracy: 0.7307\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.9045 - accuracy: 0.6605\n",
      "60000/60000 - 3s - loss: 0.6599 - accuracy: 0.7038\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.8849 - accuracy: 0.6648\n",
      "60000/60000 - 3s - loss: 0.6657 - accuracy: 0.7443\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.9210 - accuracy: 0.6572\n",
      "10000/10000 - 1s - loss: 0.7170 - accuracy: 0.7155\n",
      "#############################\n",
      "Starting to shrinking the model by 11\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 2.6421 - accuracy: 0.5727\n",
      "60000/60000 - 3s - loss: 0.8128 - accuracy: 0.7006\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 1.0625 - accuracy: 0.6086\n",
      "60000/60000 - 3s - loss: 0.8113 - accuracy: 0.7117\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 1.0128 - accuracy: 0.6169\n",
      "60000/60000 - 18s - loss: 1.0260 - accuracy: 0.5888\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 1.0963 - accuracy: 0.5832\n",
      "60000/60000 - 3s - loss: 0.8480 - accuracy: 0.6703\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 1.0180 - accuracy: 0.6129\n",
      "60000/60000 - 3s - loss: 0.7308 - accuracy: 0.7191\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 1.0462 - accuracy: 0.6097\n",
      "60000/60000 - 3s - loss: 0.8140 - accuracy: 0.7057\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.9992 - accuracy: 0.6242\n",
      "10000/10000 - 0s - loss: 0.8478 - accuracy: 0.6852\n",
      "#############################\n",
      "Starting to shrinking the model by 12\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 2.4359 - accuracy: 0.5861\n",
      "60000/60000 - 3s - loss: 0.8656 - accuracy: 0.6916\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 1.0549 - accuracy: 0.6205\n",
      "60000/60000 - 3s - loss: 0.7556 - accuracy: 0.7104\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.9598 - accuracy: 0.6417\n",
      "60000/60000 - 3s - loss: 0.7300 - accuracy: 0.7162\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.9676 - accuracy: 0.6408\n",
      "60000/60000 - 3s - loss: 0.7047 - accuracy: 0.7165\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.9216 - accuracy: 0.6479\n",
      "60000/60000 - 3s - loss: 0.6851 - accuracy: 0.7119\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.9447 - accuracy: 0.6421\n",
      "60000/60000 - 3s - loss: 0.7329 - accuracy: 0.7054\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.9369 - accuracy: 0.6497\n",
      "10000/10000 - 1s - loss: 0.8181 - accuracy: 0.6878\n",
      "#############################\n",
      "Starting to shrinking the model by 13\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 2.3334 - accuracy: 0.5932\n",
      "60000/60000 - 3s - loss: 0.8249 - accuracy: 0.6903\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.0164 - accuracy: 0.6210\n",
      "60000/60000 - 3s - loss: 0.7196 - accuracy: 0.6982\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.9203 - accuracy: 0.6510\n",
      "60000/60000 - 3s - loss: 0.6636 - accuracy: 0.7461\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.9576 - accuracy: 0.6316\n",
      "60000/60000 - 3s - loss: 0.9149 - accuracy: 0.6439\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.9797 - accuracy: 0.6239\n",
      "60000/60000 - 2s - loss: 0.7792 - accuracy: 0.6998\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.9686 - accuracy: 0.6355\n",
      "60000/60000 - 3s - loss: 0.7088 - accuracy: 0.7202\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.9643 - accuracy: 0.6338\n",
      "10000/10000 - 1s - loss: 0.7547 - accuracy: 0.7080\n",
      "#############################\n",
      "Starting to shrinking the model by 14\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 2.4301 - accuracy: 0.5944\n",
      "60000/60000 - 3s - loss: 0.8353 - accuracy: 0.6751\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 1.0272 - accuracy: 0.6256\n",
      "60000/60000 - 4s - loss: 0.7008 - accuracy: 0.7237\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.9346 - accuracy: 0.6520\n",
      "60000/60000 - 13s - loss: 0.6963 - accuracy: 0.7159\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 10s 159us/sample - loss: 0.8862 - accuracy: 0.6666\n",
      "60000/60000 - 5s - loss: 0.7454 - accuracy: 0.7098\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.9030 - accuracy: 0.6678\n",
      "60000/60000 - 2s - loss: 0.7372 - accuracy: 0.7022\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.9009 - accuracy: 0.6644\n",
      "60000/60000 - 2s - loss: 0.6579 - accuracy: 0.7295\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.9107 - accuracy: 0.6640\n",
      "10000/10000 - 1s - loss: 0.7301 - accuracy: 0.7202\n",
      "#############################\n",
      "Starting to shrinking the model by 15\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 2.4406 - accuracy: 0.5894\n",
      "60000/60000 - 3s - loss: 0.8989 - accuracy: 0.6697\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 1.0012 - accuracy: 0.6397\n",
      "60000/60000 - 3s - loss: 0.7054 - accuracy: 0.7276\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.9025 - accuracy: 0.6643\n",
      "60000/60000 - 4s - loss: 0.7209 - accuracy: 0.7219\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.8779 - accuracy: 0.6746\n",
      "60000/60000 - 3s - loss: 0.7140 - accuracy: 0.7236\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.8780 - accuracy: 0.6740\n",
      "60000/60000 - 3s - loss: 0.6665 - accuracy: 0.7329\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.8884 - accuracy: 0.6649\n",
      "60000/60000 - 2s - loss: 0.6772 - accuracy: 0.7350\n",
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.9438 - accuracy: 0.6479\n",
      "10000/10000 - 0s - loss: 0.7619 - accuracy: 0.7205\n",
      "#############################\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "rep = 6\n",
    "\n",
    "best_models = []\n",
    "sizes = []\n",
    "scores = []\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "start_weights = copy.deepcopy(model.get_weights())\n",
    "model.fit(x_train, y_train, epochs=7)\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"#############################\")\n",
    "best_models += [model]\n",
    "scores += [(loss, acc)]\n",
    "sizes +=[128]\n",
    "for i in range(1, 16):\n",
    "    print(f\"Starting to shrinking the model by {i}\")\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.set_weights(start_weights)\n",
    "    size = 128\n",
    "    to_test = 25\n",
    "    for _ in range(rep):\n",
    "        model.fit(x_train, y_train, epochs=1)\n",
    "        model, size = shrink_model(model, i, x_train, y_train, size, to_test)\n",
    "    model.fit(x_train, y_train, epochs=1)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"#############################\")\n",
    "    best_models += [model]\n",
    "    scores += [(loss, acc)]\n",
    "    sizes +=[128-(i*rep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7237960828781128, 0.7112), (0.7618597736358642, 0.7253), (0.673652571439743, 0.745), (0.7662316777706146, 0.7151), (0.7450256084918976, 0.7132), (0.6963049783706665, 0.7402), (0.7658660774230956, 0.7014), (0.7515249763488769, 0.7133), (0.7926195541381836, 0.6945), (0.709289279460907, 0.7216), (0.7170024285316468, 0.7155), (0.8477518383979797, 0.6852), (0.8180793400764466, 0.6878), (0.7546764973640442, 0.708), (0.7301193749427796, 0.7202), (0.7619338217735291, 0.7205)]\n",
      "[128, 122, 116, 110, 104, 98, 92, 86, 80, 74, 68, 62, 56, 50, 44, 38]\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting plain train of Dense size 122\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 2.5971 - accuracy: 0.6032\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.9368 - accuracy: 0.6525\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.8826 - accuracy: 0.6628\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.8395 - accuracy: 0.6764\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.8031 - accuracy: 0.6951\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.8171 - accuracy: 0.6923\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.7992 - accuracy: 0.6992\n",
      "10000/10000 - 1s - loss: 0.6936 - accuracy: 0.7615\n",
      "###############################\n",
      "Starting plain train of Dense size 116\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 2.4989 - accuracy: 0.5857\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.9057 - accuracy: 0.6422\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.8347 - accuracy: 0.6716\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.8063 - accuracy: 0.6898\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.7907 - accuracy: 0.6945\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.7943 - accuracy: 0.7010\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.7774 - accuracy: 0.7068\n",
      "10000/10000 - 1s - loss: 0.6456 - accuracy: 0.7546\n",
      "###############################\n",
      "Starting plain train of Dense size 110\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 2.5267 - accuracy: 0.5800\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.9819 - accuracy: 0.6301\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.8986 - accuracy: 0.6512\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.8806 - accuracy: 0.6609\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.8709 - accuracy: 0.6670\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.8717 - accuracy: 0.6649\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.8494 - accuracy: 0.6755\n",
      "10000/10000 - 1s - loss: 0.6715 - accuracy: 0.7370\n",
      "###############################\n",
      "Starting plain train of Dense size 104\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 2.5844 - accuracy: 0.5544\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 1.0073 - accuracy: 0.6280\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.9659 - accuracy: 0.6407\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.9849 - accuracy: 0.6329\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.9104 - accuracy: 0.6665\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.8748 - accuracy: 0.6807\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.8673 - accuracy: 0.6821\n",
      "10000/10000 - 1s - loss: 0.7064 - accuracy: 0.7583\n",
      "###############################\n",
      "Starting plain train of Dense size 98\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 2.5092 - accuracy: 0.5612\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 1.0244 - accuracy: 0.6202\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.9202 - accuracy: 0.6489\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.8889 - accuracy: 0.6589\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.8704 - accuracy: 0.6603\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.8646 - accuracy: 0.6632\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.8475 - accuracy: 0.6694\n",
      "10000/10000 - 1s - loss: 0.6980 - accuracy: 0.7233\n",
      "###############################\n",
      "Starting plain train of Dense size 92\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 2.5782 - accuracy: 0.4894\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 1.1745 - accuracy: 0.5402\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 1.0961 - accuracy: 0.5620\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 1.0625 - accuracy: 0.5774\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.9659 - accuracy: 0.6205\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.8971 - accuracy: 0.6531\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.8689 - accuracy: 0.6629\n",
      "10000/10000 - 1s - loss: 0.7351 - accuracy: 0.7190\n",
      "###############################\n",
      "Starting plain train of Dense size 86\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 2.7019 - accuracy: 0.5527\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 1.0625 - accuracy: 0.6056\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.9917 - accuracy: 0.6168\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.9063 - accuracy: 0.6489\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.8585 - accuracy: 0.6634\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.8444 - accuracy: 0.6667\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.8244 - accuracy: 0.6790\n",
      "10000/10000 - 1s - loss: 0.6708 - accuracy: 0.7276\n",
      "###############################\n",
      "Starting plain train of Dense size 80\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 2.3895 - accuracy: 0.5051\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 1.0340 - accuracy: 0.6151\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.9413 - accuracy: 0.6393\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.9565 - accuracy: 0.6346\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.9256 - accuracy: 0.6451\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.9182 - accuracy: 0.6501\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.9313 - accuracy: 0.6433\n",
      "10000/10000 - 1s - loss: 0.7103 - accuracy: 0.7248\n",
      "###############################\n",
      "Starting plain train of Dense size 74\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 2.3781 - accuracy: 0.5091\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 1.0766 - accuracy: 0.6040\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.9564 - accuracy: 0.6369\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.9145 - accuracy: 0.6481\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.8792 - accuracy: 0.6620\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.8701 - accuracy: 0.6629\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.8707 - accuracy: 0.6649\n",
      "10000/10000 - 1s - loss: 0.7372 - accuracy: 0.7241\n",
      "###############################\n",
      "Starting plain train of Dense size 68\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 2.0556 - accuracy: 0.4931\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 1.1159 - accuracy: 0.5826\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 1.0237 - accuracy: 0.6130\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.9836 - accuracy: 0.6258\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.9651 - accuracy: 0.6348\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.9474 - accuracy: 0.6412\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.9498 - accuracy: 0.6391\n",
      "10000/10000 - 1s - loss: 0.7255 - accuracy: 0.7178\n",
      "###############################\n",
      "Starting plain train of Dense size 62\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 2.4840 - accuracy: 0.4814\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 1.1354 - accuracy: 0.5754\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 1.0033 - accuracy: 0.6187\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.9605 - accuracy: 0.6292\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.9197 - accuracy: 0.6444\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.8980 - accuracy: 0.6502\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.8916 - accuracy: 0.6559\n",
      "10000/10000 - 1s - loss: 0.7661 - accuracy: 0.6617\n",
      "###############################\n",
      "Starting plain train of Dense size 56\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 2.7042 - accuracy: 0.4583\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 1.2686 - accuracy: 0.5135\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 1.1081 - accuracy: 0.5709\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 1.0851 - accuracy: 0.5752\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 1.0326 - accuracy: 0.5895\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.9918 - accuracy: 0.6090\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.9460 - accuracy: 0.6279\n",
      "10000/10000 - 1s - loss: 0.7338 - accuracy: 0.7126\n",
      "###############################\n",
      "Starting plain train of Dense size 50\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 2.4500 - accuracy: 0.3468\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 1.4734 - accuracy: 0.4233\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 1.2595 - accuracy: 0.5107\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1.0835 - accuracy: 0.5831\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.9916 - accuracy: 0.6157\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.9644 - accuracy: 0.6281\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 1.0412 - accuracy: 0.5915\n",
      "10000/10000 - 1s - loss: 0.9209 - accuracy: 0.6198\n",
      "###############################\n",
      "Starting plain train of Dense size 44\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 2.3410 - accuracy: 0.4026\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 1.2650 - accuracy: 0.4948\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 1.1706 - accuracy: 0.5245\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 1.1746 - accuracy: 0.5290\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 1.2415 - accuracy: 0.4825\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 1.0962 - accuracy: 0.5601\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.0581 - accuracy: 0.5736\n",
      "10000/10000 - 1s - loss: 0.8902 - accuracy: 0.6578\n",
      "###############################\n",
      "Starting plain train of Dense size 38\n",
      "Train on 60000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 2.8569 - accuracy: 0.2837\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.7748 - accuracy: 0.3197\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.5068 - accuracy: 0.4118\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 1.3133 - accuracy: 0.4820\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 1.2336 - accuracy: 0.5144\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 1.1393 - accuracy: 0.5504\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.0886 - accuracy: 0.5688\n",
      "10000/10000 - 4s - loss: 0.8812 - accuracy: 0.6335\n",
      "###############################\n"
     ]
    }
   ],
   "source": [
    "scores_plain = [scores[0]]\n",
    "for i in range(1, len(scores)):\n",
    "    print(f\"Starting plain train of Dense size {sizes[i]}\")\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(sizes[i], activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=7)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    scores_plain += [(loss, acc)]\n",
    "    print(\"###############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7237960828781128, 0.7112), (0.7618597736358642, 0.7253), (0.673652571439743, 0.745), (0.7662316777706146, 0.7151), (0.7450256084918976, 0.7132), (0.6963049783706665, 0.7402), (0.7658660774230956, 0.7014), (0.7515249763488769, 0.7133), (0.7926195541381836, 0.6945), (0.709289279460907, 0.7216), (0.7170024285316468, 0.7155), (0.8477518383979797, 0.6852), (0.8180793400764466, 0.6878), (0.7546764973640442, 0.708), (0.7301193749427796, 0.7202), (0.7619338217735291, 0.7205)]\n",
      "[(0.7237960828781128, 0.7112), (0.6935532826900482, 0.7615), (0.6456035744667054, 0.7546), (0.6715431744098663, 0.737), (0.7063772183418274, 0.7583), (0.6979923491477966, 0.7233), (0.7351104323387146, 0.719), (0.6707839384555817, 0.7276), (0.7102795698642731, 0.7248), (0.7371831143856049, 0.7241), (0.7254973567962647, 0.7178), (0.7660896290302277, 0.6617), (0.7337547297000885, 0.7126), (0.920928341293335, 0.6198), (0.8901723062515259, 0.6578), (0.8812190089702606, 0.6335)]\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(scores_plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss change: 0.0 --- Acc change: -0.0\n",
      "Loss change: -9.84877336040848 --- Acc change: -4.753773659467697\n",
      "Loss change: -4.344616120845869 --- Acc change: -1.2721950188279152\n",
      "Loss change: -14.100136367845293 --- Acc change: -2.971505932509899\n",
      "Loss change: -5.471352861689768 --- Acc change: -5.947518348693848\n",
      "Loss change: 0.2417463141523362 --- Acc change: 2.3365136235952377\n",
      "Loss change: -4.183812898224496 --- Acc change: -2.447844296693802\n",
      "Loss change: -12.036817410863183 --- Acc change: -1.9653640687465668\n",
      "Loss change: -11.592616170791008 --- Acc change: -4.180458188056946\n",
      "Loss change: 3.78384072835765 --- Acc change: -0.3452558536082506\n",
      "Loss change: 1.1709109874818622 --- Acc change: -0.32042686361819506\n",
      "Loss change: -10.659615568889249 --- Acc change: 3.5514529794454575\n",
      "Loss change: -11.492206722922866 --- Acc change: -3.4802138805389404\n",
      "Loss change: 18.052636288270783 --- Acc change: 14.230403304100037\n",
      "Loss change: 17.979994455536563 --- Acc change: 9.486163407564163\n",
      "Loss change: 13.536383802719032 --- Acc change: 13.733230531215668\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(scores)):\n",
    "    print(\"Loss change:\", (scores_plain[i][0] - scores[i][0])/scores_plain[i][0] *100, \"--- Acc change:\", -(scores_plain[i][1] - scores[i][1]) / scores_plain[i][1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_list = np.arange(1, 65)\n",
    "num_rep = 100\n",
    "loss_diff = np.zeros(num_rep)\n",
    "acc_diff = np.zeros(num_rep)\n",
    "loss_change = np.zeros(num_rep)\n",
    "acc_change = np.zeros(num_rep)\n",
    "nodes_removed_list = []\n",
    "num_nodes_removed = np.zeros(num_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64]\n"
     ]
    }
   ],
   "source": [
    "print(to_remove_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 3.3515 - accuracy: 0.7021\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.7128 - accuracy: 0.7297\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6359 - accuracy: 0.7535\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5845 - accuracy: 0.7891\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.5339 - accuracy: 0.8131\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5174 - accuracy: 0.8195\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4978 - accuracy: 0.8282\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4899 - accuracy: 0.8309\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4758 - accuracy: 0.8338 - loss: 0.4765 \n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.4717 - accuracy: 0.8363\n",
      "10000/10000 - 1s - loss: 0.5208 - accuracy: 0.8294\n",
      "[20]\n",
      "10000/10000 - 3s - loss: 0.8701 - accuracy: 0.7210\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 200us/sample - loss: 3.2496 - accuracy: 0.6574\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.8114 - accuracy: 0.6977\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.7030 - accuracy: 0.7217\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.6480 - accuracy: 0.7367\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6296 - accuracy: 0.7437\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6121 - accuracy: 0.7477\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5923 - accuracy: 0.7651\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5669 - accuracy: 0.7879\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5578 - accuracy: 0.7922\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5503 - accuracy: 0.7963\n",
      "10000/10000 - 3s - loss: 0.5803 - accuracy: 0.7977\n",
      "[38]\n",
      "10000/10000 - 1s - loss: 1.0578 - accuracy: 0.6841\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 2.5681 - accuracy: 0.7001\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.6735 - accuracy: 0.7538\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.5740 - accuracy: 0.7944\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.5431 - accuracy: 0.8102\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.5191 - accuracy: 0.8190\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.5015 - accuracy: 0.8265\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4853 - accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4776 - accuracy: 0.8341\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4777 - accuracy: 0.8358\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.4686 - accuracy: 0.8385\n",
      "10000/10000 - 1s - loss: 0.5793 - accuracy: 0.8219\n",
      "[62]\n",
      "10000/10000 - 1s - loss: 2.2972 - accuracy: 0.3904\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 3.2176 - accuracy: 0.6894\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.6849 - accuracy: 0.7400\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5926 - accuracy: 0.7883\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5468 - accuracy: 0.8086\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.5309 - accuracy: 0.8159\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5167 - accuracy: 0.8210\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.5100 - accuracy: 0.8249\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5017 - accuracy: 0.8272\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4983 - accuracy: 0.8307\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4911 - accuracy: 0.8303\n",
      "10000/10000 - 1s - loss: 0.5418 - accuracy: 0.8113\n",
      "[14]\n",
      "10000/10000 - 1s - loss: 0.9529 - accuracy: 0.6612\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 3.0445 - accuracy: 0.6873\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.6964 - accuracy: 0.7516\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5970 - accuracy: 0.7882\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5388 - accuracy: 0.8113\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5069 - accuracy: 0.8237\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4926 - accuracy: 0.8309\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4848 - accuracy: 0.8336\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4712 - accuracy: 0.8391\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4676 - accuracy: 0.8395\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4635 - accuracy: 0.8409\n",
      "10000/10000 - 1s - loss: 0.5883 - accuracy: 0.8104\n",
      "[27]\n",
      "10000/10000 - 1s - loss: 0.7880 - accuracy: 0.7213\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 3.0158 - accuracy: 0.6819\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7046 - accuracy: 0.7434\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.6046 - accuracy: 0.7805\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.5693 - accuracy: 0.7959\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5471 - accuracy: 0.8063\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5239 - accuracy: 0.8147\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5122 - accuracy: 0.8187\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5029 - accuracy: 0.8235\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4993 - accuracy: 0.8242\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4823 - accuracy: 0.8278\n",
      "10000/10000 - 1s - loss: 0.5129 - accuracy: 0.8250\n",
      "[37]\n",
      "10000/10000 - 1s - loss: 1.0649 - accuracy: 0.6472\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 14s 238us/sample - loss: 2.9960 - accuracy: 0.6779\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.6868 - accuracy: 0.7394\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5994 - accuracy: 0.7781\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5463 - accuracy: 0.8092\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.5194 - accuracy: 0.8204\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.5082 - accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4962 - accuracy: 0.8285\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4888 - accuracy: 0.8335\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4814 - accuracy: 0.8355\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4818 - accuracy: 0.8365\n",
      "10000/10000 - 1s - loss: 0.5594 - accuracy: 0.8031\n",
      "[33]\n",
      "10000/10000 - 0s - loss: 0.6855 - accuracy: 0.7421\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 3.3478 - accuracy: 0.7070\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.6930 - accuracy: 0.7548\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.5902 - accuracy: 0.7917\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5605 - accuracy: 0.8050\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.5238 - accuracy: 0.8173\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.5001 - accuracy: 0.8282\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4991 - accuracy: 0.8321\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4903 - accuracy: 0.8353\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4818 - accuracy: 0.8379\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4783 - accuracy: 0.8414\n",
      "10000/10000 - 1s - loss: 0.5778 - accuracy: 0.8164\n",
      "[64]\n",
      "10000/10000 - 1s - loss: 1.6310 - accuracy: 0.5701\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 25s 413us/sample - loss: 3.5099 - accuracy: 0.6975\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6562 - accuracy: 0.7726\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5756 - accuracy: 0.7950\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5355 - accuracy: 0.8168\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5029 - accuracy: 0.8263\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4969 - accuracy: 0.8327\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4790 - accuracy: 0.8362\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4713 - accuracy: 0.8385\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4645 - accuracy: 0.8405\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4650 - accuracy: 0.8409\n",
      "10000/10000 - 1s - loss: 0.5082 - accuracy: 0.8264\n",
      "[37]\n",
      "10000/10000 - 1s - loss: 1.4286 - accuracy: 0.5311\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 2.7146 - accuracy: 0.6883\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.6571 - accuracy: 0.7797\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5704 - accuracy: 0.8027\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.5194 - accuracy: 0.8233\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4975 - accuracy: 0.8301\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4785 - accuracy: 0.8373\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4725 - accuracy: 0.8385\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4699 - accuracy: 0.8428\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4604 - accuracy: 0.8440\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4589 - accuracy: 0.8463\n",
      "10000/10000 - 1s - loss: 0.5462 - accuracy: 0.8285\n",
      "[24]\n",
      "10000/10000 - 0s - loss: 0.7013 - accuracy: 0.7885\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 3.4130 - accuracy: 0.6743\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.7448 - accuracy: 0.7128\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.6480 - accuracy: 0.7445\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5963 - accuracy: 0.7678\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5825 - accuracy: 0.7754\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5581 - accuracy: 0.7861\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5418 - accuracy: 0.7904\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5361 - accuracy: 0.8038\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4893 - accuracy: 0.8308\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4850 - accuracy: 0.8343\n",
      "10000/10000 - 1s - loss: 0.5408 - accuracy: 0.8281\n",
      "[60]\n",
      "10000/10000 - 0s - loss: 2.1988 - accuracy: 0.3265\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 2.3751 - accuracy: 0.7028\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.6568 - accuracy: 0.7641\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6006 - accuracy: 0.7812\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5840 - accuracy: 0.7903\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.5672 - accuracy: 0.7961\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5450 - accuracy: 0.8052\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5313 - accuracy: 0.8119\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5226 - accuracy: 0.8219\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5119 - accuracy: 0.8288\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5075 - accuracy: 0.8280\n",
      "10000/10000 - 1s - loss: 0.5607 - accuracy: 0.8211\n",
      "[48]\n",
      "10000/10000 - 0s - loss: 1.0570 - accuracy: 0.7281\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 3.1712 - accuracy: 0.6892\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.6931 - accuracy: 0.7377\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6228 - accuracy: 0.7720\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5715 - accuracy: 0.7993\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5347 - accuracy: 0.8134\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5189 - accuracy: 0.8203\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.5082 - accuracy: 0.8247\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4915 - accuracy: 0.8296\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4908 - accuracy: 0.8304\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4855 - accuracy: 0.8314\n",
      "10000/10000 - 1s - loss: 0.5417 - accuracy: 0.8246\n",
      "[35]\n",
      "10000/10000 - 0s - loss: 1.5664 - accuracy: 0.4636\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 3.0804 - accuracy: 0.7111\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.6295 - accuracy: 0.7786\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5457 - accuracy: 0.8080\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5193 - accuracy: 0.8197\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4997 - accuracy: 0.8284\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4898 - accuracy: 0.8322\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4854 - accuracy: 0.8349\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4738 - accuracy: 0.8375\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4689 - accuracy: 0.8399\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4624 - accuracy: 0.8422\n",
      "10000/10000 - 1s - loss: 0.5351 - accuracy: 0.8350\n",
      "[52]\n",
      "10000/10000 - 1s - loss: 1.8200 - accuracy: 0.5448\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 3.5617 - accuracy: 0.6836\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.7573 - accuracy: 0.7174\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6505 - accuracy: 0.7568\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6067 - accuracy: 0.7802\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5757 - accuracy: 0.7879\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5654 - accuracy: 0.7934\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5523 - accuracy: 0.7953\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5484 - accuracy: 0.7996\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5403 - accuracy: 0.8015\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5371 - accuracy: 0.8033\n",
      "10000/10000 - 1s - loss: 0.6119 - accuracy: 0.7796\n",
      "[9]\n",
      "10000/10000 - 1s - loss: 0.6153 - accuracy: 0.7789\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 3.9396 - accuracy: 0.6963\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6957 - accuracy: 0.7394\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6206 - accuracy: 0.7587\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5804 - accuracy: 0.7828\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5134 - accuracy: 0.8237\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4923 - accuracy: 0.8331\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4861 - accuracy: 0.8374\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4756 - accuracy: 0.8391\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4743 - accuracy: 0.8400\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4696 - accuracy: 0.8437\n",
      "10000/10000 - 1s - loss: 0.5749 - accuracy: 0.8209\n",
      "[30]\n",
      "10000/10000 - 0s - loss: 0.9203 - accuracy: 0.6740\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 3.0574 - accuracy: 0.7035\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6679 - accuracy: 0.7590\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5679 - accuracy: 0.8001\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5330 - accuracy: 0.8167\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5189 - accuracy: 0.8226\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5017 - accuracy: 0.8298\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4972 - accuracy: 0.8317\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4830 - accuracy: 0.8365\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4862 - accuracy: 0.8366\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4732 - accuracy: 0.8413\n",
      "10000/10000 - 1s - loss: 0.5575 - accuracy: 0.8201\n",
      "[29]\n",
      "10000/10000 - 1s - loss: 0.8147 - accuracy: 0.7392\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 3.2736 - accuracy: 0.6751\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.7468 - accuracy: 0.7067\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.6381 - accuracy: 0.7568\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5686 - accuracy: 0.7978\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5440 - accuracy: 0.8110\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.5277 - accuracy: 0.8174\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5070 - accuracy: 0.8244\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5069 - accuracy: 0.8239\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4949 - accuracy: 0.8289\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4935 - accuracy: 0.8312\n",
      "10000/10000 - 1s - loss: 0.5718 - accuracy: 0.8152\n",
      "[24]\n",
      "10000/10000 - 0s - loss: 1.3321 - accuracy: 0.4937\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 3.3787 - accuracy: 0.6806\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.6899 - accuracy: 0.7595\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5748 - accuracy: 0.7992\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5374 - accuracy: 0.8127\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5288 - accuracy: 0.8190\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5094 - accuracy: 0.8281\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4957 - accuracy: 0.8313\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4868 - accuracy: 0.8331\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4860 - accuracy: 0.8354\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4688 - accuracy: 0.8405\n",
      "10000/10000 - 1s - loss: 0.5907 - accuracy: 0.8071\n",
      "[38]\n",
      "10000/10000 - 0s - loss: 1.6143 - accuracy: 0.5267\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 3.1588 - accuracy: 0.6906\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.7037 - accuracy: 0.7450\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6007 - accuracy: 0.7864\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5446 - accuracy: 0.8125\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5200 - accuracy: 0.8234\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5020 - accuracy: 0.8319\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4881 - accuracy: 0.8362\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4767 - accuracy: 0.8396\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4767 - accuracy: 0.8401\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4699 - accuracy: 0.8430\n",
      "10000/10000 - 1s - loss: 0.5183 - accuracy: 0.8346\n",
      "[7]\n",
      "10000/10000 - 0s - loss: 0.7391 - accuracy: 0.7501\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 4.4490 - accuracy: 0.7366\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.6325 - accuracy: 0.7898\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5801 - accuracy: 0.8067\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5243 - accuracy: 0.8235\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5009 - accuracy: 0.8325\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4882 - accuracy: 0.8374\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4731 - accuracy: 0.8413\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4703 - accuracy: 0.8426\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4684 - accuracy: 0.8433\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4666 - accuracy: 0.8454\n",
      "10000/10000 - 1s - loss: 0.5252 - accuracy: 0.8352\n",
      "[53]\n",
      "10000/10000 - 1s - loss: 1.6492 - accuracy: 0.4221\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 3.5880 - accuracy: 0.6916\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6734 - accuracy: 0.7579\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.6002 - accuracy: 0.7862\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5596 - accuracy: 0.8064\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5439 - accuracy: 0.8158\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5232 - accuracy: 0.8220\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5088 - accuracy: 0.8253\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5073 - accuracy: 0.8271\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4897 - accuracy: 0.8320\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4858 - accuracy: 0.8340\n",
      "10000/10000 - 1s - loss: 0.5575 - accuracy: 0.8225\n",
      "[36]\n",
      "10000/10000 - 0s - loss: 1.0115 - accuracy: 0.6117\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 3.6468 - accuracy: 0.6801\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.7033 - accuracy: 0.7364\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5896 - accuracy: 0.7917\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5225 - accuracy: 0.8188\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5021 - accuracy: 0.8301\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4848 - accuracy: 0.8368\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4708 - accuracy: 0.8417\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4661 - accuracy: 0.8431\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4556 - accuracy: 0.8473\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4492 - accuracy: 0.8488\n",
      "10000/10000 - 1s - loss: 0.5508 - accuracy: 0.8328\n",
      "[51]\n",
      "10000/10000 - 1s - loss: 1.7394 - accuracy: 0.4218\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 3.0953 - accuracy: 0.6763\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.7065 - accuracy: 0.7180\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.6136 - accuracy: 0.7702\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5773 - accuracy: 0.7978\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.5411 - accuracy: 0.8119\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5323 - accuracy: 0.8158\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.5237 - accuracy: 0.8193\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5132 - accuracy: 0.8235\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5085 - accuracy: 0.8256\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4965 - accuracy: 0.8289\n",
      "10000/10000 - 1s - loss: 0.5877 - accuracy: 0.8021\n",
      "[1]\n",
      "10000/10000 - 0s - loss: 0.5877 - accuracy: 0.8021\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 3.0720 - accuracy: 0.6836\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.7075 - accuracy: 0.7603\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5752 - accuracy: 0.7962\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5333 - accuracy: 0.8172\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5138 - accuracy: 0.8231\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4992 - accuracy: 0.8292\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4791 - accuracy: 0.8403\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4797 - accuracy: 0.8405\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4694 - accuracy: 0.8444\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4549 - accuracy: 0.8486\n",
      "10000/10000 - 1s - loss: 0.5124 - accuracy: 0.8373\n",
      "[20]\n",
      "10000/10000 - 1s - loss: 1.2271 - accuracy: 0.6832\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 3.0135 - accuracy: 0.6977\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.6377 - accuracy: 0.7696\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5690 - accuracy: 0.7980\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5384 - accuracy: 0.8141\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5164 - accuracy: 0.8206\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5078 - accuracy: 0.8247\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.4945 - accuracy: 0.8315\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4910 - accuracy: 0.8343\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4745 - accuracy: 0.8381\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4717 - accuracy: 0.8409\n",
      "10000/10000 - 1s - loss: 0.5400 - accuracy: 0.8198\n",
      "[18]\n",
      "10000/10000 - 1s - loss: 0.9249 - accuracy: 0.7133\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 3.9011 - accuracy: 0.6838\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.8004 - accuracy: 0.7020\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6342 - accuracy: 0.7583\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5369 - accuracy: 0.8107\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5165 - accuracy: 0.8238\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4982 - accuracy: 0.8325\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4907 - accuracy: 0.8346\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4776 - accuracy: 0.8413\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4798 - accuracy: 0.8432\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4746 - accuracy: 0.8423\n",
      "10000/10000 - 1s - loss: 0.5180 - accuracy: 0.8379\n",
      "[62]\n",
      "10000/10000 - 1s - loss: 2.6026 - accuracy: 0.3679\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 3.2065 - accuracy: 0.6707\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.7622 - accuracy: 0.7163\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6733 - accuracy: 0.7315\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.6445 - accuracy: 0.7409\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6025 - accuracy: 0.7723\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5838 - accuracy: 0.7857\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5645 - accuracy: 0.7929\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5617 - accuracy: 0.7949\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5568 - accuracy: 0.7959\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5433 - accuracy: 0.7989\n",
      "10000/10000 - 1s - loss: 0.5776 - accuracy: 0.7938\n",
      "[40]\n",
      "10000/10000 - 1s - loss: 1.2543 - accuracy: 0.6317\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 2.8223 - accuracy: 0.6783\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.7300 - accuracy: 0.7308\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.6315 - accuracy: 0.7626\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5966 - accuracy: 0.7854\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5360 - accuracy: 0.8148\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5160 - accuracy: 0.8219\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4967 - accuracy: 0.8283\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4832 - accuracy: 0.8328\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4808 - accuracy: 0.8346\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.4751 - accuracy: 0.8377\n",
      "10000/10000 - 1s - loss: 0.5543 - accuracy: 0.8204\n",
      "[44]\n",
      "10000/10000 - 1s - loss: 0.8004 - accuracy: 0.7335\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 3.0420 - accuracy: 0.7100\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.6484 - accuracy: 0.7751\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5547 - accuracy: 0.8078\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5133 - accuracy: 0.8235\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5006 - accuracy: 0.8290\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4824 - accuracy: 0.8354\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4749 - accuracy: 0.8394\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4725 - accuracy: 0.8392\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4638 - accuracy: 0.8421\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4644 - accuracy: 0.8436\n",
      "10000/10000 - 1s - loss: 0.5045 - accuracy: 0.8376\n",
      "[42]\n",
      "10000/10000 - 0s - loss: 1.6213 - accuracy: 0.4636\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 3.8539 - accuracy: 0.7144\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6617 - accuracy: 0.7786\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5649 - accuracy: 0.8094\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5205 - accuracy: 0.8214\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5070 - accuracy: 0.8285\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4892 - accuracy: 0.8357\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4819 - accuracy: 0.8385\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4690 - accuracy: 0.8423\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4666 - accuracy: 0.8444\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4609 - accuracy: 0.8454\n",
      "10000/10000 - 1s - loss: 0.5699 - accuracy: 0.8303\n",
      "[22]\n",
      "10000/10000 - 0s - loss: 0.7969 - accuracy: 0.7362\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 3.2772 - accuracy: 0.6723\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.7401 - accuracy: 0.7078\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6567 - accuracy: 0.7434\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.5865 - accuracy: 0.7935\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5636 - accuracy: 0.8060\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5500 - accuracy: 0.8108\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5391 - accuracy: 0.8174\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5368 - accuracy: 0.8168\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5285 - accuracy: 0.8218\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5238 - accuracy: 0.8226\n",
      "10000/10000 - 1s - loss: 0.6661 - accuracy: 0.7837\n",
      "[55]\n",
      "10000/10000 - 0s - loss: 1.4943 - accuracy: 0.5588\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 3.3928 - accuracy: 0.6756\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.7975 - accuracy: 0.7323\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.6707 - accuracy: 0.7610\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6096 - accuracy: 0.7794\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5790 - accuracy: 0.7901\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5651 - accuracy: 0.7966\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5528 - accuracy: 0.8013\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5458 - accuracy: 0.8033\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5332 - accuracy: 0.8061\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5291 - accuracy: 0.8077\n",
      "10000/10000 - 1s - loss: 0.6044 - accuracy: 0.7878\n",
      "[52]\n",
      "10000/10000 - 1s - loss: 1.4216 - accuracy: 0.5427\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 2.6747 - accuracy: 0.6914\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.6689 - accuracy: 0.7278\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.6199 - accuracy: 0.7473\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5971 - accuracy: 0.7610\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5472 - accuracy: 0.8031\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5197 - accuracy: 0.8191\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5052 - accuracy: 0.8267\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4978 - accuracy: 0.8302\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4817 - accuracy: 0.8348\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4790 - accuracy: 0.8363\n",
      "10000/10000 - 1s - loss: 0.5384 - accuracy: 0.8282\n",
      "[17]\n",
      "10000/10000 - 1s - loss: 1.9945 - accuracy: 0.5476\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 3.1239 - accuracy: 0.6956\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.6796 - accuracy: 0.7627\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5811 - accuracy: 0.7958\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5427 - accuracy: 0.8106\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5337 - accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5150 - accuracy: 0.8204\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5114 - accuracy: 0.8252\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5106 - accuracy: 0.8251\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4985 - accuracy: 0.8279\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4974 - accuracy: 0.8259\n",
      "10000/10000 - 1s - loss: 0.6028 - accuracy: 0.8131\n",
      "[27]\n",
      "10000/10000 - 1s - loss: 0.6368 - accuracy: 0.8038\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 2.6580 - accuracy: 0.6756\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.6386 - accuracy: 0.7713\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.5820 - accuracy: 0.7942\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5399 - accuracy: 0.8118\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5212 - accuracy: 0.8217\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4995 - accuracy: 0.8271\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4999 - accuracy: 0.8295\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4925 - accuracy: 0.8313\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4814 - accuracy: 0.8367\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4755 - accuracy: 0.8381\n",
      "10000/10000 - 1s - loss: 0.5701 - accuracy: 0.8130\n",
      "[11]\n",
      "10000/10000 - 1s - loss: 0.7676 - accuracy: 0.7321\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 3.5983 - accuracy: 0.6779\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.7777 - accuracy: 0.7139\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.6673 - accuracy: 0.7360\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.6345 - accuracy: 0.7446\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.6211 - accuracy: 0.7487\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.6110 - accuracy: 0.7512\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6058 - accuracy: 0.7536\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.6004 - accuracy: 0.7548\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5920 - accuracy: 0.7574\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5839 - accuracy: 0.7592\n",
      "10000/10000 - 1s - loss: 0.6721 - accuracy: 0.7289\n",
      "[59]\n",
      "10000/10000 - 1s - loss: 1.2510 - accuracy: 0.5537\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 2.9000 - accuracy: 0.6999\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6805 - accuracy: 0.7253\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.6056 - accuracy: 0.7592\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5515 - accuracy: 0.7942\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5354 - accuracy: 0.8042\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5175 - accuracy: 0.8126\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5088 - accuracy: 0.8201\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4956 - accuracy: 0.8242\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4950 - accuracy: 0.8251\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4876 - accuracy: 0.8297\n",
      "10000/10000 - 1s - loss: 0.4977 - accuracy: 0.8308\n",
      "[2]\n",
      "10000/10000 - 1s - loss: 0.4977 - accuracy: 0.8308\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 2.9030 - accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.6941 - accuracy: 0.7196\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6056 - accuracy: 0.7559\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.5566 - accuracy: 0.8036\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5293 - accuracy: 0.8183\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5095 - accuracy: 0.8257\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5025 - accuracy: 0.8278\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4967 - accuracy: 0.8303\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4860 - accuracy: 0.8336\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4921 - accuracy: 0.8329\n",
      "10000/10000 - 6s - loss: 0.5262 - accuracy: 0.8275\n",
      "[56]\n",
      "10000/10000 - 1s - loss: 0.7268 - accuracy: 0.7737\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 3.6118 - accuracy: 0.6744\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.7513 - accuracy: 0.7127\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6888 - accuracy: 0.7369\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5885 - accuracy: 0.7848\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5322 - accuracy: 0.8103\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5080 - accuracy: 0.8228\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4982 - accuracy: 0.8282\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4912 - accuracy: 0.8303\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4908 - accuracy: 0.8325\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4746 - accuracy: 0.8367\n",
      "10000/10000 - 1s - loss: 0.5083 - accuracy: 0.8248\n",
      "[34]\n",
      "10000/10000 - 1s - loss: 1.5526 - accuracy: 0.6248\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 3.3400 - accuracy: 0.7037\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6255 - accuracy: 0.7829\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5636 - accuracy: 0.8060\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5295 - accuracy: 0.8194\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.5015 - accuracy: 0.8278\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4931 - accuracy: 0.8307\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4791 - accuracy: 0.8356\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4762 - accuracy: 0.8349\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4654 - accuracy: 0.8428\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4653 - accuracy: 0.8418\n",
      "10000/10000 - 1s - loss: 0.5486 - accuracy: 0.8187\n",
      "[31]\n",
      "10000/10000 - 1s - loss: 1.1853 - accuracy: 0.6541\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 3.2946 - accuracy: 0.6973\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.6519 - accuracy: 0.7617\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5638 - accuracy: 0.8017\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5375 - accuracy: 0.8140\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5185 - accuracy: 0.8233\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5087 - accuracy: 0.8275\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4972 - accuracy: 0.8316\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4887 - accuracy: 0.8342\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4830 - accuracy: 0.8348\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4880 - accuracy: 0.8353\n",
      "10000/10000 - 1s - loss: 0.5252 - accuracy: 0.8293\n",
      "[33]\n",
      "10000/10000 - 1s - loss: 1.0037 - accuracy: 0.7285\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 3.5906 - accuracy: 0.6922\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.6826 - accuracy: 0.7443\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.5858 - accuracy: 0.7919\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5424 - accuracy: 0.8138\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5223 - accuracy: 0.8196\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5137 - accuracy: 0.8259\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.5029 - accuracy: 0.8278\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4884 - accuracy: 0.8360\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4870 - accuracy: 0.8342\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4797 - accuracy: 0.8382\n",
      "10000/10000 - 1s - loss: 0.5888 - accuracy: 0.8039\n",
      "[17]\n",
      "10000/10000 - 1s - loss: 1.4053 - accuracy: 0.6215\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 2.9737 - accuracy: 0.6712\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.6729 - accuracy: 0.7410\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5867 - accuracy: 0.7890\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5440 - accuracy: 0.8097\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5306 - accuracy: 0.8156\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5185 - accuracy: 0.8212\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5082 - accuracy: 0.8243\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5025 - accuracy: 0.8271\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4936 - accuracy: 0.8314\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4859 - accuracy: 0.8342\n",
      "10000/10000 - 1s - loss: 0.6126 - accuracy: 0.8026\n",
      "[35]\n",
      "10000/10000 - 1s - loss: 0.6248 - accuracy: 0.7964\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 3.2293 - accuracy: 0.6910\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.6376 - accuracy: 0.7738\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5506 - accuracy: 0.8037\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5215 - accuracy: 0.8167\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5145 - accuracy: 0.8209\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.5081 - accuracy: 0.8239\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4965 - accuracy: 0.8274\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4897 - accuracy: 0.8297\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4845 - accuracy: 0.8320\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4801 - accuracy: 0.8326\n",
      "10000/10000 - 1s - loss: 0.5278 - accuracy: 0.8278\n",
      "[34]\n",
      "10000/10000 - 1s - loss: 1.7601 - accuracy: 0.5467\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 3.4298 - accuracy: 0.6915\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.7157 - accuracy: 0.7426\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6100 - accuracy: 0.7908\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5498 - accuracy: 0.8187\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5172 - accuracy: 0.8299\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4971 - accuracy: 0.8364\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4890 - accuracy: 0.8384\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4814 - accuracy: 0.8415\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4785 - accuracy: 0.8436\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4749 - accuracy: 0.8458\n",
      "10000/10000 - 1s - loss: 0.5648 - accuracy: 0.8156\n",
      "[8]\n",
      "10000/10000 - 1s - loss: 0.5648 - accuracy: 0.8156\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 2.8789 - accuracy: 0.6875\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.6617 - accuracy: 0.7568\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5932 - accuracy: 0.7814\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5640 - accuracy: 0.7929\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5416 - accuracy: 0.8037\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5188 - accuracy: 0.8170\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.4928 - accuracy: 0.8292\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4762 - accuracy: 0.8344\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4709 - accuracy: 0.8359\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4626 - accuracy: 0.8390\n",
      "10000/10000 - 1s - loss: 0.5564 - accuracy: 0.8176\n",
      "[42]\n",
      "10000/10000 - 1s - loss: 1.2490 - accuracy: 0.5844\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 3.0258 - accuracy: 0.7174\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6239 - accuracy: 0.7868\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5564 - accuracy: 0.8101\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5078 - accuracy: 0.8273\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4976 - accuracy: 0.8296\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4795 - accuracy: 0.8387\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4595 - accuracy: 0.8440\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4553 - accuracy: 0.8458\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4537 - accuracy: 0.8475\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.4486 - accuracy: 0.8483\n",
      "10000/10000 - 1s - loss: 0.6010 - accuracy: 0.8144\n",
      "[53]\n",
      "10000/10000 - 1s - loss: 1.7617 - accuracy: 0.4224\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 2.8433 - accuracy: 0.7280\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.6048 - accuracy: 0.7931\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5267 - accuracy: 0.8189\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.5065 - accuracy: 0.8274\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5026 - accuracy: 0.8311\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4833 - accuracy: 0.8376\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4831 - accuracy: 0.8373\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4728 - accuracy: 0.8399\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4684 - accuracy: 0.8422\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4566 - accuracy: 0.8450\n",
      "10000/10000 - 1s - loss: 0.5512 - accuracy: 0.8292\n",
      "[28]\n",
      "10000/10000 - 1s - loss: 1.7153 - accuracy: 0.5198\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 3.6834 - accuracy: 0.6836\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.6990 - accuracy: 0.7373\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.6084 - accuracy: 0.7735\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5617 - accuracy: 0.8025\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5338 - accuracy: 0.8152\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5228 - accuracy: 0.8194\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5023 - accuracy: 0.8252\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5020 - accuracy: 0.8274\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4924 - accuracy: 0.8322\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4871 - accuracy: 0.8322\n",
      "10000/10000 - 1s - loss: 0.5920 - accuracy: 0.7992\n",
      "[18]\n",
      "10000/10000 - 1s - loss: 1.0084 - accuracy: 0.6946\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 3.3118 - accuracy: 0.6849\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6949 - accuracy: 0.7420\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5874 - accuracy: 0.7876\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5470 - accuracy: 0.8100\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5191 - accuracy: 0.8216\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5047 - accuracy: 0.8274\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4960 - accuracy: 0.8338\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4915 - accuracy: 0.8329\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4844 - accuracy: 0.8363\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.4751 - accuracy: 0.8417\n",
      "10000/10000 - 1s - loss: 0.5639 - accuracy: 0.8111\n",
      "[40]\n",
      "10000/10000 - 1s - loss: 1.1400 - accuracy: 0.6264\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 2.6206 - accuracy: 0.7190\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.6223 - accuracy: 0.7826\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5723 - accuracy: 0.8012\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5292 - accuracy: 0.8182\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5018 - accuracy: 0.8286\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4846 - accuracy: 0.8340\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.4685 - accuracy: 0.8408\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4776 - accuracy: 0.8373\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4576 - accuracy: 0.8472\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4632 - accuracy: 0.8387\n",
      "10000/10000 - 1s - loss: 0.5007 - accuracy: 0.8387\n",
      "[50]\n",
      "10000/10000 - 1s - loss: 1.6655 - accuracy: 0.4981\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 2.5598 - accuracy: 0.6869\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6694 - accuracy: 0.7487\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5853 - accuracy: 0.7904\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5524 - accuracy: 0.8038\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5348 - accuracy: 0.8147\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5188 - accuracy: 0.8186\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5012 - accuracy: 0.8247\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4983 - accuracy: 0.8266\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4873 - accuracy: 0.8301\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4881 - accuracy: 0.8297\n",
      "10000/10000 - 1s - loss: 0.5393 - accuracy: 0.8220\n",
      "[4]\n",
      "10000/10000 - 1s - loss: 0.5390 - accuracy: 0.8221\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 2.9219 - accuracy: 0.6974\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6659 - accuracy: 0.7673\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5666 - accuracy: 0.8036\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5286 - accuracy: 0.8193\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5108 - accuracy: 0.8257\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4980 - accuracy: 0.8322\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4927 - accuracy: 0.8362\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4819 - accuracy: 0.8425\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4823 - accuracy: 0.8411\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4741 - accuracy: 0.8463\n",
      "10000/10000 - 1s - loss: 0.5391 - accuracy: 0.8271\n",
      "[46]\n",
      "10000/10000 - 1s - loss: 1.3545 - accuracy: 0.5958\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 3.6070 - accuracy: 0.6894\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.7065 - accuracy: 0.7291\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6031 - accuracy: 0.7800\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5521 - accuracy: 0.8056\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5211 - accuracy: 0.8180\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5113 - accuracy: 0.8220\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5039 - accuracy: 0.8269\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4941 - accuracy: 0.8302\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4886 - accuracy: 0.8317\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4832 - accuracy: 0.8331\n",
      "10000/10000 - 1s - loss: 0.5562 - accuracy: 0.8181\n",
      "[5]\n",
      "10000/10000 - 1s - loss: 0.5573 - accuracy: 0.8181\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 3.7532 - accuracy: 0.6931\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.7212 - accuracy: 0.7473\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.6324 - accuracy: 0.7675\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5859 - accuracy: 0.7878\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5522 - accuracy: 0.8106\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5280 - accuracy: 0.8256\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5122 - accuracy: 0.8305\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5018 - accuracy: 0.8341\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4991 - accuracy: 0.8382\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5300 - accuracy: 0.8157\n",
      "10000/10000 - 1s - loss: 0.5852 - accuracy: 0.7995\n",
      "[14]\n",
      "10000/10000 - 1s - loss: 0.5895 - accuracy: 0.7980\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 2.9044 - accuracy: 0.6904\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6842 - accuracy: 0.7361\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.6257 - accuracy: 0.7638\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.6047 - accuracy: 0.7739\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5926 - accuracy: 0.7845\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5705 - accuracy: 0.7932\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5566 - accuracy: 0.7997\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5459 - accuracy: 0.8029\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5391 - accuracy: 0.8058\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5340 - accuracy: 0.8068\n",
      "10000/10000 - 1s - loss: 0.6537 - accuracy: 0.7562\n",
      "[40]\n",
      "10000/10000 - 1s - loss: 1.4052 - accuracy: 0.5566\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 3.2627 - accuracy: 0.6756\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.7193 - accuracy: 0.7346\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.6021 - accuracy: 0.7832\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5400 - accuracy: 0.8107\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.5218 - accuracy: 0.8225\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5098 - accuracy: 0.8256\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4924 - accuracy: 0.8310\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4839 - accuracy: 0.8360\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4783 - accuracy: 0.8398\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.4848 - accuracy: 0.8399\n",
      "10000/10000 - 1s - loss: 0.5023 - accuracy: 0.8345\n",
      "[3]\n",
      "10000/10000 - 1s - loss: 0.5023 - accuracy: 0.8345\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 2.8396 - accuracy: 0.6808\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.6547 - accuracy: 0.7749\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5798 - accuracy: 0.7967\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5414 - accuracy: 0.8082\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5363 - accuracy: 0.8104\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5219 - accuracy: 0.8182\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5040 - accuracy: 0.8247\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.5037 - accuracy: 0.8225\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4902 - accuracy: 0.8278\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4883 - accuracy: 0.8280\n",
      "10000/10000 - 1s - loss: 0.5251 - accuracy: 0.8269\n",
      "[21]\n",
      "10000/10000 - 1s - loss: 0.6881 - accuracy: 0.7813\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 3.2531 - accuracy: 0.6689\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.7445 - accuracy: 0.7191\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.6325 - accuracy: 0.7620\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5524 - accuracy: 0.8060\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.5179 - accuracy: 0.8200\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.4989 - accuracy: 0.8296\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4886 - accuracy: 0.8337\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4821 - accuracy: 0.8354\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4734 - accuracy: 0.8393\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4810 - accuracy: 0.8375\n",
      "10000/10000 - 1s - loss: 0.5349 - accuracy: 0.8229\n",
      "[56]\n",
      "10000/10000 - 1s - loss: 1.6987 - accuracy: 0.5708\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 2.8530 - accuracy: 0.6865\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.6963 - accuracy: 0.7225\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.6280 - accuracy: 0.7516\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5775 - accuracy: 0.7845\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5292 - accuracy: 0.8126\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5100 - accuracy: 0.8203\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5006 - accuracy: 0.8251\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4847 - accuracy: 0.8308\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4803 - accuracy: 0.8357\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.4795 - accuracy: 0.8359\n",
      "10000/10000 - 1s - loss: 0.5565 - accuracy: 0.8193\n",
      "[29]\n",
      "10000/10000 - 1s - loss: 0.9316 - accuracy: 0.7012\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 3.1087 - accuracy: 0.6508\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.8090 - accuracy: 0.7002\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.6931 - accuracy: 0.7304\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.6191 - accuracy: 0.7714\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5858 - accuracy: 0.7850\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5651 - accuracy: 0.7919\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5578 - accuracy: 0.7947\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5596 - accuracy: 0.7974\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.5451 - accuracy: 0.8004\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5384 - accuracy: 0.8023\n",
      "10000/10000 - 1s - loss: 0.7853 - accuracy: 0.7686\n",
      "[24]\n",
      "10000/10000 - 1s - loss: 0.8356 - accuracy: 0.7522\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 3.1566 - accuracy: 0.6917\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.6581 - accuracy: 0.7440\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.6045 - accuracy: 0.7659\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5794 - accuracy: 0.7776\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5386 - accuracy: 0.8064\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5107 - accuracy: 0.8235\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4951 - accuracy: 0.8290\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.4889 - accuracy: 0.8340\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4801 - accuracy: 0.8355\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4799 - accuracy: 0.8383\n",
      "10000/10000 - 1s - loss: 0.5590 - accuracy: 0.8138\n",
      "[46]\n",
      "10000/10000 - 1s - loss: 1.5545 - accuracy: 0.5180\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 2.8088 - accuracy: 0.7016\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.6413 - accuracy: 0.7733\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5661 - accuracy: 0.8064\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5148 - accuracy: 0.8221\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.4998 - accuracy: 0.8283\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4862 - accuracy: 0.8349\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4923 - accuracy: 0.8310\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4854 - accuracy: 0.8336\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4650 - accuracy: 0.8452\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4612 - accuracy: 0.8453\n",
      "10000/10000 - 1s - loss: 0.5050 - accuracy: 0.8399\n",
      "[12]\n",
      "10000/10000 - 1s - loss: 0.5975 - accuracy: 0.8085\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 3.2633 - accuracy: 0.6840\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.7040 - accuracy: 0.7375\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.6025 - accuracy: 0.7808\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5516 - accuracy: 0.8108\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.5170 - accuracy: 0.8242\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5073 - accuracy: 0.8312\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.4881 - accuracy: 0.8362\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4784 - accuracy: 0.8399\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.4736 - accuracy: 0.8414\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4699 - accuracy: 0.8443\n",
      "10000/10000 - 1s - loss: 0.5595 - accuracy: 0.8223\n",
      "[59]\n",
      "10000/10000 - 1s - loss: 1.2185 - accuracy: 0.6483\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 2.7802 - accuracy: 0.6735\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.6767 - accuracy: 0.7336\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.5883 - accuracy: 0.7800\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5400 - accuracy: 0.8044\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.5207 - accuracy: 0.8164\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5069 - accuracy: 0.8213\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.4923 - accuracy: 0.8282\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4911 - accuracy: 0.8306\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.4771 - accuracy: 0.8357\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4718 - accuracy: 0.8355\n",
      "10000/10000 - 1s - loss: 0.5786 - accuracy: 0.7948\n",
      "[27]\n",
      "10000/10000 - 1s - loss: 0.7487 - accuracy: 0.7483\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 3.1577 - accuracy: 0.6941\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.6989 - accuracy: 0.7292\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.5938 - accuracy: 0.7804\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5518 - accuracy: 0.8099\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.5159 - accuracy: 0.8218\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5065 - accuracy: 0.8237\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4899 - accuracy: 0.8292\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.4901 - accuracy: 0.8309\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4798 - accuracy: 0.8344\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4741 - accuracy: 0.8336\n",
      "10000/10000 - 1s - loss: 0.5945 - accuracy: 0.8168\n",
      "[16]\n",
      "10000/10000 - 1s - loss: 0.8866 - accuracy: 0.6981\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 3.0154 - accuracy: 0.6805\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.7462 - accuracy: 0.7225\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.6969 - accuracy: 0.7355\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.6221 - accuracy: 0.7691\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5517 - accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5163 - accuracy: 0.8288\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4964 - accuracy: 0.8346\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4928 - accuracy: 0.8370\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4824 - accuracy: 0.8407\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.4784 - accuracy: 0.8417\n",
      "10000/10000 - 1s - loss: 0.6036 - accuracy: 0.8221\n",
      "[52]\n",
      "10000/10000 - 1s - loss: 2.8402 - accuracy: 0.4254\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 2.9211 - accuracy: 0.6856\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.6834 - accuracy: 0.7399\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5820 - accuracy: 0.7938\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5428 - accuracy: 0.8145\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5158 - accuracy: 0.8238\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5076 - accuracy: 0.8289\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4929 - accuracy: 0.8338\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4983 - accuracy: 0.8346\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4860 - accuracy: 0.8368\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4818 - accuracy: 0.8395\n",
      "10000/10000 - 1s - loss: 0.5312 - accuracy: 0.8243\n",
      "[53]\n",
      "10000/10000 - 1s - loss: 1.6066 - accuracy: 0.5981\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 3.3293 - accuracy: 0.6839\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.6710 - accuracy: 0.7520\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.5843 - accuracy: 0.7926\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.5400 - accuracy: 0.8129\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.5184 - accuracy: 0.8195\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5079 - accuracy: 0.8271\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4950 - accuracy: 0.8325\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4879 - accuracy: 0.8350\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.4823 - accuracy: 0.8373\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4768 - accuracy: 0.8403\n",
      "10000/10000 - 1s - loss: 0.5808 - accuracy: 0.8159\n",
      "[14]\n",
      "10000/10000 - 1s - loss: 1.0928 - accuracy: 0.6776\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 3.1091 - accuracy: 0.7053\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.6719 - accuracy: 0.7563\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5814 - accuracy: 0.7925\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5361 - accuracy: 0.8145\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.5110 - accuracy: 0.8278\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4931 - accuracy: 0.8319\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4875 - accuracy: 0.8338\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4750 - accuracy: 0.8411\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4683 - accuracy: 0.8418\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4686 - accuracy: 0.8422\n",
      "10000/10000 - 1s - loss: 0.5226 - accuracy: 0.8287\n",
      "[14]\n",
      "10000/10000 - 2s - loss: 0.6764 - accuracy: 0.7464\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 13s 223us/sample - loss: 3.1523 - accuracy: 0.6819\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 0.6930 - accuracy: 0.7183\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.6341 - accuracy: 0.7447\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.5984 - accuracy: 0.7691\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.5698 - accuracy: 0.7803\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.5264 - accuracy: 0.8133\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.4998 - accuracy: 0.8278\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.4826 - accuracy: 0.8351\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.4936 - accuracy: 0.8310\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.4767 - accuracy: 0.8363\n",
      "10000/10000 - 3s - loss: 0.5318 - accuracy: 0.8206\n",
      "[58]\n",
      "10000/10000 - 1s - loss: 2.2549 - accuracy: 0.3330\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 13s 213us/sample - loss: 2.7091 - accuracy: 0.6629\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.7553 - accuracy: 0.7180\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.6519 - accuracy: 0.7458\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.6030 - accuracy: 0.7683\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.5755 - accuracy: 0.7825- loss: 0.5728 - ac\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.5658 - accuracy: 0.7893\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.5588 - accuracy: 0.7907\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 12s 196us/sample - loss: 0.5447 - accuracy: 0.7951\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 206us/sample - loss: 0.5353 - accuracy: 0.8012\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 206us/sample - loss: 0.5255 - accuracy: 0.8046\n",
      "10000/10000 - 2s - loss: 0.5779 - accuracy: 0.7795\n",
      "[50]\n",
      "10000/10000 - 1s - loss: 1.4794 - accuracy: 0.5626\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 2.9618 - accuracy: 0.7131\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.6417 - accuracy: 0.7759\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.5964 - accuracy: 0.7890\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.5433 - accuracy: 0.8092\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5087 - accuracy: 0.8211\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4994 - accuracy: 0.8292\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.4919 - accuracy: 0.8330\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.4870 - accuracy: 0.8360\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.4779 - accuracy: 0.8389\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.4748 - accuracy: 0.8386\n",
      "10000/10000 - 4s - loss: 0.5718 - accuracy: 0.8212\n",
      "[51]\n",
      "10000/10000 - 1s - loss: 1.6621 - accuracy: 0.5676\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 3.1108 - accuracy: 0.7011\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.6595 - accuracy: 0.7519\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5814 - accuracy: 0.7933\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.5378 - accuracy: 0.8144\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.5106 - accuracy: 0.8241\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4929 - accuracy: 0.8317\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4845 - accuracy: 0.8357\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4707 - accuracy: 0.8391\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4635 - accuracy: 0.8437\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.4623 - accuracy: 0.8449\n",
      "10000/10000 - 2s - loss: 0.4994 - accuracy: 0.8388\n",
      "[45]\n",
      "10000/10000 - 1s - loss: 1.4950 - accuracy: 0.4834\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 197us/sample - loss: 3.2775 - accuracy: 0.6696\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.7368 - accuracy: 0.7390\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.6217 - accuracy: 0.7770\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5663 - accuracy: 0.8029\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5409 - accuracy: 0.8142\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.5307 - accuracy: 0.8206\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5124 - accuracy: 0.8271\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.5050 - accuracy: 0.8315\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4877 - accuracy: 0.8370\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 0.4874 - accuracy: 0.8397\n",
      "10000/10000 - 2s - loss: 0.6374 - accuracy: 0.7976\n",
      "[42]\n",
      "10000/10000 - 1s - loss: 0.7954 - accuracy: 0.7278\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 196us/sample - loss: 3.8818 - accuracy: 0.6796\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.7806 - accuracy: 0.7086\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.6558 - accuracy: 0.7527\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5731 - accuracy: 0.7905\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5337 - accuracy: 0.8107\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.5102 - accuracy: 0.8202\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5045 - accuracy: 0.8268\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.4834 - accuracy: 0.8332\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4834 - accuracy: 0.8356\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.4749 - accuracy: 0.8384\n",
      "10000/10000 - 2s - loss: 0.5142 - accuracy: 0.8303\n",
      "[2]\n",
      "10000/10000 - 1s - loss: 0.5142 - accuracy: 0.8303\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 205us/sample - loss: 3.0283 - accuracy: 0.6779\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.7139 - accuracy: 0.7328\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.6360 - accuracy: 0.7643\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5857 - accuracy: 0.7873\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5706 - accuracy: 0.7946\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5320 - accuracy: 0.8168\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.5148 - accuracy: 0.8240\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4997 - accuracy: 0.8294\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.4968 - accuracy: 0.8327\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.4984 - accuracy: 0.8313\n",
      "10000/10000 - 1s - loss: 0.5435 - accuracy: 0.8230\n",
      "[20]\n",
      "10000/10000 - 2s - loss: 0.8847 - accuracy: 0.7317\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 2.8768 - accuracy: 0.6903\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.7568 - accuracy: 0.7293\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.6388 - accuracy: 0.7743\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5864 - accuracy: 0.7904\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5635 - accuracy: 0.7954\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.5453 - accuracy: 0.8028\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5360 - accuracy: 0.8065\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.5302 - accuracy: 0.8081\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.5281 - accuracy: 0.8097\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 207us/sample - loss: 0.5252 - accuracy: 0.8102\n",
      "10000/10000 - 2s - loss: 0.5923 - accuracy: 0.7973\n",
      "[48]\n",
      "10000/10000 - 1s - loss: 2.3338 - accuracy: 0.4241\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 13s 211us/sample - loss: 3.3094 - accuracy: 0.6728\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.7343 - accuracy: 0.7147\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.6321 - accuracy: 0.7587\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.5461 - accuracy: 0.8095\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.5129 - accuracy: 0.8222\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 196us/sample - loss: 0.5008 - accuracy: 0.8284\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.4981 - accuracy: 0.8289\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.4846 - accuracy: 0.8352\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4990 - accuracy: 0.8339\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.4762 - accuracy: 0.8387\n",
      "10000/10000 - 2s - loss: 0.5510 - accuracy: 0.8123\n",
      "[64]\n",
      "10000/10000 - 1s - loss: 2.2307 - accuracy: 0.3655\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 200us/sample - loss: 3.3623 - accuracy: 0.6774\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.6460 - accuracy: 0.7582\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5671 - accuracy: 0.7993\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5393 - accuracy: 0.8138\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.5220 - accuracy: 0.8189\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5140 - accuracy: 0.8227\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.5008 - accuracy: 0.8283\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.4980 - accuracy: 0.8294\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4771 - accuracy: 0.8365\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.4811 - accuracy: 0.8339\n",
      "10000/10000 - 2s - loss: 0.5326 - accuracy: 0.8250\n",
      "[20]\n",
      "10000/10000 - 1s - loss: 0.8669 - accuracy: 0.7191\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 202us/sample - loss: 4.0437 - accuracy: 0.6981\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.7202 - accuracy: 0.7497\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.5938 - accuracy: 0.7926\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5346 - accuracy: 0.8188\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4911 - accuracy: 0.8332\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4780 - accuracy: 0.8377\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4658 - accuracy: 0.8407\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4569 - accuracy: 0.8442\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.4525 - accuracy: 0.8462\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.4491 - accuracy: 0.8475\n",
      "10000/10000 - 3s - loss: 0.5320 - accuracy: 0.8370\n",
      "[50]\n",
      "10000/10000 - 1s - loss: 1.5047 - accuracy: 0.5337\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 3.2995 - accuracy: 0.6831\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.7104 - accuracy: 0.7289\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.6132 - accuracy: 0.7817\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5571 - accuracy: 0.8110\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.5337 - accuracy: 0.8201\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.5144 - accuracy: 0.8257\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5033 - accuracy: 0.8298\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 180us/sample - loss: 0.4912 - accuracy: 0.8351\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 177us/sample - loss: 0.4887 - accuracy: 0.8358\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4872 - accuracy: 0.8352\n",
      "10000/10000 - 1s - loss: 0.6197 - accuracy: 0.7962\n",
      "[1]\n",
      "10000/10000 - 1s - loss: 0.6197 - accuracy: 0.7962\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 195us/sample - loss: 3.2219 - accuracy: 0.7041\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.6549 - accuracy: 0.7736\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.5712 - accuracy: 0.7993\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.5382 - accuracy: 0.8125\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.4971 - accuracy: 0.8260\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.4888 - accuracy: 0.8331\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 0.4733 - accuracy: 0.8373\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 180us/sample - loss: 0.4727 - accuracy: 0.8395\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 196us/sample - loss: 0.4617 - accuracy: 0.8422\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.4546 - accuracy: 0.8450\n",
      "10000/10000 - 2s - loss: 0.5563 - accuracy: 0.8263\n",
      "[46]\n",
      "10000/10000 - 1s - loss: 1.4889 - accuracy: 0.5470\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 197us/sample - loss: 3.0648 - accuracy: 0.7005\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 179us/sample - loss: 0.7262 - accuracy: 0.7540\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.6293 - accuracy: 0.7851\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.5754 - accuracy: 0.8083\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 180us/sample - loss: 0.5574 - accuracy: 0.8157\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.5304 - accuracy: 0.8253\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 179us/sample - loss: 0.5158 - accuracy: 0.8306\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 177us/sample - loss: 0.4981 - accuracy: 0.8360\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.5051 - accuracy: 0.8383\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 0.4801 - accuracy: 0.8415\n",
      "10000/10000 - 2s - loss: 0.5438 - accuracy: 0.8324\n",
      "[20]\n",
      "10000/10000 - 1s - loss: 0.5505 - accuracy: 0.8299\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 14s 232us/sample - loss: 3.8922 - accuracy: 0.6950\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.7758 - accuracy: 0.7260\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.6362 - accuracy: 0.7729\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5552 - accuracy: 0.8101\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.5374 - accuracy: 0.8180\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5402 - accuracy: 0.8117\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.5105 - accuracy: 0.8299\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.5043 - accuracy: 0.8329\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4905 - accuracy: 0.8370\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.4837 - accuracy: 0.8411\n",
      "10000/10000 - 1s - loss: 0.5531 - accuracy: 0.8173\n",
      "[31]\n",
      "10000/10000 - 1s - loss: 1.2358 - accuracy: 0.6825\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 3.9186 - accuracy: 0.6982\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 176us/sample - loss: 0.7577 - accuracy: 0.7304\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.6441 - accuracy: 0.7630\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.5780 - accuracy: 0.7901\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5357 - accuracy: 0.8094\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.5618 - accuracy: 0.7873\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.5109 - accuracy: 0.8240\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.4955 - accuracy: 0.8304\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.4839 - accuracy: 0.8343\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 0.4778 - accuracy: 0.8370\n",
      "10000/10000 - 2s - loss: 0.5124 - accuracy: 0.8282\n",
      "[25]\n",
      "10000/10000 - 1s - loss: 1.4658 - accuracy: 0.6138\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 3.8709 - accuracy: 0.7129\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.6746 - accuracy: 0.7665\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.6043 - accuracy: 0.7856\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.5618 - accuracy: 0.8028\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.5290 - accuracy: 0.8165\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.4990 - accuracy: 0.8275\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4869 - accuracy: 0.8340\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.4808 - accuracy: 0.8365\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.4707 - accuracy: 0.8413\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.4729 - accuracy: 0.8414\n",
      "10000/10000 - 1s - loss: 0.5839 - accuracy: 0.8192\n",
      "[47]\n",
      "10000/10000 - 1s - loss: 1.5897 - accuracy: 0.4769\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 3.7856 - accuracy: 0.6728\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.7323 - accuracy: 0.7163\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.6503 - accuracy: 0.7496\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5711 - accuracy: 0.7942\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5542 - accuracy: 0.8045\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5259 - accuracy: 0.8157\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5158 - accuracy: 0.8189\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.5147 - accuracy: 0.8230\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5011 - accuracy: 0.8270\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.4882 - accuracy: 0.8340\n",
      "10000/10000 - 2s - loss: 0.5477 - accuracy: 0.8289\n",
      "[7]\n",
      "10000/10000 - 2s - loss: 0.5925 - accuracy: 0.8087\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 197us/sample - loss: 2.8650 - accuracy: 0.6680\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.7271 - accuracy: 0.7183\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.6402 - accuracy: 0.7469\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5972 - accuracy: 0.7761\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.5388 - accuracy: 0.8087\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.5203 - accuracy: 0.8193\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.5066 - accuracy: 0.8249\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5018 - accuracy: 0.8281\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.4986 - accuracy: 0.8310\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.4863 - accuracy: 0.8326\n",
      "10000/10000 - 2s - loss: 0.5492 - accuracy: 0.8179\n",
      "[7]\n",
      "10000/10000 - 2s - loss: 0.5757 - accuracy: 0.8065\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 197us/sample - loss: 3.2640 - accuracy: 0.6749\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.6911 - accuracy: 0.7405\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.6177 - accuracy: 0.7695\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.5884 - accuracy: 0.7823\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.5651 - accuracy: 0.7953\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.5211 - accuracy: 0.8207\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.5041 - accuracy: 0.8265\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.5089 - accuracy: 0.8281\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.4863 - accuracy: 0.8340\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.4811 - accuracy: 0.8350\n",
      "10000/10000 - 3348s - loss: 0.5403 - accuracy: 0.8205\n",
      "[53]\n",
      "10000/10000 - 1s - loss: 1.0528 - accuracy: 0.6384\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 3.8042 - accuracy: 0.6921\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 0.6794 - accuracy: 0.7550\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5681 - accuracy: 0.7973\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.5300 - accuracy: 0.8144\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.5055 - accuracy: 0.8244\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4974 - accuracy: 0.8301\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.4860 - accuracy: 0.8328\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4820 - accuracy: 0.8365\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.4786 - accuracy: 0.8368\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.4720 - accuracy: 0.8389\n",
      "10000/10000 - 1s - loss: 0.5420 - accuracy: 0.8337\n",
      "[32]\n",
      "10000/10000 - 1s - loss: 0.9401 - accuracy: 0.6713\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 3.8933 - accuracy: 0.6948\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.6699 - accuracy: 0.7642\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.5751 - accuracy: 0.7928\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5470 - accuracy: 0.8097\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.5168 - accuracy: 0.8237\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5015 - accuracy: 0.8293\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4829 - accuracy: 0.8358\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4777 - accuracy: 0.8378\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4720 - accuracy: 0.8395\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.4672 - accuracy: 0.8407\n",
      "10000/10000 - 1s - loss: 0.5154 - accuracy: 0.8320\n",
      "[48]\n",
      "10000/10000 - 1s - loss: 1.4974 - accuracy: 0.5023\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 3.4837 - accuracy: 0.6916\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.6540 - accuracy: 0.7625\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.5810 - accuracy: 0.7980\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.5399 - accuracy: 0.8116\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5234 - accuracy: 0.8200\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4992 - accuracy: 0.8295\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4830 - accuracy: 0.8386\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4735 - accuracy: 0.8420\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4688 - accuracy: 0.8424\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4654 - accuracy: 0.8456\n",
      "10000/10000 - 1s - loss: 0.5369 - accuracy: 0.8389\n",
      "[11]\n",
      "10000/10000 - 1s - loss: 1.0150 - accuracy: 0.7388\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 3.1359 - accuracy: 0.7156\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6391 - accuracy: 0.7823\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5769 - accuracy: 0.7984\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5503 - accuracy: 0.8107\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5032 - accuracy: 0.8275\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4834 - accuracy: 0.8384\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4734 - accuracy: 0.8405\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4690 - accuracy: 0.8422\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.4618 - accuracy: 0.8446\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.4579 - accuracy: 0.8457\n",
      "10000/10000 - 1s - loss: 0.5280 - accuracy: 0.8307\n",
      "[16]\n",
      "10000/10000 - 1s - loss: 0.6503 - accuracy: 0.7898\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 3.2760 - accuracy: 0.6905\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6713 - accuracy: 0.7510\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5708 - accuracy: 0.8039\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5457 - accuracy: 0.8155\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5111 - accuracy: 0.8252\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.4956 - accuracy: 0.8323\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.4823 - accuracy: 0.8371\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4741 - accuracy: 0.8421\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4700 - accuracy: 0.8425\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4689 - accuracy: 0.8425\n",
      "10000/10000 - 1s - loss: 0.5567 - accuracy: 0.8271\n",
      "[34]\n",
      "10000/10000 - 1s - loss: 1.0880 - accuracy: 0.6470\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 2.9145 - accuracy: 0.7267\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.6364 - accuracy: 0.7827\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.5737 - accuracy: 0.7974\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.5263 - accuracy: 0.8157\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.5037 - accuracy: 0.8237\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5144 - accuracy: 0.8199\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4923 - accuracy: 0.8307\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4821 - accuracy: 0.8342\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4694 - accuracy: 0.8367\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4652 - accuracy: 0.8410\n",
      "10000/10000 - 1s - loss: 0.6710 - accuracy: 0.8152\n",
      "[53]\n",
      "10000/10000 - 1s - loss: 1.4686 - accuracy: 0.5270\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 3.1517 - accuracy: 0.6734\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.7520 - accuracy: 0.7147\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6100 - accuracy: 0.7756\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.5718 - accuracy: 0.7952\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.5234 - accuracy: 0.8144\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.5037 - accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4986 - accuracy: 0.8250\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4945 - accuracy: 0.8297\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4824 - accuracy: 0.8322\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4844 - accuracy: 0.8350\n",
      "10000/10000 - 1s - loss: 0.5558 - accuracy: 0.8091\n",
      "[8]\n",
      "10000/10000 - 1s - loss: 1.1163 - accuracy: 0.6489\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 3.2017 - accuracy: 0.6776\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.7447 - accuracy: 0.7193\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.6830 - accuracy: 0.7323\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.6543 - accuracy: 0.7403\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.6212 - accuracy: 0.7455\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.6074 - accuracy: 0.7525\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.6009 - accuracy: 0.7524\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5860 - accuracy: 0.7563\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5780 - accuracy: 0.7812\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5539 - accuracy: 0.7950\n",
      "10000/10000 - 2s - loss: 0.6018 - accuracy: 0.7848\n",
      "[33]\n",
      "10000/10000 - 1s - loss: 0.8695 - accuracy: 0.7191\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 3.3368 - accuracy: 0.7214\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.6356 - accuracy: 0.7865\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5611 - accuracy: 0.8098\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5270 - accuracy: 0.8198\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.5126 - accuracy: 0.8239\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.4975 - accuracy: 0.8309\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4998 - accuracy: 0.8307\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4872 - accuracy: 0.8334\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4766 - accuracy: 0.8369\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4817 - accuracy: 0.8365\n",
      "10000/10000 - 2s - loss: 0.5493 - accuracy: 0.8214\n",
      "[59]\n",
      "10000/10000 - 1s - loss: 1.0445 - accuracy: 0.7011\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_rep):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10)\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    \n",
    "    n = np.random.choice(to_remove_list, 1)\n",
    "    \n",
    "    best_weights, _, nodes_removed = remove_nodes(acc, loss, model.get_weights(), n, 1, x_train, y_train, 0)\n",
    "    \n",
    "    model.set_weights(best_weights)\n",
    "    print(n)\n",
    "    \n",
    "    loss_new, acc_new = model.evaluate(x_test, y_test, verbose=2)\n",
    "    \n",
    "    loss_diff[i] = loss - loss_new\n",
    "    acc_diff[i] = acc_new - acc\n",
    "    loss_change[i] = loss_diff[i] / loss * 100\n",
    "    acc_change[i] = acc_diff[i] / acc * 100\n",
    "    num_nodes_removed[i] = n\n",
    "    nodes_removed_list += [nodes_removed]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 nodes removed\n",
      "Loss changes: [0. 0.]\n",
      "Accuracy changes: [0. 0.]\n",
      "#########################\n",
      "2 nodes removed\n",
      "Loss changes: [0. 0.]\n",
      "Accuracy changes: [0. 0.]\n",
      "#########################\n",
      "3 nodes removed\n",
      "Loss changes: [0.]\n",
      "Accuracy changes: [0.]\n",
      "#########################\n",
      "4 nodes removed\n",
      "Loss changes: [0.04453448]\n",
      "Accuracy changes: [0.01216022]\n",
      "#########################\n",
      "5 nodes removed\n",
      "Loss changes: [-0.18496728]\n",
      "Accuracy changes: [0.]\n",
      "#########################\n",
      "6 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "7 nodes removed\n",
      "Loss changes: [-42.59455126  -8.18437804  -4.82913439]\n",
      "Accuracy changes: [-10.12460556  -2.43695922  -1.39381151]\n",
      "#########################\n",
      "8 nodes removed\n",
      "Loss changes: [ 9.83436119e-05 -1.00845591e+02]\n",
      "Accuracy changes: [  0.         -19.79977819]\n",
      "#########################\n",
      "9 nodes removed\n",
      "Loss changes: [-0.55514907]\n",
      "Accuracy changes: [-0.08978924]\n",
      "#########################\n",
      "10 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "11 nodes removed\n",
      "Loss changes: [-34.6403519  -89.05737002]\n",
      "Accuracy changes: [ -9.95080088 -11.93229669]\n",
      "#########################\n",
      "12 nodes removed\n",
      "Loss changes: [-18.33592889]\n",
      "Accuracy changes: [-3.73854319]\n",
      "#########################\n",
      "13 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "14 nodes removed\n",
      "Loss changes: [-75.86154683  -0.73940414 -88.17254094 -29.42391553]\n",
      "Accuracy changes: [-18.50117053  -0.18761858 -16.95060631  -9.93121838]\n",
      "#########################\n",
      "15 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "16 nodes removed\n",
      "Loss changes: [-49.11813322 -23.14247044]\n",
      "Accuracy changes: [-14.53232465  -4.9235576 ]\n",
      "#########################\n",
      "17 nodes removed\n",
      "Loss changes: [-270.41825803 -138.67973814]\n",
      "Accuracy changes: [-33.88070722 -22.68938766]\n",
      "#########################\n",
      "18 nodes removed\n",
      "Loss changes: [-71.27492053 -70.32908897]\n",
      "Accuracy changes: [-12.99097671 -13.08808965]\n",
      "#########################\n",
      "19 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "20 nodes removed\n",
      "Loss changes: [ -67.06604044 -139.49775081  -62.78604311  -62.76575488   -1.2285556 ]\n",
      "Accuracy changes: [-13.06968734 -18.40439513 -11.09356126 -12.83636256  -0.30033608]\n",
      "#########################\n",
      "21 nodes removed\n",
      "Loss changes: [-31.04187769]\n",
      "Accuracy changes: [-5.51457211]\n",
      "#########################\n",
      "22 nodes removed\n",
      "Loss changes: [-39.84299706]\n",
      "Accuracy changes: [-11.33325322]\n",
      "#########################\n",
      "23 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "24 nodes removed\n",
      "Loss changes: [ -28.41337972 -132.9702191    -6.39703548]\n",
      "Accuracy changes: [ -4.82799797 -39.43817283  -2.13374706]\n",
      "#########################\n",
      "25 nodes removed\n",
      "Loss changes: [-186.05833628]\n",
      "Accuracy changes: [-25.88746654]\n",
      "#########################\n",
      "26 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "27 nodes removed\n",
      "Loss changes: [-33.94037137  -5.6523973  -29.40133285]\n",
      "Accuracy changes: [-10.99457086  -1.14376998  -5.85052448]\n",
      "#########################\n",
      "28 nodes removed\n",
      "Loss changes: [-211.20246616]\n",
      "Accuracy changes: [-37.31307418]\n",
      "#########################\n",
      "29 nodes removed\n",
      "Loss changes: [-46.13739201 -67.4168079 ]\n",
      "Accuracy changes: [ -9.86465218 -14.41474284]\n",
      "#########################\n",
      "30 nodes removed\n",
      "Loss changes: [-60.0788803]\n",
      "Accuracy changes: [-17.89499258]\n",
      "#########################\n",
      "31 nodes removed\n",
      "Loss changes: [-116.04808898 -123.42246427]\n",
      "Accuracy changes: [-20.105046   -16.49333333]\n",
      "#########################\n",
      "32 nodes removed\n",
      "Loss changes: [-73.43554072]\n",
      "Accuracy changes: [-19.47942989]\n",
      "#########################\n",
      "33 nodes removed\n",
      "Loss changes: [-22.53505187 -91.09520545 -44.46675902]\n",
      "Accuracy changes: [ -7.59556597 -12.15482689  -8.37155901]\n",
      "#########################\n",
      "34 nodes removed\n",
      "Loss changes: [-205.46180793 -233.47483252  -95.44778653]\n",
      "Accuracy changes: [-24.24830075 -33.95747563 -21.77487229]\n",
      "#########################\n",
      "35 nodes removed\n",
      "Loss changes: [-189.13863485   -1.99119117]\n",
      "Accuracy changes: [-43.7787994   -0.77249132]\n",
      "#########################\n",
      "36 nodes removed\n",
      "Loss changes: [-81.45048643]\n",
      "Accuracy changes: [-25.62917867]\n",
      "#########################\n",
      "37 nodes removed\n",
      "Loss changes: [-107.62058038 -181.13567258]\n",
      "Accuracy changes: [-21.55151543 -35.73330268]\n",
      "#########################\n",
      "38 nodes removed\n",
      "Loss changes: [ -82.28836137 -173.29140645]\n",
      "Accuracy changes: [-14.24094489 -34.74166508]\n",
      "#########################\n",
      "39 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "40 nodes removed\n",
      "Loss changes: [-117.17866065 -102.15949602 -114.98083455]\n",
      "Accuracy changes: [-20.42076314 -22.77154615 -26.39513843]\n",
      "#########################\n",
      "41 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "42 nodes removed\n",
      "Loss changes: [-221.34058581 -124.47903813  -24.78604314]\n",
      "Accuracy changes: [-44.65138327 -28.52250615  -8.75124902]\n",
      "#########################\n",
      "43 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "44 nodes removed\n",
      "Loss changes: [-44.40397406]\n",
      "Accuracy changes: [-10.59239345]\n",
      "#########################\n",
      "45 nodes removed\n",
      "Loss changes: [-199.34280874]\n",
      "Accuracy changes: [-42.37005488]\n",
      "#########################\n",
      "46 nodes removed\n",
      "Loss changes: [-151.25385059 -178.10606525 -167.64176524]\n",
      "Accuracy changes: [-27.96517982 -36.34799446 -33.80128593]\n",
      "#########################\n",
      "47 nodes removed\n",
      "Loss changes: [-172.25078232]\n",
      "Accuracy changes: [-41.78466512]\n",
      "#########################\n",
      "48 nodes removed\n",
      "Loss changes: [ -88.52634975 -294.04768939 -190.51903647]\n",
      "Accuracy changes: [-11.32626905 -46.80797421 -39.6274022 ]\n",
      "#########################\n",
      "49 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "50 nodes removed\n",
      "Loss changes: [-232.64853432 -156.01330167 -182.86760383]\n",
      "Accuracy changes: [-40.61046684 -27.82552776 -36.23656136]\n",
      "#########################\n",
      "51 nodes removed\n",
      "Loss changes: [-215.78877189 -190.68246685]\n",
      "Accuracy changes: [-49.35158475 -30.8816363 ]\n",
      "#########################\n",
      "52 nodes removed\n",
      "Loss changes: [-240.10598182 -135.19302111 -370.52272477]\n",
      "Accuracy changes: [-34.75449132 -31.11195951 -48.25447058]\n",
      "#########################\n",
      "53 nodes removed\n",
      "Loss changes: [-214.02637833 -193.11772426 -202.41952816  -94.84105309 -118.86192781]\n",
      "Accuracy changes: [-49.4612067  -48.13359667 -27.44146391 -22.1937836  -35.35328402]\n",
      "#########################\n",
      "54 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "55 nodes removed\n",
      "Loss changes: [-124.32939783]\n",
      "Accuracy changes: [-28.69720688]\n",
      "#########################\n",
      "56 nodes removed\n",
      "Loss changes: [ -38.12116491 -217.55586551]\n",
      "Accuracy changes: [ -6.5015091  -30.63555617]\n",
      "#########################\n",
      "57 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "58 nodes removed\n",
      "Loss changes: [-323.98377256]\n",
      "Accuracy changes: [-59.41993478]\n",
      "#########################\n",
      "59 nodes removed\n",
      "Loss changes: [ -86.11507307 -117.79081196  -90.16440931]\n",
      "Accuracy changes: [-24.03622464 -21.16016311 -14.64572643]\n",
      "#########################\n",
      "60 nodes removed\n",
      "Loss changes: [-306.60761749]\n",
      "Accuracy changes: [-60.57239603]\n",
      "#########################\n",
      "61 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "62 nodes removed\n",
      "Loss changes: [-296.55230087 -402.47718106]\n",
      "Accuracy changes: [-52.50030567 -56.09260993]\n",
      "#########################\n",
      "63 nodes removed\n",
      "Loss changes: []\n",
      "Accuracy changes: []\n",
      "#########################\n",
      "64 nodes removed\n",
      "Loss changes: [-182.26070332 -304.83915925]\n",
      "Accuracy changes: [-30.1690329  -55.00430983]\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 65):\n",
    "    print(f\"{i} nodes removed\")\n",
    "    print(\"Loss changes:\",loss_change[num_nodes_removed == i])\n",
    "    print(\"Accuracy changes:\",acc_change[num_nodes_removed == i])\n",
    "    print(\"#########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 2.8547 - accuracy: 0.7114\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.6164 - accuracy: 0.7834\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5554 - accuracy: 0.8067\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5156 - accuracy: 0.8238\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 178us/sample - loss: 0.5149 - accuracy: 0.8282- loss: 0.5167 - accuracy: 0.82 -\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 21604s 360ms/sample - loss: 0.4828 - accuracy: 0.8374- loss: 0.4873 - accuracy: - ETA: 42s - loss: 0.5132 - accuracy: 0. - ETA: 42s - loss - ETA: 38s - loss: 0.4869 - accurac\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.4719 - accuracy: 0.8396\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.4700 - accuracy: 0.8424\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 36398s 607ms/sample - loss: 0.4634 - accuracy: 0.84321:36:20 -  - ETA: 32:44 - loss: 0.4628 - accuracy:  - ETA: 24:34 - loss: 0.4630 - \n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 14s 234us/sample - loss: 0.4673 - accuracy: 0.8422\n",
      "10000/10000 - 3s - loss: 0.5573 - accuracy: 0.8277\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "1.5454864501940337e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0005999746513366766\n",
      "Found something better\n",
      "0.00776419440269469\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8304 --- Loss: 0.5292710494995118 --- Change: 0.00776419440269469 --- New tol: -0.00776419440269469\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "1.5454864501940337e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0004897526741027836\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5264222197532654 --- Change: 0.0004897526741027836 --- New tol: -0.008253947076797474\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "1.5454864501940337e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "5.481535911560087e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261481429576874 --- Change: 5.481535911560087e-05 --- New tol: -0.008308762435913076\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "1.5454864501940337e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404155254364 --- Change: 1.5454864501940337e-06 --- New tol: -0.00831030792236327\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "7.629394893626796e-11\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 7.629394893626796e-11 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151439667 --- Change: 0.0 --- New tol: -0.008310307998657219\n",
      "-2.651214603410779e-09\n",
      "Found something better\n",
      "-9.536749168148618e-12\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404151916504 --- Change: -9.536749168148618e-12 --- New tol: -0.008310307989120469\n",
      "-2.651214581206318e-09\n",
      "Found something better\n",
      "-4.5776367141314723e-10\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404174804688 --- Change: -4.5776367141314723e-10 --- New tol: -0.008310307531356798\n",
      "-2.651214581206318e-09\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404307365417 --- Change: -2.651214581206318e-09 --- New tol: -0.008310304880142217\n",
      "-1.1967849731453485e-06\n",
      "Found something better\n",
      "-7.36236573928295e-09\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261404675483704 --- Change: -7.36236573928295e-09 --- New tol: -0.008310297517776477\n",
      "-1.1967849731453485e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261464514732361 --- Change: -1.1967849731453485e-06 --- New tol: -0.008309100732803332\n",
      "-3.29872131346054e-06\n",
      "Found something better\n",
      "-2.044258117672726e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261566727638245 --- Change: -2.044258117672726e-06 --- New tol: -0.008307056474685659\n",
      "-6.11839294433203e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5261872647285462 --- Change: -6.11839294433203e-06 --- New tol: -0.008300938081741326\n",
      "-6.987037658690021e-06\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5262221999168396 --- Change: -6.987037658690021e-06 --- New tol: -0.008293951044082636\n",
      "-0.0021831324863433865\n",
      "Found something better\n",
      "-1.8271074295039825e-05\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8303 --- Loss: 0.5263135552883148 --- Change: -1.8271074295039825e-05 --- New tol: -0.008275679969787597\n",
      "-0.0021831324005126885\n",
      "Found something better\n",
      "-0.0005452426052093617\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8299 --- Loss: 0.5274399796485901 --- Change: -0.0005452426052093617 --- New tol: -0.007730437364578235\n",
      "-0.0021831797885894623\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8285 --- Loss: 0.5327556645870208 --- Change: -0.0021831797885894623 --- New tol: -0.005547257575988773\n",
      "10000/10000 - 1s - loss: 0.5328 - accuracy: 0.8285\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "end_not_reached = True\n",
    "improved = False\n",
    "size = 128\n",
    "tol = -1e-30\n",
    "current_pos = 0\n",
    "best_pos = -1\n",
    "best_change = tol\n",
    "original = model.get_weights()\n",
    "bas = [acc]\n",
    "bls = [loss]\n",
    "best_weights = model.get_weights()\n",
    "nodes_removed = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed = 0\n",
    "while end_not_reached or improved:\n",
    "    if not(end_not_reached):\n",
    "        end_not_reached = True\n",
    "        improved = False\n",
    "        current_pos = 0\n",
    "        size -= 1\n",
    "        nodes_removed += [best_pos]\n",
    "        best_weights[0][:,best_pos] = 0\n",
    "        best_weights[1][best_pos] = 0\n",
    "        best_weights[2][best_pos,:] = 0\n",
    "        best_pos = -1\n",
    "        tol -= best_change\n",
    "        ol = best_loss\n",
    "        oa = best_acc\n",
    "        bas += [best_acc]\n",
    "        bls += [best_loss]\n",
    "        print(\"Improvement has occured!! Accuracy:\", best_acc, \"--- Loss:\", best_loss, '--- Change:', best_change, '--- New tol:', tol)\n",
    "        best_change = tol\n",
    "        num_removed += 1\n",
    "    if current_pos in nodes_removed:\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed >= size:\n",
    "            end_not_reached = False\n",
    "        continue\n",
    "    w = copy.deepcopy(best_weights)\n",
    "    w[0][:,current_pos] = 0\n",
    "    w[1][current_pos] = 0\n",
    "    w[2][current_pos,:] = 0\n",
    "    model.set_weights(w)\n",
    "    nl, na = model.evaluate(x_test, y_test, verbose=0)\n",
    "    if 0.8*(na - oa) + 0.2*(ol - nl) >= best_change:\n",
    "        best_change = 0.8*(na - oa) + 0.2*(ol - nl)\n",
    "        print(best_change)\n",
    "        best_pos = current_pos\n",
    "        improved = True\n",
    "        best_acc = na\n",
    "        best_loss = nl\n",
    "        print(\"Found something better\")\n",
    "    current_pos += 1\n",
    "    if current_pos - num_removed >= size:\n",
    "        end_not_reached = False\n",
    "    if current_pos%20 == 0:\n",
    "        print(\"Did 20 iterations\")\n",
    "\n",
    "model.set_weights(best_weights)\n",
    "loss2, acc2 = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024536472129821796\n",
      "0.00079995394\n",
      "4.402802500385656\n",
      "0.09664780809544027\n",
      "116\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.55492324  0.          0.          0.          0.17086436  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.5797819   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.37631464  0.\n",
      "  0.21805815  0.          0.          0.          0.7172741   0.\n",
      "  0.8216421   0.          0.          0.          0.         -0.48516443\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.06365152 -0.10873702  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.18913214  0.\n",
      "  0.58155465  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(loss - loss2)\n",
    "print(acc2 - acc)\n",
    "print((loss - loss2)/loss * 100)\n",
    "print((acc2 - acc)/acc * 100)\n",
    "print(num_removed)\n",
    "print(best_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.35520445e-02 -8.29671975e-03 -5.35568409e-02 -3.01919542e-02\n",
      " -3.61214168e-02 -1.26946131e-02 -3.29394825e-02  9.89562250e-04\n",
      "  9.76379216e-03 -1.55554097e-02 -2.13282239e-02 -3.72213423e-02\n",
      " -2.39025634e-02 -1.00066233e-02 -3.21980081e-02 -2.01076251e-02\n",
      " -9.82459355e-03 -4.72169966e-02 -3.93438041e-02 -3.32839303e-02\n",
      " -8.45289230e-02 -3.72487418e-02 -4.38574478e-02  9.48753580e-03\n",
      " -5.54923236e-01 -1.77981462e-02 -4.05660905e-02 -3.53319235e-02\n",
      "  1.70864359e-01 -4.53706421e-02  7.00719946e-04  1.48898317e-02\n",
      " -4.26049978e-02 -4.43987511e-02 -2.29403414e-02 -5.45145497e-02\n",
      "  1.91695374e-02  1.64827798e-02 -1.67541616e-02  1.27871626e-03\n",
      " -8.86084046e-03 -3.19623761e-02 -4.19521742e-02 -1.56642459e-02\n",
      " -5.44507839e-02 -1.68461706e-02 -3.23471017e-02 -1.64868310e-02\n",
      " -2.58421432e-02 -5.79781890e-01  4.44697728e-03 -3.34452763e-02\n",
      " -5.26826642e-03 -2.52922121e-02 -2.47266442e-02 -6.63382933e-03\n",
      " -2.96660177e-02 -3.14345919e-02 -1.28267228e-03 -3.54811139e-02\n",
      " -2.05833837e-02  9.42861140e-02  8.41025338e-02 -2.09084824e-02\n",
      "  3.76314640e-01 -9.95146111e-03  2.18058154e-01 -1.84187051e-02\n",
      " -1.74554735e-02 -2.13284735e-02  7.17274129e-01 -1.85909290e-02\n",
      "  8.21642101e-01 -1.03342663e-02 -1.87953115e-02  6.49318891e-03\n",
      " -3.11857834e-02 -4.85164434e-01 -4.38215733e-02 -5.77042140e-02\n",
      " -2.57658511e-02 -3.10352612e-02  1.69713706e-01 -2.74672545e-02\n",
      " -1.93548482e-02 -6.36515170e-02 -1.08737022e-01 -8.21061898e-03\n",
      " -3.39777172e-02 -5.33693880e-02 -3.54691111e-02 -2.54858527e-02\n",
      " -3.56568769e-02  1.92256421e-01 -3.48286554e-02 -4.20732200e-02\n",
      " -2.65415367e-02  1.28892716e-02 -7.22248165e-04 -4.72772159e-02\n",
      " -1.36113279e-02 -3.40865888e-02 -4.58636656e-02  2.00716779e-01\n",
      " -4.18409333e-02 -2.88299676e-02 -1.89132139e-01  1.32252313e-02\n",
      "  5.81554651e-01 -3.93272862e-02  5.54901082e-03  1.25154629e-02\n",
      " -2.02622525e-02 -1.10066980e-02 -1.09423511e-02 -1.94951445e-02\n",
      "  1.28357457e-02 -5.76464236e-02  7.21179135e-03 -4.80953455e-02\n",
      " -3.50442668e-03 -3.45133170e-02 -2.77245343e-02 -3.27065140e-02\n",
      " -1.31970504e-02 -4.37978245e-02 -1.51099004e-02 -2.02709436e-02]\n"
     ]
    }
   ],
   "source": [
    "or_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "or_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "or_model.set_weights(original)\n",
    "print(or_model.get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 - 4s - loss: 0.4512 - accuracy: 0.8487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4511857439637184, 0.84866667]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train, y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 - 3s - loss: 0.4545 - accuracy: 0.8495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45453119107882184, 0.8494833]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_model.evaluate(x_train, y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.5328 - accuracy: 0.8285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5327556645870208, 0.8285]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.5573 - accuracy: 0.8277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5572921367168426, 0.8277]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 2.8388 - accuracy: 0.2358\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 1.6076 - accuracy: 0.3043\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.4493 - accuracy: 0.3878\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.3024 - accuracy: 0.4735\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.1999 - accuracy: 0.5158\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.0712 - accuracy: 0.5661\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.9916 - accuracy: 0.5918\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.9755 - accuracy: 0.5972\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.9444 - accuracy: 0.6111\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.9075 - accuracy: 0.6229\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.8944 - accuracy: 0.6280\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.8902 - accuracy: 0.6369\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.8476 - accuracy: 0.6558\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.8359 - accuracy: 0.6564\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.8298 - accuracy: 0.6568\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.8169 - accuracy: 0.6610\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.8233 - accuracy: 0.6581\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.8148 - accuracy: 0.6611\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.8141 - accuracy: 0.6630\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.8142 - accuracy: 0.6613\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.8096 - accuracy: 0.6616\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.8092 - accuracy: 0.6658\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.8109 - accuracy: 0.6615\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7985 - accuracy: 0.6662\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.8076 - accuracy: 0.6653\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.8166 - accuracy: 0.6658\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7966 - accuracy: 0.6676\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.7934 - accuracy: 0.6694\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.8048 - accuracy: 0.6668\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.7940 - accuracy: 0.6681\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.7930 - accuracy: 0.6683\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7973 - accuracy: 0.6694\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7874 - accuracy: 0.6718\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.8043 - accuracy: 0.6659\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7945 - accuracy: 0.6684\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7920 - accuracy: 0.6696\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7925 - accuracy: 0.6741\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.7761 - accuracy: 0.6752\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7793 - accuracy: 0.6744\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.7902 - accuracy: 0.6723\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.7687 - accuracy: 0.6787\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.7804 - accuracy: 0.6728\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.7747 - accuracy: 0.6736\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7764 - accuracy: 0.6722\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.7793 - accuracy: 0.6712\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7670 - accuracy: 0.6791\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7729 - accuracy: 0.6762\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7686 - accuracy: 0.6792\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7752 - accuracy: 0.6723\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7747 - accuracy: 0.6750\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7693 - accuracy: 0.6773\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.7672 - accuracy: 0.6810\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7712 - accuracy: 0.6740\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7668 - accuracy: 0.6828\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.7709 - accuracy: 0.6788\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.7673 - accuracy: 0.6780\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7601 - accuracy: 0.6848\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7588 - accuracy: 0.6815\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7653 - accuracy: 0.6802\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7654 - accuracy: 0.6842\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7657 - accuracy: 0.6833\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7541 - accuracy: 0.6852\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.7590 - accuracy: 0.6866\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7401 - accuracy: 0.6961\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7604 - accuracy: 0.6838\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.7509 - accuracy: 0.6885\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7576 - accuracy: 0.6883\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7486 - accuracy: 0.6910\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7564 - accuracy: 0.6885\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7544 - accuracy: 0.6854\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7514 - accuracy: 0.6921\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7365 - accuracy: 0.7004\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.7297 - accuracy: 0.7010\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7432 - accuracy: 0.6948\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.7309 - accuracy: 0.7013\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7436 - accuracy: 0.6962\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.7340 - accuracy: 0.6978\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7349 - accuracy: 0.7006\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.7444 - accuracy: 0.6989\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7217 - accuracy: 0.7066\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7376 - accuracy: 0.7066\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7211 - accuracy: 0.7071\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7159 - accuracy: 0.7095\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7273 - accuracy: 0.7086\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7152 - accuracy: 0.7131\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.7161 - accuracy: 0.7143\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7112 - accuracy: 0.7159\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.7049 - accuracy: 0.7197\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6944 - accuracy: 0.7207\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6888 - accuracy: 0.7239\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.6905 - accuracy: 0.7219\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.6887 - accuracy: 0.7225\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6915 - accuracy: 0.7235\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.6938 - accuracy: 0.7251\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6796 - accuracy: 0.7268\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.6922 - accuracy: 0.7235\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.6911 - accuracy: 0.7259\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.6805 - accuracy: 0.7275\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.6857 - accuracy: 0.7252\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6797 - accuracy: 0.7276\n",
      "10000/10000 - 1s - loss: 0.7860 - accuracy: 0.7123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7859716332435608, 0.7123]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(12, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "red_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "red_model.fit(x_train, y_train, epochs=100, verbose=1)\n",
    "red_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.9309 - accuracy: 0.6176\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.9274 - accuracy: 0.6191\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.9258 - accuracy: 0.6201\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.9253 - accuracy: 0.6199\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.9147 - accuracy: 0.6251\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.9189 - accuracy: 0.6193\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.9179 - accuracy: 0.6217\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.9160 - accuracy: 0.6224\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.9173 - accuracy: 0.6227\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.9219 - accuracy: 0.6238\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.9121 - accuracy: 0.6259\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.9078 - accuracy: 0.6270\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.9114 - accuracy: 0.6257\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.9052 - accuracy: 0.6292\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.9133 - accuracy: 0.6235\n",
      "10000/10000 - 0s - loss: 0.9421 - accuracy: 0.6255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9421217614173889, 0.6255]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model.fit(x_train, y_train, epochs=15, verbose=1)\n",
    "red_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml h5py  \n",
    "# Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./reduced/fashion_mnist_128_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_model.save_weights('./original/fashion_mnist_128_12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = [np.zeros((best_weights[0].shape[0], best_weights[0].shape[1] - num_removed)), np.zeros((best_weights[1].shape[0] - num_removed)), np.zeros((best_weights[2].shape[0] - num_removed, best_weights[2].shape[1])), best_weights[3]]\n",
    "\n",
    "j = 0\n",
    "for i in range(len(best_weights[1])):\n",
    "    if i not in nodes_removed:\n",
    "        new_weights[0][:, j] = best_weights[0][:, i]\n",
    "        new_weights[1][j] = best_weights[1][i]\n",
    "        new_weights[2][j, :] = best_weights[2][i, :]\n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.5328 - accuracy: 0.8285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5327556574344635, 0.8285]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(12, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "red_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "red_model.set_weights(new_weights)\n",
    "red_model.save_weights('./full_reduced/fashion_mnist_128_12')\n",
    "red_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.4102 - accuracy: 0.8588\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3810 - accuracy: 0.8645\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 0s 7us/sample - loss: 0.3697 - accuracy: 0.8678\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3640 - accuracy: 0.8695\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3612 - accuracy: 0.8698\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 1s 8us/sample - loss: 0.3577 - accuracy: 0.8722\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3559 - accuracy: 0.8725\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3546 - accuracy: 0.8723\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3530 - accuracy: 0.8737\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3521 - accuracy: 0.8734\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3511 - accuracy: 0.8737\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3499 - accuracy: 0.8751\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3491 - accuracy: 0.8748\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3483 - accuracy: 0.8753\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 0s 8us/sample - loss: 0.3476 - accuracy: 0.8747\n",
      "10000/10000 - 0s - loss: 0.4444 - accuracy: 0.8531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44439594821929934, 0.8531]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model.set_weights(new_weights)\n",
    "red_model.fit(x_train, y_train, epochs=15, verbose=1, batch_size=4096)\n",
    "red_model.evaluate(x_test, y_test, verbose=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.5328 - accuracy: 0.8285\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.3895 - accuracy: 0.8636\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3679 - accuracy: 0.8694\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3596 - accuracy: 0.8719\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3580 - accuracy: 0.8724\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3528 - accuracy: 0.8745\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3551 - accuracy: 0.8727\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3511 - accuracy: 0.8743\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3466 - accuracy: 0.8760\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3454 - accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 0.3458 - accuracy: 0.8752\n",
      "10000/10000 - 1s - loss: 0.4515 - accuracy: 0.8530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4514933671712875, 0.853]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "new_model.set_weights(best_weights)\n",
    "new_model.evaluate(x_test, y_test, verbose=2)\n",
    "new_model.fit(x_train, y_train, epochs=10, verbose=1, batch_size=2048)\n",
    "new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.4515 - accuracy: 0.8530\n",
      "Node 0: 0.0\n",
      "0.0\n",
      "Found something better\n",
      "Node 1: 0.0\n",
      "Node 2: 0.0\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Did 20 iterations\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: -0.18667189320087435\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: -0.009689953212738045\n",
      "Node 29: 0.0\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Did 20 iterations\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: 0.0\n",
      "Node 45: 0.0\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: -0.10380605744838717\n",
      "Node 50: 0.0\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0\n",
      "Node 59: 0.0\n",
      "Did 20 iterations\n",
      "Node 60: 0.0\n",
      "Node 61: 0.0\n",
      "Node 62: 0.0\n",
      "Node 63: 0.0\n",
      "Node 64: -0.009742786335945131\n",
      "Node 65: 0.0\n",
      "Node 66: -0.11662762632846833\n",
      "Node 67: 0.0\n",
      "Node 68: 0.0\n",
      "Node 69: 0.0\n",
      "Node 70: -0.10773643366336823\n",
      "Node 71: 0.0\n",
      "Node 72: -0.028416832709312438\n",
      "Node 73: 0.0\n",
      "Node 74: 0.0\n",
      "Node 75: 0.0\n",
      "Node 76: 0.0\n",
      "Node 77: -0.1718921076154709\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0\n",
      "Did 20 iterations\n",
      "Node 80: 0.0\n",
      "Node 81: 0.0\n",
      "Node 82: 0.0\n",
      "Node 83: 0.0\n",
      "Node 84: 0.0\n",
      "Node 85: -0.041709439458847045\n",
      "Node 86: -0.049924930863380436\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0\n",
      "Node 90: 0.0\n",
      "Node 91: 0.0\n",
      "Node 92: 0.0\n",
      "Node 93: 0.0\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: 0.0\n",
      "Node 97: 0.0\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Did 20 iterations\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: 0.0\n",
      "Node 106: -0.1299862809896469\n",
      "Node 107: 0.0\n",
      "Node 108: -0.11099927045345306\n",
      "Node 109: 0.0\n",
      "Node 110: 0.0\n",
      "Node 111: 0.0\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: 0.0\n",
      "Node 115: 0.0\n",
      "Node 116: 0.0\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0\n",
      "Did 20 iterations\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: 0.0\n",
      "Node 123: 0.0\n",
      "Node 124: 0.0\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0\n",
      "Node 127: 0.0\n",
      "Improvement has occured!! Accuracy: 0.853 --- Loss: 0.4514933671712875 --- Change: 0.0 --- New tol: -1e-30\n",
      "Node 1: 0.0\n",
      "0.0\n",
      "Found something better\n",
      "Node 2: 0.0\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-411-75eafbedb268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mnl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Node {current_pos}:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moa\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mol\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moa\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mol\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_change\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \"\"\"\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, acc = new_model.evaluate(x_test, y_test, verbose=2)\n",
    "end_not_reached = True\n",
    "improved = False\n",
    "size = 128\n",
    "tol = -1e-30\n",
    "current_pos = 0\n",
    "best_pos = -1\n",
    "best_change = tol\n",
    "original2 = new_model.get_weights()\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights2 = new_model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "while end_not_reached or improved:\n",
    "    if not(end_not_reached):\n",
    "        end_not_reached = True\n",
    "        improved = False\n",
    "        current_pos = 0\n",
    "        size -= 1\n",
    "        nodes_removed2 += [best_pos]\n",
    "        best_weights2[0][:,best_pos] = 0\n",
    "        best_weights2[1][best_pos] = 0\n",
    "        best_weights2[2][best_pos,:] = 0\n",
    "        best_pos = -1\n",
    "        tol -= best_change\n",
    "        ol = best_loss\n",
    "        oa = best_acc\n",
    "        bas2 += [best_acc]\n",
    "        bls2 += [best_loss]\n",
    "        print(\"Improvement has occured!! Accuracy:\", best_acc, \"--- Loss:\", best_loss, '--- Change:', best_change, '--- New tol:', tol)\n",
    "        best_change = tol\n",
    "        num_removed2 += 1\n",
    "    if current_pos in nodes_removed2:\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            end_not_reached = False\n",
    "        continue\n",
    "    w = copy.deepcopy(best_weights2)\n",
    "    w[0][:,current_pos] = 0\n",
    "    w[1][current_pos] = 0\n",
    "    w[2][current_pos,:] = 0\n",
    "    new_model.set_weights(w)\n",
    "    nl, na = new_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {current_pos}:\", 0.8*(na - oa) + 0.2*(ol - nl))\n",
    "    if 0.8*(na - oa) + 0.2*(ol - nl) > best_change:\n",
    "        best_change = 0.8*(na - oa) + 0.2*(ol - nl)\n",
    "        print(best_change)\n",
    "        best_pos = current_pos\n",
    "        improved = True\n",
    "        best_acc = na\n",
    "        best_loss = nl\n",
    "        print(\"Found something better\")\n",
    "    current_pos += 1\n",
    "    if current_pos - num_removed2 >= size:\n",
    "        end_not_reached = False\n",
    "    if current_pos%20 == 0:\n",
    "        print(\"Did 20 iterations\")\n",
    "\n",
    "new_model.set_weights(best_weights2)\n",
    "loss2, acc2 = new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.5573 - accuracy: 0.8277\n",
      "Node 0: 0.0\n",
      "Node 1: 0.0\n",
      "Node 2: 0.0\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 3.0909729003880674e-06\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: -0.21466120378494266\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: -0.0012001917839050249\n",
      "Node 29: 0.0\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Node 40: -3.36151123070394e-07\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: -2.3934936523417607e-06\n",
      "Node 44: 0.0\n",
      "Node 45: 0.0\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: -0.10663373857498168\n",
      "Node 50: -4.79598999025832e-06\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0\n",
      "Node 56: 0.0\n",
      "Node 57: -1.3974075317380042e-05\n",
      "Node 58: 0.0\n",
      "Node 59: 0.0\n",
      "Node 60: 0.0\n",
      "Node 61: -0.002966270942687999\n",
      "Node 62: -3.6526679992698877e-05\n",
      "Node 63: 0.0\n",
      "Node 64: -0.007828902339935296\n",
      "Node 65: 0.0\n",
      "Node 66: -0.1352027407646179\n",
      "Node 67: 0.0\n",
      "Node 68: 0.0\n",
      "Node 69: 0.0\n",
      "Node 70: -0.13564257736206056\n",
      "Node 71: 0.0\n",
      "Node 72: -0.021314487819671647\n",
      "Node 73: 0.0\n",
      "Node 74: -2.750549316399287e-06\n",
      "Node 75: 0.0\n",
      "Node 76: 0.0\n",
      "Node 77: -0.23124059846878053\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0\n",
      "Node 80: 0.0\n",
      "Node 81: 0.0\n",
      "Node 82: 0.0011999493026733532\n",
      "Node 83: 0.0\n",
      "Node 84: 0.0\n",
      "Node 85: -0.0396855739593506\n",
      "Node 86: -0.019666539087295522\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0\n",
      "Node 90: 0.0\n",
      "Node 91: 0.0\n",
      "Node 92: 0.0\n",
      "Node 93: 0.012828417606353736\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: 0.0\n",
      "Node 97: 0.00024046285629273357\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: -0.000875942611694347\n",
      "Node 104: 0.0\n",
      "Node 105: 0.0\n",
      "Node 106: -0.18386298566818238\n",
      "Node 107: 0.0\n",
      "Node 108: -0.07952018777847289\n",
      "Node 109: 0.0\n",
      "Node 110: 0.0\n",
      "Node 111: 0.0\n",
      "Node 112: 0.0\n",
      "Node 113: -2.57461547850113e-06\n",
      "Node 114: 3.064270019503823e-06\n",
      "Node 115: 0.0\n",
      "Node 116: 0.0\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: 0.0\n",
      "Node 123: 0.0\n",
      "Node 124: 0.0\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0\n",
      "Node 127: 0.0\n"
     ]
    }
   ],
   "source": [
    "l, a = or_model.evaluate(x_test, y_test, verbose=2)\n",
    "or_weights = or_model.get_weights()\n",
    "size = 128\n",
    "for i in range(128):\n",
    "    w = copy.deepcopy(or_weights)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.6*(na - a) + 0.4*(l - nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 3.0740 - accuracy: 0.6916\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.6885 - accuracy: 0.7359\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.5880 - accuracy: 0.7849\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.5474 - accuracy: 0.8037\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.5234 - accuracy: 0.8121\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.5118 - accuracy: 0.8171\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.5062 - accuracy: 0.8194\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4946 - accuracy: 0.8250\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.4765 - accuracy: 0.8315\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.4736 - accuracy: 0.8328\n",
      "10000/10000 - 1s - loss: 0.5564 - accuracy: 0.8208\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 3.2911 - accuracy: 0.6711\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.7028 - accuracy: 0.7201\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.6215 - accuracy: 0.7564\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5494 - accuracy: 0.8039\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5247 - accuracy: 0.8154\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5197 - accuracy: 0.8200\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4950 - accuracy: 0.8268\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.4966 - accuracy: 0.8284\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4910 - accuracy: 0.8309\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4822 - accuracy: 0.8345\n",
      "10000/10000 - 1s - loss: 0.5633 - accuracy: 0.8223\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 2.9783 - accuracy: 0.6793\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.6973 - accuracy: 0.7459\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.6104 - accuracy: 0.7827\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5689 - accuracy: 0.8012\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5410 - accuracy: 0.8125\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5281 - accuracy: 0.8175\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5215 - accuracy: 0.8227\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5121 - accuracy: 0.8248\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4994 - accuracy: 0.8285\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5016 - accuracy: 0.8313\n",
      "10000/10000 - 1s - loss: 0.5656 - accuracy: 0.8165\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 2.9621 - accuracy: 0.6755\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.7425 - accuracy: 0.7123\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.6266 - accuracy: 0.7442\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.5688 - accuracy: 0.7905\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.5339 - accuracy: 0.8088\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5206 - accuracy: 0.8174\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5092 - accuracy: 0.8238\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4985 - accuracy: 0.8243\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4900 - accuracy: 0.8306\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4846 - accuracy: 0.8321\n",
      "10000/10000 - 1s - loss: 0.5629 - accuracy: 0.8139\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 3.2868 - accuracy: 0.6900\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.6579 - accuracy: 0.7616\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5707 - accuracy: 0.7951\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5438 - accuracy: 0.8107\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5179 - accuracy: 0.8221\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5137 - accuracy: 0.8229\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5097 - accuracy: 0.8251\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4962 - accuracy: 0.8296\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4953 - accuracy: 0.8296\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4934 - accuracy: 0.8304\n",
      "10000/10000 - 1s - loss: 0.5729 - accuracy: 0.8215\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 3.4865 - accuracy: 0.6743\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.7114 - accuracy: 0.7165\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.6359 - accuracy: 0.7639\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5613 - accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.5362 - accuracy: 0.8155\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5115 - accuracy: 0.8250\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5077 - accuracy: 0.8285\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4904 - accuracy: 0.8321\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.4882 - accuracy: 0.8343\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.4845 - accuracy: 0.8359\n",
      "10000/10000 - 1s - loss: 0.5649 - accuracy: 0.8088\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 3.0570 - accuracy: 0.7089\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.6557 - accuracy: 0.7681\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5680 - accuracy: 0.8042\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.5265 - accuracy: 0.8189\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.5077 - accuracy: 0.8280\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4881 - accuracy: 0.8328\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4783 - accuracy: 0.8351\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.4665 - accuracy: 0.8394\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4669 - accuracy: 0.8411\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4624 - accuracy: 0.8407\n",
      "10000/10000 - 1s - loss: 0.5305 - accuracy: 0.8378\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 3.8109 - accuracy: 0.6993\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.6580 - accuracy: 0.7750\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5553 - accuracy: 0.8108\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.5178 - accuracy: 0.8244\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s 145us/sample - loss: 0.4960 - accuracy: 0.8332\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4737 - accuracy: 0.8396\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4717 - accuracy: 0.8415\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4644 - accuracy: 0.8422\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4592 - accuracy: 0.8452\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4510 - accuracy: 0.8485\n",
      "10000/10000 - 1s - loss: 0.5061 - accuracy: 0.8316\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 2.6439 - accuracy: 0.6834\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.6702 - accuracy: 0.7626\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5666 - accuracy: 0.8054\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5271 - accuracy: 0.8161\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5159 - accuracy: 0.8215\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.5087 - accuracy: 0.8247\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4924 - accuracy: 0.8295\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4889 - accuracy: 0.8295\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4921 - accuracy: 0.8321\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4774 - accuracy: 0.8351\n",
      "10000/10000 - 1s - loss: 0.5304 - accuracy: 0.8271\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 3.4581 - accuracy: 0.7031\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.7377 - accuracy: 0.7456\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.6142 - accuracy: 0.7882\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5641 - accuracy: 0.8085\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5340 - accuracy: 0.8217\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5165 - accuracy: 0.8294\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 9s 153us/sample - loss: 0.5062 - accuracy: 0.8321\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.4985 - accuracy: 0.8358\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4908 - accuracy: 0.8393\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4940 - accuracy: 0.8384\n",
      "10000/10000 - 1s - loss: 0.5563 - accuracy: 0.8241\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 2.7398 - accuracy: 0.7090\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.6523 - accuracy: 0.7813\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5822 - accuracy: 0.7995\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5340 - accuracy: 0.8154\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5085 - accuracy: 0.8252\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4988 - accuracy: 0.8281\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4910 - accuracy: 0.8300\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4832 - accuracy: 0.8320\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4751 - accuracy: 0.8357\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.4738 - accuracy: 0.8362\n",
      "10000/10000 - 1s - loss: 0.6339 - accuracy: 0.8092\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 3.2318 - accuracy: 0.6720\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.7577 - accuracy: 0.7016\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.6420 - accuracy: 0.7526\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5621 - accuracy: 0.7991\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5379 - accuracy: 0.8146\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5211 - accuracy: 0.8216\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.5049 - accuracy: 0.8282\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5068 - accuracy: 0.8304\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4989 - accuracy: 0.8331\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4923 - accuracy: 0.8351\n",
      "10000/10000 - 1s - loss: 0.5177 - accuracy: 0.8328\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 3.0401 - accuracy: 0.6777\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.7185 - accuracy: 0.7288\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.6414 - accuracy: 0.7584\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5947 - accuracy: 0.7717\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.5681 - accuracy: 0.7800\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.5413 - accuracy: 0.8035\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.5000 - accuracy: 0.8267\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5000 - accuracy: 0.8289\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4866 - accuracy: 0.8347\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.4838 - accuracy: 0.8338\n",
      "10000/10000 - 1s - loss: 0.6074 - accuracy: 0.8046\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 3.3619 - accuracy: 0.7108\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.6657 - accuracy: 0.7725\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 9s 148us/sample - loss: 0.5859 - accuracy: 0.7931\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.5429 - accuracy: 0.8112\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.5167 - accuracy: 0.8232\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4910 - accuracy: 0.8317\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4830 - accuracy: 0.8349\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4732 - accuracy: 0.8402\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4741 - accuracy: 0.8393\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4636 - accuracy: 0.8433\n",
      "10000/10000 - 1s - loss: 0.5061 - accuracy: 0.8416\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 2.6589 - accuracy: 0.7110\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.6522 - accuracy: 0.7663\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.5890 - accuracy: 0.7875\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5521 - accuracy: 0.8023\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5195 - accuracy: 0.8190\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.5039 - accuracy: 0.8254\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4918 - accuracy: 0.8298\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4888 - accuracy: 0.8311\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4825 - accuracy: 0.8346\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4710 - accuracy: 0.8398\n",
      "10000/10000 - 1s - loss: 0.7518 - accuracy: 0.8042\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 4.4608 - accuracy: 0.6860\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 9s 153us/sample - loss: 0.7266 - accuracy: 0.7252\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 9s 147us/sample - loss: 0.6379 - accuracy: 0.7627\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.5817 - accuracy: 0.7881\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5597 - accuracy: 0.7998\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5265 - accuracy: 0.8207\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4993 - accuracy: 0.8298\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4891 - accuracy: 0.8351\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4862 - accuracy: 0.8386\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4834 - accuracy: 0.8386\n",
      "10000/10000 - 1s - loss: 0.5293 - accuracy: 0.8285\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 3.0174 - accuracy: 0.6694\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.7030 - accuracy: 0.7306\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.6064 - accuracy: 0.7850\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5632 - accuracy: 0.7992\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5288 - accuracy: 0.8128\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5157 - accuracy: 0.8177\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5011 - accuracy: 0.8252\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4963 - accuracy: 0.8271\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4892 - accuracy: 0.8302\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4810 - accuracy: 0.8323\n",
      "10000/10000 - 1s - loss: 0.5263 - accuracy: 0.8136\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 2.7194 - accuracy: 0.7122\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.6778 - accuracy: 0.7689\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.5844 - accuracy: 0.8055\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5425 - accuracy: 0.8183\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5253 - accuracy: 0.8251\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5055 - accuracy: 0.8295\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.4888 - accuracy: 0.8340\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.4835 - accuracy: 0.8348\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4759 - accuracy: 0.8372\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4618 - accuracy: 0.8401\n",
      "10000/10000 - 1s - loss: 0.5650 - accuracy: 0.8193\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 3.1256 - accuracy: 0.7147\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.6231 - accuracy: 0.7793\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5537 - accuracy: 0.8116\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5116 - accuracy: 0.8246\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.5032 - accuracy: 0.8281\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4949 - accuracy: 0.8312\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4820 - accuracy: 0.8369\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.4698 - accuracy: 0.8380\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4634 - accuracy: 0.8424\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4629 - accuracy: 0.8433\n",
      "10000/10000 - 1s - loss: 0.5872 - accuracy: 0.8106\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 2.6506 - accuracy: 0.6855\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.6347 - accuracy: 0.7765\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.5577 - accuracy: 0.8050\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.5159 - accuracy: 0.8219\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.5147 - accuracy: 0.8240\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.4954 - accuracy: 0.8319\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4837 - accuracy: 0.8350\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4747 - accuracy: 0.8383\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4691 - accuracy: 0.8404\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.4698 - accuracy: 0.8407\n",
      "10000/10000 - 1s - loss: 0.5130 - accuracy: 0.8327\n"
     ]
    }
   ],
   "source": [
    "num_test = 20\n",
    "num_zeros = np.zeros(num_test)\n",
    "num_worse = np.zeros(num_test)\n",
    "num_important = np.zeros(num_test)\n",
    "losses = np.zeros(num_test)\n",
    "accs = np.zeros(num_test)\n",
    "zero_nodes = []\n",
    "worsening_nodes = []\n",
    "important_nodes = []\n",
    "tol = -1e-4\n",
    "for j in range(num_test):\n",
    "    blank_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    blank_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    blank_model.fit(x_train, y_train, epochs=10)\n",
    "    l, a = blank_model.evaluate(x_test, y_test, verbose=2)\n",
    "    losses[j] = l\n",
    "    accs[j] = a\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(128):\n",
    "        w = blank_model.get_weights()\n",
    "        w[0][:,i] = 0\n",
    "        w[1][i] = 0\n",
    "        w[2][i,:] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "        change = 0.8*(na - a) + 0.2*(l - nl)\n",
    "        if change <= 0 and change >= tol:\n",
    "            num_zeros[j] += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse[j] += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important[j] += 1\n",
    "            imp += [i]\n",
    "    zero_nodes += [z]\n",
    "    worsening_nodes += [wr]\n",
    "    important_nodes += [imp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110. 113. 106. 109. 108. 112. 106. 104. 108. 112. 109. 113. 110. 109.\n",
      " 109. 111. 113. 108. 107. 113.]\n",
      "[3. 2. 6. 5. 7. 3. 5. 7. 2. 3. 4. 2. 1. 2. 3. 4. 4. 4. 6. 1.]\n",
      "[15. 13. 16. 14. 13. 13. 17. 17. 18. 13. 15. 13. 17. 17. 16. 13. 11. 16.\n",
      " 15. 14.]\n"
     ]
    }
   ],
   "source": [
    "print(num_zeros)\n",
    "print(num_worse)\n",
    "print(num_important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to reduce overfitting through node removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 3.2576 - accuracy: 0.6708\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.7225 - accuracy: 0.7218\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.6401 - accuracy: 0.7577\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.6032 - accuracy: 0.7704\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.5604 - accuracy: 0.7876\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.5519 - accuracy: 0.7950\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.5333 - accuracy: 0.8010\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.5292 - accuracy: 0.8036\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.5070 - accuracy: 0.8197\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4788 - accuracy: 0.8337\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4706 - accuracy: 0.8367\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4665 - accuracy: 0.8400\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4605 - accuracy: 0.8435\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4598 - accuracy: 0.8451\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4573 - accuracy: 0.8445\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4560 - accuracy: 0.8450\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4570 - accuracy: 0.8471\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4528 - accuracy: 0.8470\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4446 - accuracy: 0.8488\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4520 - accuracy: 0.8491\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4436 - accuracy: 0.8501\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4416 - accuracy: 0.8497\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4419 - accuracy: 0.8497\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4513 - accuracy: 0.8510\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4396 - accuracy: 0.8535\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4455 - accuracy: 0.8522\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.4392 - accuracy: 0.8529\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4365 - accuracy: 0.8537\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4307 - accuracy: 0.8543\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4333 - accuracy: 0.8534\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4366 - accuracy: 0.8537\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4401 - accuracy: 0.8533\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4305 - accuracy: 0.8532\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4407 - accuracy: 0.8529\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4273 - accuracy: 0.8553\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4391 - accuracy: 0.8534\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4322 - accuracy: 0.8549\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4286 - accuracy: 0.8576\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4301 - accuracy: 0.8548\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4396 - accuracy: 0.8534\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4309 - accuracy: 0.8562\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4238 - accuracy: 0.8563\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4258 - accuracy: 0.8568\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4357 - accuracy: 0.8569\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4252 - accuracy: 0.8585\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4260 - accuracy: 0.8566\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4236 - accuracy: 0.8568\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4238 - accuracy: 0.8566\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.4293 - accuracy: 0.8573\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4281 - accuracy: 0.8551\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4290 - accuracy: 0.8571\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4215 - accuracy: 0.8594\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4314 - accuracy: 0.8548\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4174 - accuracy: 0.8579\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4245 - accuracy: 0.8582\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4318 - accuracy: 0.8551\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4184 - accuracy: 0.8577\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4292 - accuracy: 0.8548\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4171 - accuracy: 0.8586\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4206 - accuracy: 0.8573\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4270 - accuracy: 0.8597\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4166 - accuracy: 0.8569\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4264 - accuracy: 0.8600\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4196 - accuracy: 0.8599\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4210 - accuracy: 0.8590\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4202 - accuracy: 0.8582\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4167 - accuracy: 0.8594\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4098 - accuracy: 0.8612\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4304 - accuracy: 0.8566\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4198 - accuracy: 0.8592\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4280 - accuracy: 0.8586\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4122 - accuracy: 0.8605\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4149 - accuracy: 0.8619\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4156 - accuracy: 0.8597\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4215 - accuracy: 0.8584\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4190 - accuracy: 0.8592\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4131 - accuracy: 0.8616\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4192 - accuracy: 0.8599\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4230 - accuracy: 0.8581\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4081 - accuracy: 0.8616\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4180 - accuracy: 0.8593\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.5028 - accuracy: 0.8445\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4195 - accuracy: 0.8599\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4108 - accuracy: 0.8601\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4098 - accuracy: 0.8616\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4222 - accuracy: 0.8591\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4170 - accuracy: 0.8595\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4192 - accuracy: 0.8605\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4169 - accuracy: 0.8596\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4054 - accuracy: 0.8615\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4198 - accuracy: 0.8584\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4161 - accuracy: 0.8594\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4106 - accuracy: 0.8602\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4212 - accuracy: 0.8595\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4073 - accuracy: 0.8621\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4167 - accuracy: 0.8598\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4135 - accuracy: 0.8596\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4125 - accuracy: 0.8602\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.4228 - accuracy: 0.8594\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4086 - accuracy: 0.8619\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4070 - accuracy: 0.8626\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4276 - accuracy: 0.8587\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4181 - accuracy: 0.8607\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4106 - accuracy: 0.8621\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4088 - accuracy: 0.8632\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4030 - accuracy: 0.8640\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4107 - accuracy: 0.8621\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4190 - accuracy: 0.8603\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4098 - accuracy: 0.8625\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4041 - accuracy: 0.8633\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4115 - accuracy: 0.8619\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4173 - accuracy: 0.8596\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4074 - accuracy: 0.8619\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4183 - accuracy: 0.8591\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4131 - accuracy: 0.8601\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.4123 - accuracy: 0.8602\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4137 - accuracy: 0.8632\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4046 - accuracy: 0.8616\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4221 - accuracy: 0.8594\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4029 - accuracy: 0.8620\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4161 - accuracy: 0.8615\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4310 - accuracy: 0.8582\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4139 - accuracy: 0.8629\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4142 - accuracy: 0.8617\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4094 - accuracy: 0.8631\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4177 - accuracy: 0.8622\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4101 - accuracy: 0.8622\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3984 - accuracy: 0.8633\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4026 - accuracy: 0.8640\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4447 - accuracy: 0.8601\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4122 - accuracy: 0.8631\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4132 - accuracy: 0.8621\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4126 - accuracy: 0.8625\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4255 - accuracy: 0.8591\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4184 - accuracy: 0.8606\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4083 - accuracy: 0.8630\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4109 - accuracy: 0.8619\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4070 - accuracy: 0.8622\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4097 - accuracy: 0.8619\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4353 - accuracy: 0.8600\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4087 - accuracy: 0.8627\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4129 - accuracy: 0.8610\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4172 - accuracy: 0.8614\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4075 - accuracy: 0.8617\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4182 - accuracy: 0.8606\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4039 - accuracy: 0.8640\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4193 - accuracy: 0.8622\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4035 - accuracy: 0.8633\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3931 - accuracy: 0.8651\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4112 - accuracy: 0.8625\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4094 - accuracy: 0.8631\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4121 - accuracy: 0.8620\n",
      "Epoch 153/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4126 - accuracy: 0.8604\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4138 - accuracy: 0.8623\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3992 - accuracy: 0.8633\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4031 - accuracy: 0.8630\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4090 - accuracy: 0.8639\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4138 - accuracy: 0.8623\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4013 - accuracy: 0.8652\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4007 - accuracy: 0.8635\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4124 - accuracy: 0.8629\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4006 - accuracy: 0.8640\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4070 - accuracy: 0.8638\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4011 - accuracy: 0.8638\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4080 - accuracy: 0.8617\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4082 - accuracy: 0.8615\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4028 - accuracy: 0.8638\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4082 - accuracy: 0.8626\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4064 - accuracy: 0.8632\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4097 - accuracy: 0.8635\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4166 - accuracy: 0.8618\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4165 - accuracy: 0.8617\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4023 - accuracy: 0.8631\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3966 - accuracy: 0.8640\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4001 - accuracy: 0.8637\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4203 - accuracy: 0.8572\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.4073 - accuracy: 0.8638\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4041 - accuracy: 0.8636\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4009 - accuracy: 0.8635\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.4317 - accuracy: 0.8611\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4016 - accuracy: 0.8629\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4091 - accuracy: 0.8644\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4053 - accuracy: 0.8640\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4014 - accuracy: 0.8633\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4102 - accuracy: 0.8639\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4035 - accuracy: 0.8641\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3978 - accuracy: 0.8644\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4404 - accuracy: 0.8605\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4335 - accuracy: 0.8574\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3916 - accuracy: 0.8648\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4057 - accuracy: 0.8647\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3972 - accuracy: 0.8667\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3937 - accuracy: 0.8657\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4081 - accuracy: 0.8623\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.4093 - accuracy: 0.8619\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4110 - accuracy: 0.8639\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4320 - accuracy: 0.8598\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3884 - accuracy: 0.8675\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4194 - accuracy: 0.8629\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3909 - accuracy: 0.8650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab4ec54a48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 1.1750 - accuracy: 0.8174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1749584731578826, 0.8174]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 1.1750 - accuracy: 0.8174\n",
      "Node 0: -0.0022306003236771013\n",
      "Node 1: 0.0\n",
      "Node 2: -0.23230817093372347\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0002523882102965835\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.00016146503448477567\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: 0.0\n",
      "Node 29: -0.10123041025638593\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: 0.0\n",
      "Node 45: -0.010169116518497478\n",
      "Node 46: -0.20006351330280317\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: 0.0\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: -0.02401607532978074\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: -0.5975315176916123\n",
      "Node 59: -0.26955549643039717\n",
      "Node 60: -0.0003906211566925055\n",
      "Node 61: 0.0\n",
      "Node 62: -0.2103039181804657\n",
      "Node 63: 0.0\n",
      "Node 64: 0.0\n",
      "Node 65: 0.0\n",
      "Node 66: 0.0\n",
      "Node 67: 0.0\n",
      "Node 68: 0.0\n",
      "Node 69: 0.0\n",
      "Node 70: 0.0\n",
      "Node 71: 0.0\n",
      "Node 72: 0.0\n",
      "Node 73: 0.0\n",
      "Node 74: 0.0029544466590880392\n",
      "Node 75: 0.0\n",
      "Node 76: 0.0\n",
      "Node 77: 0.0\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0\n",
      "Node 80: 0.0\n",
      "Node 81: 0.0\n",
      "Node 82: -0.00015977760314951351\n",
      "Node 83: 0.0\n",
      "Node 84: 0.1240757795238494\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: -2.486257553235838e-06\n",
      "Node 90: 0.0\n",
      "Node 91: 0.0011526362609862728\n",
      "Node 92: 0.0\n",
      "Node 93: 0.0\n",
      "Node 94: 0.0032456126403807017\n",
      "Node 95: 0.0\n",
      "Node 96: -0.0008601910400392266\n",
      "Node 97: 0.0\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: 0.0\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0\n",
      "Node 108: 0.0\n",
      "Node 109: -0.031721562805175865\n",
      "Node 110: 0.0\n",
      "Node 111: 0.008490790863036966\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: 0.0\n",
      "Node 115: -0.18207221188068398\n",
      "Node 116: 0.0\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: 0.0\n",
      "Node 123: -0.07304143131732957\n",
      "Node 124: 0.0\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0\n",
      "Node 127: 5.853652952358956e-07\n"
     ]
    }
   ],
   "source": [
    "l, a = model.evaluate(x_test, y_test, verbose=2)\n",
    "or_weights = model.get_weights()\n",
    "size = 128\n",
    "for i in range(128):\n",
    "    w = copy.deepcopy(or_weights)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.*(na - a) + 1.0*(l - nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 1.1750 - accuracy: 0.8174\n",
      "0.0\n",
      "Found something better\n",
      "0.0002523882102965835\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0029544466590880392\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.1240757795238494\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7968 --- Loss: 1.034807611322403 --- Change: 0.1240757795238494 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "0.000252382249832106\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0029544418573378864\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0032456126403809017\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.008490776958465673\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7842 --- Loss: 1.0239734141826629 --- Change: 0.008490776958465673 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "0.0002523882102965835\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.00900516997337326\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7832 --- Loss: 1.013856560087204 --- Change: 0.00900516997337326 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "0.00025238224983230583\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0011526303005219953\n",
      "Found something better\n",
      "0.00324560251235968\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7832 --- Loss: 1.010250335073471 --- Change: 0.00324560251235968 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "0.000252382249832106\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0011526303005217954\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7833 --- Loss: 1.008980741071701 --- Change: 0.0011526303005217954 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "0.0002523882102965835\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087114226818086 --- Change: 0.0002523882102965835 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "1.595249176156166e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.008709650182724 --- Change: 1.595249176156166e-06 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "5.853652954357358e-07\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 5.853652954357358e-07 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.7834 --- Loss: 1.0087089997768401 --- Change: 0.0 --- New tol: -1e-30\n",
      "Did 20 iterations\n",
      "10000/10000 - 0s - loss: 1.0087 - accuracy: 0.7834\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "end_not_reached = True\n",
    "improved = False\n",
    "size = 128\n",
    "tol = -1e-30\n",
    "current_pos = 0\n",
    "best_pos = -1\n",
    "best_change = tol\n",
    "original2 = model.get_weights()\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights2 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "while end_not_reached or improved:\n",
    "    if not(end_not_reached):\n",
    "        end_not_reached = True\n",
    "        improved = False\n",
    "        current_pos = 0\n",
    "        size -= 1\n",
    "        nodes_removed2 += [best_pos]\n",
    "        best_weights2[0][:,best_pos] = 0\n",
    "        best_weights2[1][best_pos] = 0\n",
    "        best_weights2[2][best_pos,:] = 0\n",
    "        best_pos = -1\n",
    "        #tol -= best_change\n",
    "        ol = best_loss\n",
    "        oa = best_acc\n",
    "        bas2 += [best_acc]\n",
    "        bls2 += [best_loss]\n",
    "        print(\"Improvement has occured!! Accuracy:\", best_acc, \"--- Loss:\", best_loss, '--- Change:', best_change, '--- New tol:', tol)\n",
    "        best_change = tol\n",
    "        num_removed2 += 1\n",
    "    if current_pos in nodes_removed2:\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            end_not_reached = False\n",
    "        continue\n",
    "    w = copy.deepcopy(best_weights2)\n",
    "    w[0][:,current_pos] = 0\n",
    "    w[1][current_pos] = 0\n",
    "    w[2][current_pos,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    if 0.1*(na - oa) + 0.9*(ol - nl) > best_change:\n",
    "        best_change = 0.1*(na - oa) + 0.9*(ol - nl)\n",
    "        print(best_change)\n",
    "        best_pos = current_pos\n",
    "        improved = True\n",
    "        best_acc = na\n",
    "        best_loss = nl\n",
    "        print(\"Found something better\")\n",
    "    current_pos += 1\n",
    "    if current_pos - num_removed2 >= size:\n",
    "        end_not_reached = False\n",
    "    if current_pos%20 == 0:\n",
    "        print(\"Did 20 iterations\")\n",
    "\n",
    "tester_model.set_weights(best_weights2)\n",
    "loss2, acc2 = tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nodes_removed2:\n",
    "    best_weights2[0][:,i] = np.random.normal(0, 2/np.sqrt(28*28 + 128), 784)\n",
    "    best_weights2[1][i] = 0\n",
    "    best_weights2[2][i,:] = np.random.normal(0, 2/np.sqrt(138), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 214.6456 - accuracy: 0.2633\n",
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 147us/sample - loss: 3.3701 - accuracy: 0.8340\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.4435 - accuracy: 0.8580\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 137us/sample - loss: 0.4390 - accuracy: 0.8564\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.4422 - accuracy: 0.8556\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.4341 - accuracy: 0.8562\n",
      "10000/10000 - 1s - loss: 0.9824 - accuracy: 0.8258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9823681085109711, 0.8258]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = tf.keras.models.Sequential()\n",
    "new_model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "new_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "new_model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "new_model.set_weights(best_weights2)\n",
    "\n",
    "new_model.evaluate(x_test, y_test, verbose=2)\n",
    "new_model.fit(x_train, y_train, epochs=5)\n",
    "new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 1.1750 - accuracy: 0.8174\n",
      "Node 0: -0.0024451125621796077\n",
      "Node 1: 0.0\n",
      "Node 2: -0.24894241242408754\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.00026931838989252554\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.00017940559387197297\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: 0.0\n",
      "Node 29: -0.10543379244804396\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: 0.0\n",
      "Node 45: -0.011132357144355787\n",
      "Node 46: -0.2188261268138887\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: 0.0\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: -0.02602897639274615\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: -0.6538350210666657\n",
      "Node 59: -0.29442833447456374\n",
      "Node 60: -0.00042291717529296946\n",
      "Node 61: 0.0\n",
      "Node 62: -0.22737102117538455\n",
      "Node 63: 0.0\n",
      "Node 64: 0.0\n",
      "Node 65: 0.0\n",
      "Node 66: 0.0\n",
      "Node 67: 0.0\n",
      "Node 68: 0.0\n",
      "Node 69: 0.0\n",
      "Node 70: 0.0\n",
      "Node 71: 0.0\n",
      "Node 72: 0.0\n",
      "Node 73: 0.0\n",
      "Node 74: 0.0035493830680846106\n",
      "Node 75: 0.0\n",
      "Node 76: 0.0\n",
      "Node 77: 0.0\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0\n",
      "Node 80: 0.0\n",
      "Node 81: 0.0\n",
      "Node 82: -0.00017753067016612611\n",
      "Node 83: 0.0\n",
      "Node 84: 0.14015086183547965\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: -2.762508392484264e-06\n",
      "Node 90: 0.0\n",
      "Node 91: 0.001269594001769958\n",
      "Node 92: 0.0\n",
      "Node 93: 0.0\n",
      "Node 94: 0.0036062362670896686\n",
      "Node 95: 0.0\n",
      "Node 96: -0.0009557678222658073\n",
      "Node 97: 0.0\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: 0.0\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0\n",
      "Node 108: 0.0\n",
      "Node 109: -0.034857293701171965\n",
      "Node 110: 0.0\n",
      "Node 111: 0.010834212589263759\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: 0.0\n",
      "Node 115: -0.19253579287529\n",
      "Node 116: 0.0\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: 0.0\n",
      "Node 123: -0.0766015960216524\n",
      "Node 124: 0.0\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0\n",
      "Node 127: 6.504058835954396e-07\n",
      "10000/10000 - 0s - loss: 1.0360 - accuracy: 0.7788\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "end_not_reached = True\n",
    "improved = False\n",
    "size = 128\n",
    "tol = -1e-30\n",
    "current_pos = 0\n",
    "best_pos = -1\n",
    "best_change = tol\n",
    "original2 = model.get_weights()\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights2 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "l = loss\n",
    "a = acc\n",
    "num_removed2 = 0\n",
    "\n",
    "for i in range(128):\n",
    "    w = copy.deepcopy(original2)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    change = 0.*(na - a) + 1.0*(l - nl)\n",
    "    print(f\"Node {i}:\", change)\n",
    "    if change > tol:\n",
    "        nodes_removed2 += [i]\n",
    "        num_removed2 += 1\n",
    "        \n",
    "for i in nodes_removed2:\n",
    "    best_weights2[0][:,i] = 0\n",
    "    best_weights2[1][i] = 0\n",
    "    best_weights2[2][i,:] = 0\n",
    "\n",
    "tester_model.set_weights(best_weights2)\n",
    "loss2, acc2 = tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 1.1750 - accuracy: 0.8174\n",
      "Node 0: -0.0024451125621796077\n",
      "Node 1: 0.0\n",
      "Node 2: -0.24894241242408754\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.00026931838989252554\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.00017940559387197297\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: 0.0\n",
      "Node 29: -0.10543379244804396\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: 0.0\n",
      "Node 45: -0.011132357144355787\n",
      "Node 46: -0.2188261268138887\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: 0.0\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: -0.02602897639274615\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: -0.6538350210666657\n",
      "Node 59: -0.29442833447456374\n",
      "Node 60: -0.00042291717529296946\n",
      "Node 61: 0.0\n",
      "Node 62: -0.22737102117538455\n",
      "Node 63: 0.0\n",
      "Node 64: 0.0\n",
      "Node 65: 0.0\n",
      "Node 66: 0.0\n",
      "Node 67: 0.0\n",
      "Node 68: 0.0\n",
      "Node 69: 0.0\n",
      "Node 70: 0.0\n",
      "Node 71: 0.0\n",
      "Node 72: 0.0\n",
      "Node 73: 0.0\n",
      "Node 74: 0.0035493830680846106\n",
      "Node 75: 0.0\n",
      "Node 76: 0.0\n",
      "Node 77: 0.0\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0\n",
      "Node 80: 0.0\n",
      "Node 81: 0.0\n",
      "Node 82: -0.00017753067016612611\n",
      "Node 83: 0.0\n",
      "Node 84: 0.14015086183547965\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: -2.762508392484264e-06\n",
      "Node 90: 0.0\n",
      "Node 91: 0.001269594001769958\n",
      "Node 92: 0.0\n",
      "Node 93: 0.0\n",
      "Node 94: 0.0036062362670896686\n",
      "Node 95: 0.0\n",
      "Node 96: -0.0009557678222658073\n",
      "Node 97: 0.0\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: 0.0\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0\n",
      "Node 108: 0.0\n",
      "Node 109: -0.034857293701171965\n",
      "Node 110: 0.0\n",
      "Node 111: 0.010834212589263759\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: 0.0\n",
      "Node 115: -0.19253579287529\n",
      "Node 116: 0.0\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: 0.0\n",
      "Node 123: -0.0766015960216524\n",
      "Node 124: 0.0\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0\n",
      "Node 127: 6.504058835954396e-07\n",
      "10000/10000 - 0s - loss: 1.8288 - accuracy: 0.7266\n",
      "Node 0: -0.0024451131820679706\n",
      "Node 1: 0.0\n",
      "Node 2: -0.22610096988677975\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0002693187713622702\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.00017940597534171765\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: -3.8146974468133976e-10\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: 0.0\n",
      "Node 29: -0.10543379364013683\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: 0.0\n",
      "Node 45: -0.011419636917114229\n",
      "Node 46: -0.20202419223785406\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: 0.0\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: -0.02981619510650635\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0\n",
      "Node 59: -0.2856481540679934\n",
      "Node 60: -0.00042291717529296946\n",
      "Node 61: 0.0\n",
      "Node 62: -0.3132860363006593\n",
      "Node 63: 0.0\n",
      "Node 64: 0.0\n",
      "Node 65: 0.0\n",
      "Node 66: 0.0\n",
      "Node 67: 0.0\n",
      "Node 68: 0.0\n",
      "Node 69: 0.0\n",
      "Node 70: 0.0\n",
      "Node 71: 0.0\n",
      "Node 72: 0.0\n",
      "Node 73: 0.0\n",
      "Node 74: 0.003824231529235833\n",
      "Node 75: 0.0\n",
      "Node 76: 0.0\n",
      "Node 77: 0.0\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0\n",
      "Node 80: 0.0\n",
      "Node 81: 0.0\n",
      "Node 82: -0.00017753028869638143\n",
      "Node 83: 0.0\n",
      "Node 84: 0.14086949558258044\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0\n",
      "Node 90: 0.0\n",
      "Node 91: 0.0012695941925049414\n",
      "Node 92: 0.0\n",
      "Node 93: 0.0\n",
      "Node 94: 0.0036062362670898906\n",
      "Node 95: 0.0\n",
      "Node 96: -0.0009557678222655852\n",
      "Node 97: 0.0\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: 0.0\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0\n",
      "Node 108: 0.0\n",
      "Node 109: -0.022929939174652247\n",
      "Node 110: 0.0\n",
      "Node 111: 0.015807231521606457\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: 0.0\n",
      "Node 115: -0.1891333965301516\n",
      "Node 116: 0.0\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: 0.0\n",
      "Node 123: -0.0363237233161926\n",
      "Node 124: 0.0\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0\n",
      "Node 127: 6.504058838174842e-07\n"
     ]
    }
   ],
   "source": [
    "l, a = model.evaluate(x_test, y_test, verbose=2)\n",
    "or_weights = model.get_weights()\n",
    "size = 128\n",
    "worst_remove = -1\n",
    "wc = 0\n",
    "w2 = model.get_weights()\n",
    "for i in range(128):\n",
    "    w = copy.deepcopy(or_weights)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.*(na - a) + 1.0*(l - nl))\n",
    "    if 0.*(na - a) + 1.0*(l - nl) < wc:\n",
    "        worst_remove = i\n",
    "        wc = (l - nl)\n",
    "w2[0][:,worst_remove] = 0\n",
    "w2[1][worst_remove] = 0\n",
    "w2[2][worst_remove,:] = 0\n",
    "tester_model.set_weights(w2)\n",
    "loss2, acc2 = tester_model.evaluate(x_test, y_test, verbose=2)\n",
    "l = loss2\n",
    "a = acc2\n",
    "for i in range(128):\n",
    "    w = copy.deepcopy(w2)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.*(na - a) + 1.0*(l - nl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on higher node counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 19.9953 - accuracy: 0.76011s - loss: 26.132\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 3.9897 - accuracy: 0.8185\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 2.3723 - accuracy: 0.8311\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 1.3808 - accuracy: 0.8479\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 1.0572 - accuracy: 0.8513\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.7049 - accuracy: 0.8604\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4565 - accuracy: 0.8730\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.3596 - accuracy: 0.8835s - los\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3153 - accuracy: 0.8923\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2899 - accuracy: 0.8983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab43677248>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(size, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(x_train, y_train, epochs=10, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 5.8017 - accuracy: 0.7843\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 4.8484 - accuracy: 0.7966\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 3.9173 - accuracy: 0.8080\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 3.2884 - accuracy: 0.8173\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 2.9437 - accuracy: 0.8228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab390ce788>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, epochs=5, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 3.3791 - accuracy: 0.8087\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 2.3104 - accuracy: 0.8163\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 1.4819 - accuracy: 0.8303\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 16us/sample - loss: 1.0692 - accuracy: 0.8352\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.7564 - accuracy: 0.8351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab391002c8>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.6213 - accuracy: 0.8041\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.5068 - accuracy: 0.8273\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4967 - accuracy: 0.8300\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4790 - accuracy: 0.8376\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4643 - accuracy: 0.8421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab474b4c88>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model2 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(size, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "tester_model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.5534 - accuracy: 0.8573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5534319600820541, 0.8573]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x_test, y_test, verbose=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.5534 - accuracy: 0.8573\n",
      "Node 0: 0.0\n",
      "Node 1: -0.000299990177154541\n",
      "Node 2: 0.0\n",
      "Node 3: -9.995698928833008e-05\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: -0.001199960708618164\n",
      "Node 8: 0.00010001659393310547\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: -0.000299990177154541\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: -0.00019997358322143555\n",
      "Node 19: 0.000800013542175293\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: -0.000599980354309082\n",
      "Node 24: -0.00019997358322143555\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: 0.0\n",
      "Node 28: 0.00010001659393310547\n",
      "Node 29: -0.001199960708618164\n",
      "Node 30: 0.0\n",
      "Node 31: -9.995698928833008e-05\n",
      "Node 32: 0.0\n",
      "Node 33: -0.000299990177154541\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: 0.00010001659393310547\n",
      "Node 38: 0.0\n",
      "Node 39: 0.00010001659393310547\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: -9.995698928833008e-05\n",
      "Node 45: -9.995698928833008e-05\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: 0.0\n",
      "Node 51: 0.00010001659393310547\n",
      "Node 52: -0.0009999871253967285\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: -0.00019997358322143555\n",
      "Node 59: 0.0\n",
      "Node 60: 0.00020003318786621094\n",
      "Node 61: 0.0\n",
      "Node 62: -0.000299990177154541\n",
      "Node 63: 0.0\n",
      "Node 64: 0.0\n",
      "Node 65: 0.00010001659393310547\n",
      "Node 66: -0.00019997358322143555\n",
      "Node 67: 0.00010001659393310547\n",
      "Node 68: 0.0004000067710876465\n",
      "Node 69: 0.0\n",
      "Node 70: -0.000299990177154541\n",
      "Node 71: 0.0\n",
      "Node 72: -0.004700005054473877\n",
      "Node 73: 0.0\n",
      "Node 74: 0.00010001659393310547\n",
      "Node 75: 0.0006999969482421875\n",
      "Node 76: 0.0\n",
      "Node 77: 0.00010001659393310547\n",
      "Node 78: -9.995698928833008e-05\n",
      "Node 79: -9.995698928833008e-05\n",
      "Node 80: 0.00010001659393310547\n",
      "Node 81: -0.000899970531463623\n",
      "Node 82: 0.0\n",
      "Node 83: 0.0\n",
      "Node 84: 0.0\n",
      "Node 85: 0.00020003318786621094\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0\n",
      "Node 88: -0.0004999637603759766\n",
      "Node 89: 0.0\n",
      "Node 90: 0.0\n",
      "Node 91: 0.0\n",
      "Node 92: 0.0\n",
      "Node 93: 0.0\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: 0.0\n",
      "Node 97: -9.995698928833008e-05\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Node 100: 0.0\n",
      "Node 101: 0.0\n",
      "Node 102: -0.0006999969482421875\n",
      "Node 103: -0.000299990177154541\n",
      "Node 104: -0.0054999589920043945\n",
      "Node 105: 0.0\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0\n",
      "Node 108: 0.00010001659393310547\n",
      "Node 109: -0.0007999539375305176\n",
      "Node 110: -0.00019997358322143555\n",
      "Node 111: -0.00019997358322143555\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: -9.995698928833008e-05\n",
      "Node 115: 0.0\n",
      "Node 116: 0.0\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0\n",
      "Node 122: -0.0015999674797058105\n",
      "Node 123: 0.0\n",
      "Node 124: 0.0\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0\n",
      "Node 127: 0.0\n",
      "Node 128: -0.0004000067710876465\n",
      "Node 129: 0.0\n",
      "Node 130: 0.00010001659393310547\n",
      "Node 131: -0.002599954605102539\n",
      "Node 132: 0.0\n",
      "Node 133: 0.0\n",
      "Node 134: 0.00020003318786621094\n",
      "Node 135: 0.0\n",
      "Node 136: 0.0\n",
      "Node 137: 0.0\n",
      "Node 138: 0.0\n",
      "Node 139: 0.0\n",
      "Node 140: -9.995698928833008e-05\n",
      "Node 141: 0.0\n",
      "Node 142: 0.0\n",
      "Node 143: 0.0\n",
      "Node 144: -9.995698928833008e-05\n",
      "Node 145: 0.0\n",
      "Node 146: 0.00010001659393310547\n",
      "Node 147: 0.0\n",
      "Node 148: 0.0\n",
      "Node 149: -0.0004000067710876465\n",
      "Node 150: 0.0\n",
      "Node 151: 0.0\n",
      "Node 152: -9.995698928833008e-05\n",
      "Node 153: 0.000299990177154541\n",
      "Node 154: 0.0\n",
      "Node 155: 0.0\n",
      "Node 156: 0.0004000067710876465\n",
      "Node 157: 0.0\n",
      "Node 158: -0.001699984073638916\n",
      "Node 159: 0.0\n",
      "Node 160: 0.0\n",
      "Node 161: 0.0\n",
      "Node 162: 0.0\n",
      "Node 163: 0.0\n",
      "Node 164: -9.995698928833008e-05\n",
      "Node 165: 0.00010001659393310547\n",
      "Node 166: 0.0\n",
      "Node 167: 0.0\n",
      "Node 168: 0.0006000399589538574\n",
      "Node 169: 0.0\n",
      "Node 170: 0.0\n",
      "Node 171: 0.0\n",
      "Node 172: 0.0\n",
      "Node 173: 0.0\n",
      "Node 174: 0.0\n",
      "Node 175: 0.0\n",
      "Node 176: 0.0\n",
      "Node 177: 0.0\n",
      "Node 178: 0.0\n",
      "Node 179: -0.000299990177154541\n",
      "Node 180: 0.0\n",
      "Node 181: 0.0\n",
      "Node 182: -9.995698928833008e-05\n",
      "Node 183: 0.0\n",
      "Node 184: 0.0\n",
      "Node 185: 0.0\n",
      "Node 186: 0.0\n",
      "Node 187: -0.0009999871253967285\n",
      "Node 188: 0.0\n",
      "Node 189: 0.0\n",
      "Node 190: -0.000599980354309082\n",
      "Node 191: 0.00020003318786621094\n",
      "Node 192: 0.00010001659393310547\n",
      "Node 193: 0.0\n",
      "Node 194: 0.0\n",
      "Node 195: 0.0\n",
      "Node 196: 0.0\n",
      "Node 197: 0.0\n",
      "Node 198: -0.000299990177154541\n",
      "Node 199: 0.0\n",
      "Node 200: 0.00010001659393310547\n",
      "Node 201: -9.995698928833008e-05\n",
      "Node 202: 0.0\n",
      "Node 203: -9.995698928833008e-05\n",
      "Node 204: 0.0\n",
      "Node 205: 0.0\n",
      "Node 206: 0.0\n",
      "Node 207: 0.0\n",
      "Node 208: 0.00010001659393310547\n",
      "Node 209: 0.0\n",
      "Node 210: 0.0\n",
      "Node 211: -9.995698928833008e-05\n",
      "Node 212: 0.0\n",
      "Node 213: -9.995698928833008e-05\n",
      "Node 214: 0.0\n",
      "Node 215: 0.0\n",
      "Node 216: 0.0\n",
      "Node 217: 0.0\n",
      "Node 218: 0.0\n",
      "Node 219: -9.995698928833008e-05\n",
      "Node 220: -0.000299990177154541\n",
      "Node 221: 0.00010001659393310547\n",
      "Node 222: -0.00019997358322143555\n",
      "Node 223: 0.0\n",
      "Node 224: 0.0\n",
      "Node 225: 0.0\n",
      "Node 226: 0.0\n",
      "Node 227: -0.0009999871253967285\n",
      "Node 228: 0.0\n",
      "Node 229: 0.0\n",
      "Node 230: 0.000299990177154541\n",
      "Node 231: -0.000299990177154541\n",
      "Node 232: 0.0\n",
      "Node 233: 0.0\n",
      "Node 234: 0.0\n",
      "Node 235: 0.0\n",
      "Node 236: -9.995698928833008e-05\n",
      "Node 237: -9.995698928833008e-05\n",
      "Node 238: 0.0004000067710876465\n",
      "Node 239: 0.0\n",
      "Node 240: 0.0\n",
      "Node 241: 0.0\n",
      "Node 242: 0.0\n",
      "Node 243: 0.0\n",
      "Node 244: 0.0\n",
      "Node 245: 0.0\n",
      "Node 246: 0.00010001659393310547\n",
      "Node 247: 0.0\n",
      "Node 248: 0.0\n",
      "Node 249: 0.0\n",
      "Node 250: 0.0\n",
      "Node 251: 0.0\n",
      "Node 252: 0.0\n",
      "Node 253: 0.0\n",
      "Node 254: 0.0\n",
      "Node 255: 0.0\n",
      "Node 256: 0.0\n",
      "Node 257: 0.0\n",
      "Node 258: 0.0\n",
      "Node 259: 0.0\n",
      "Node 260: -0.0076999664306640625\n",
      "Node 261: 0.000299990177154541\n",
      "Node 262: 0.0\n",
      "Node 263: -9.995698928833008e-05\n",
      "Node 264: 0.0\n",
      "Node 265: 0.0\n",
      "Node 266: -9.995698928833008e-05\n",
      "Node 267: 0.00010001659393310547\n",
      "Node 268: 0.0\n",
      "Node 269: 0.0\n",
      "Node 270: 0.0\n",
      "Node 271: 0.0\n",
      "Node 272: 0.0\n",
      "Node 273: -9.995698928833008e-05\n",
      "Node 274: -9.995698928833008e-05\n",
      "Node 275: 0.0\n",
      "Node 276: 0.0\n",
      "Node 277: 0.0\n",
      "Node 278: 0.00010001659393310547\n",
      "Node 279: 0.0\n",
      "Node 280: 0.0\n",
      "Node 281: 0.0\n",
      "Node 282: 0.0\n",
      "Node 283: 0.0\n",
      "Node 284: 0.0\n",
      "Node 285: 0.0\n",
      "Node 286: -0.000599980354309082\n",
      "Node 287: 0.0\n",
      "Node 288: -0.00019997358322143555\n",
      "Node 289: 0.0\n",
      "Node 290: 0.0\n",
      "Node 291: -9.995698928833008e-05\n",
      "Node 292: 0.0\n",
      "Node 293: 0.0\n",
      "Node 294: 0.0\n",
      "Node 295: 0.0\n",
      "Node 296: 0.00010001659393310547\n",
      "Node 297: 0.0\n",
      "Node 298: 0.0004000067710876465\n",
      "Node 299: 0.0\n",
      "Node 300: 0.0\n",
      "Node 301: 0.0\n",
      "Node 302: -0.0004000067710876465\n",
      "Node 303: 0.0\n",
      "Node 304: 0.00010001659393310547\n",
      "Node 305: 0.0\n",
      "Node 306: -9.995698928833008e-05\n",
      "Node 307: 0.000299990177154541\n",
      "Node 308: -0.0018999576568603516\n",
      "Node 309: -0.00019997358322143555\n",
      "Node 310: 0.0\n",
      "Node 311: 0.0\n",
      "Node 312: -9.995698928833008e-05\n",
      "Node 313: 0.0\n",
      "Node 314: -0.001399993896484375\n",
      "Node 315: 0.0\n",
      "Node 316: 0.0\n",
      "Node 317: -9.995698928833008e-05\n",
      "Node 318: -9.995698928833008e-05\n",
      "Node 319: 0.0\n",
      "Node 320: 0.0\n",
      "Node 321: 0.0006999969482421875\n",
      "Node 322: -0.000299990177154541\n",
      "Node 323: 0.0\n",
      "Node 324: 0.0\n",
      "Node 325: 0.0\n",
      "Node 326: 0.00020003318786621094\n",
      "Node 327: 0.00010001659393310547\n",
      "Node 328: -9.995698928833008e-05\n",
      "Node 329: -9.995698928833008e-05\n",
      "Node 330: 0.0\n",
      "Node 331: 0.0\n",
      "Node 332: 0.0\n",
      "Node 333: 0.0\n",
      "Node 334: 0.0\n",
      "Node 335: 0.0\n",
      "Node 336: 0.0\n",
      "Node 337: 0.00010001659393310547\n",
      "Node 338: 0.0\n",
      "Node 339: 0.0\n",
      "Node 340: 0.0\n",
      "Node 341: 0.0\n",
      "Node 342: 0.0\n",
      "Node 343: 0.00020003318786621094\n",
      "Node 344: 0.00020003318786621094\n",
      "Node 345: 0.0\n",
      "Node 346: 0.0\n",
      "Node 347: 0.0\n",
      "Node 348: -0.001100003719329834\n",
      "Node 349: 0.0\n",
      "Node 350: -9.995698928833008e-05\n",
      "Node 351: 0.0\n",
      "Node 352: -9.995698928833008e-05\n",
      "Node 353: -0.00019997358322143555\n",
      "Node 354: 0.0\n",
      "Node 355: 0.0\n",
      "Node 356: 0.00010001659393310547\n",
      "Node 357: 0.0\n",
      "Node 358: 0.0\n",
      "Node 359: 0.0\n",
      "Node 360: 0.0\n",
      "Node 361: 0.0\n",
      "Node 362: 0.0\n",
      "Node 363: -9.995698928833008e-05\n",
      "Node 364: 0.0\n",
      "Node 365: 0.0\n",
      "Node 366: 0.0\n",
      "Node 367: 0.0\n",
      "Node 368: 0.0\n",
      "Node 369: 0.0\n",
      "Node 370: -0.0009999871253967285\n",
      "Node 371: -0.0004999637603759766\n",
      "Node 372: 0.0\n",
      "Node 373: 0.0\n",
      "Node 374: 0.0\n",
      "Node 375: 0.0\n",
      "Node 376: 0.0\n",
      "Node 377: 0.0\n",
      "Node 378: 0.0\n",
      "Node 379: 0.00010001659393310547\n",
      "Node 380: 0.0\n",
      "Node 381: 0.0\n",
      "Node 382: 0.00010001659393310547\n",
      "Node 383: 0.0\n",
      "Node 384: 0.0\n",
      "Node 385: 0.0\n",
      "Node 386: -9.995698928833008e-05\n",
      "Node 387: 0.0\n",
      "Node 388: 0.0\n",
      "Node 389: -0.0004999637603759766\n",
      "Node 390: 0.0\n",
      "Node 391: -9.995698928833008e-05\n",
      "Node 392: 0.0\n",
      "Node 393: 0.0\n",
      "Node 394: 0.00010001659393310547\n",
      "Node 395: 0.0\n",
      "Node 396: -0.0006999969482421875\n",
      "Node 397: 0.0\n",
      "Node 398: 0.0\n",
      "Node 399: -9.995698928833008e-05\n",
      "Node 400: 0.00010001659393310547\n",
      "Node 401: 0.0\n",
      "Node 402: 0.00010001659393310547\n",
      "Node 403: -9.995698928833008e-05\n",
      "Node 404: -0.000599980354309082\n",
      "Node 405: -9.995698928833008e-05\n",
      "Node 406: -9.995698928833008e-05\n",
      "Node 407: 0.0\n",
      "Node 408: 0.0\n",
      "Node 409: -0.000599980354309082\n",
      "Node 410: 0.0\n",
      "Node 411: 0.0\n",
      "Node 412: 0.0\n",
      "Node 413: 0.0\n",
      "Node 414: -0.00019997358322143555\n",
      "Node 415: 0.0\n",
      "Node 416: 0.0\n",
      "Node 417: -0.001199960708618164\n",
      "Node 418: 0.0\n",
      "Node 419: -0.000599980354309082\n",
      "Node 420: 0.0\n",
      "Node 421: 0.0\n",
      "Node 422: 0.0\n",
      "Node 423: 0.0\n",
      "Node 424: 0.00020003318786621094\n",
      "Node 425: 0.0\n",
      "Node 426: 0.0\n",
      "Node 427: 0.0\n",
      "Node 428: 0.0\n",
      "Node 429: -0.00019997358322143555\n",
      "Node 430: 0.0\n",
      "Node 431: 0.0\n",
      "Node 432: -0.0004999637603759766\n",
      "Node 433: -9.995698928833008e-05\n",
      "Node 434: -9.995698928833008e-05\n",
      "Node 435: 0.001100003719329834\n",
      "Node 436: 0.000299990177154541\n",
      "Node 437: 0.0\n",
      "Node 438: 0.0\n",
      "Node 439: 0.00010001659393310547\n",
      "Node 440: 0.0\n",
      "Node 441: 0.0\n",
      "Node 442: -0.00019997358322143555\n",
      "Node 443: -0.0012999773025512695\n",
      "Node 444: -0.00019997358322143555\n",
      "Node 445: 0.0\n",
      "Node 446: 0.0\n",
      "Node 447: 0.0\n",
      "Node 448: -0.0004000067710876465\n",
      "Node 449: -9.995698928833008e-05\n",
      "Node 450: 0.00020003318786621094\n",
      "Node 451: -0.0004999637603759766\n",
      "Node 452: -0.00019997358322143555\n",
      "Node 453: 0.0\n",
      "Node 454: 0.0\n",
      "Node 455: -9.995698928833008e-05\n",
      "Node 456: -9.995698928833008e-05\n",
      "Node 457: 0.0\n",
      "Node 458: 0.0\n",
      "Node 459: -9.995698928833008e-05\n",
      "Node 460: -9.995698928833008e-05\n",
      "Node 461: 0.0\n",
      "Node 462: 0.0\n",
      "Node 463: 0.0\n",
      "Node 464: -0.000899970531463623\n",
      "Node 465: 0.0\n",
      "Node 466: -0.003699958324432373\n",
      "Node 467: 0.0\n",
      "Node 468: 0.0\n",
      "Node 469: -9.995698928833008e-05\n",
      "Node 470: 0.00010001659393310547\n",
      "Node 471: 0.0\n",
      "Node 472: 0.0\n",
      "Node 473: 0.0\n",
      "Node 474: -0.044799983501434326\n",
      "Node 475: 0.0006000399589538574\n",
      "Node 476: 0.0\n",
      "Node 477: 0.00010001659393310547\n",
      "Node 478: 0.00010001659393310547\n",
      "Node 479: -0.00019997358322143555\n",
      "Node 480: 0.0\n",
      "Node 481: 0.0\n",
      "Node 482: -9.995698928833008e-05\n",
      "Node 483: 0.0\n",
      "Node 484: 0.00010001659393310547\n",
      "Node 485: 0.0\n",
      "Node 486: 0.0\n",
      "Node 487: -0.0004000067710876465\n",
      "Node 488: -0.00019997358322143555\n",
      "Node 489: 0.0\n",
      "Node 490: 0.0\n",
      "Node 491: 0.0\n",
      "Node 492: 0.0\n",
      "Node 493: 0.0\n",
      "Node 494: 0.0\n",
      "Node 495: 0.0\n",
      "Node 496: 0.0\n",
      "Node 497: -0.00019997358322143555\n",
      "Node 498: -0.0029000043869018555\n",
      "Node 499: 0.0\n",
      "Node 500: -9.995698928833008e-05\n",
      "Node 501: -0.00019997358322143555\n",
      "Node 502: 0.0\n",
      "Node 503: 0.00010001659393310547\n",
      "Node 504: 0.0\n",
      "Node 505: 0.0\n",
      "Node 506: 0.0004000067710876465\n",
      "Node 507: 0.0\n",
      "Node 508: -0.0007999539375305176\n",
      "Node 509: 0.0\n",
      "Node 510: -9.995698928833008e-05\n",
      "Node 511: 0.0\n",
      "Node 512: 0.0\n",
      "Node 513: 0.0\n",
      "Node 514: 0.0\n",
      "Node 515: 0.0\n",
      "Node 516: 0.0\n",
      "Node 517: -9.995698928833008e-05\n",
      "Node 518: 0.0\n",
      "Node 519: -0.000299990177154541\n",
      "Node 520: 0.0\n",
      "Node 521: 0.0\n",
      "Node 522: 0.0\n",
      "Node 523: 0.0\n",
      "Node 524: -9.995698928833008e-05\n",
      "Node 525: 0.0\n",
      "Node 526: 0.0\n",
      "Node 527: 0.0\n",
      "Node 528: 0.0\n",
      "Node 529: -0.0006999969482421875\n",
      "Node 530: 0.0\n",
      "Node 531: 0.0\n",
      "Node 532: 0.0\n",
      "Node 533: 0.0\n",
      "Node 534: 0.0\n",
      "Node 535: 0.0\n",
      "Node 536: -9.995698928833008e-05\n",
      "Node 537: 0.0\n",
      "Node 538: 0.00020003318786621094\n",
      "Node 539: 0.0\n",
      "Node 540: -0.000599980354309082\n",
      "Node 541: -0.0065999627113342285\n",
      "Node 542: 0.0\n",
      "Node 543: -0.000899970531463623\n",
      "Node 544: 0.0\n",
      "Node 545: -0.00019997358322143555\n",
      "Node 546: -9.995698928833008e-05\n",
      "Node 547: 0.0\n",
      "Node 548: 0.0\n",
      "Node 549: 0.0\n",
      "Node 550: -9.995698928833008e-05\n",
      "Node 551: 0.0\n",
      "Node 552: 0.0\n",
      "Node 553: -9.995698928833008e-05\n",
      "Node 554: 0.0\n",
      "Node 555: 0.0\n",
      "Node 556: 0.0\n",
      "Node 557: -0.00019997358322143555\n",
      "Node 558: -9.995698928833008e-05\n",
      "Node 559: -9.995698928833008e-05\n",
      "Node 560: 0.0\n",
      "Node 561: 0.00020003318786621094\n",
      "Node 562: -0.000299990177154541\n",
      "Node 563: 0.0\n",
      "Node 564: -0.000599980354309082\n",
      "Node 565: 0.0\n",
      "Node 566: -0.0012999773025512695\n",
      "Node 567: 0.00010001659393310547\n",
      "Node 568: -9.995698928833008e-05\n",
      "Node 569: 0.0\n",
      "Node 570: 0.0\n",
      "Node 571: -9.995698928833008e-05\n",
      "Node 572: 0.0\n",
      "Node 573: 0.0\n",
      "Node 574: -0.000599980354309082\n",
      "Node 575: 0.00010001659393310547\n",
      "Node 576: 0.00010001659393310547\n",
      "Node 577: 0.0\n",
      "Node 578: 0.00010001659393310547\n",
      "Node 579: 0.0\n",
      "Node 580: 0.0\n",
      "Node 581: -0.00019997358322143555\n",
      "Node 582: 0.0\n",
      "Node 583: 0.0\n",
      "Node 584: 0.0\n",
      "Node 585: -0.0015000104904174805\n",
      "Node 586: 0.0\n",
      "Node 587: 0.0\n",
      "Node 588: 0.0\n",
      "Node 589: 0.0\n",
      "Node 590: 0.0\n",
      "Node 591: 0.0\n",
      "Node 592: -0.0006999969482421875\n",
      "Node 593: 0.0\n",
      "Node 594: 0.0\n",
      "Node 595: 0.00010001659393310547\n",
      "Node 596: 0.0\n",
      "Node 597: 0.0\n",
      "Node 598: -9.995698928833008e-05\n",
      "Node 599: 0.0\n",
      "Node 600: 0.0\n",
      "Node 601: 0.0\n",
      "Node 602: -0.00019997358322143555\n",
      "Node 603: 0.0\n",
      "Node 604: 0.000299990177154541\n",
      "Node 605: 0.0\n",
      "Node 606: 0.0\n",
      "Node 607: 0.0\n",
      "Node 608: 0.0\n",
      "Node 609: -0.00019997358322143555\n",
      "Node 610: 0.00010001659393310547\n",
      "Node 611: 0.0\n",
      "Node 612: 0.00010001659393310547\n",
      "Node 613: 0.0\n",
      "Node 614: 0.0\n",
      "Node 615: 0.0\n",
      "Node 616: 0.0\n",
      "Node 617: 0.0\n",
      "Node 618: -9.995698928833008e-05\n",
      "Node 619: 0.0\n",
      "Node 620: 0.0\n",
      "Node 621: -9.995698928833008e-05\n",
      "Node 622: 0.0\n",
      "Node 623: 0.0\n",
      "Node 624: 0.00010001659393310547\n",
      "Node 625: 0.0\n",
      "Node 626: -0.021499991416931152\n",
      "Node 627: -9.995698928833008e-05\n",
      "Node 628: 0.0\n",
      "Node 629: 0.0\n",
      "Node 630: 0.0\n",
      "Node 631: 0.0\n",
      "Node 632: 0.0\n",
      "Node 633: 0.0\n",
      "Node 634: -9.995698928833008e-05\n",
      "Node 635: 0.0\n",
      "Node 636: 0.0\n",
      "Node 637: 0.0\n",
      "Node 638: 0.000299990177154541\n",
      "Node 639: 0.0\n",
      "Node 640: 0.0\n",
      "Node 641: 0.0\n",
      "Node 642: 0.0\n",
      "Node 643: 0.0\n",
      "Node 644: 0.00010001659393310547\n",
      "Node 645: -0.0004000067710876465\n",
      "Node 646: 0.0\n",
      "Node 647: 0.0\n",
      "Node 648: 0.0\n",
      "Node 649: 0.0\n",
      "Node 650: 0.0\n",
      "Node 651: 0.0\n",
      "Node 652: 0.0\n",
      "Node 653: 0.000299990177154541\n",
      "Node 654: 0.0\n",
      "Node 655: -9.995698928833008e-05\n",
      "Node 656: 0.0\n",
      "Node 657: 0.0\n",
      "Node 658: -9.995698928833008e-05\n",
      "Node 659: -9.995698928833008e-05\n",
      "Node 660: 0.0\n",
      "Node 661: -9.995698928833008e-05\n",
      "Node 662: 0.00010001659393310547\n",
      "Node 663: 0.0\n",
      "Node 664: 0.0\n",
      "Node 665: 0.0\n",
      "Node 666: 0.0004000067710876465\n",
      "Node 667: 0.0\n",
      "Node 668: 0.0\n",
      "Node 669: 0.0\n",
      "Node 670: 0.0\n",
      "Node 671: 0.0\n",
      "Node 672: 0.0\n",
      "Node 673: 0.0\n",
      "Node 674: 0.0\n",
      "Node 675: -0.00019997358322143555\n",
      "Node 676: 0.0\n",
      "Node 677: 0.0\n",
      "Node 678: 0.00010001659393310547\n",
      "Node 679: -9.995698928833008e-05\n",
      "Node 680: 0.0\n",
      "Node 681: 0.0\n",
      "Node 682: -9.995698928833008e-05\n",
      "Node 683: 0.0\n",
      "Node 684: -0.00019997358322143555\n",
      "Node 685: 0.0\n",
      "Node 686: 0.0\n",
      "Node 687: 0.0\n",
      "Node 688: 0.0\n",
      "Node 689: 0.0\n",
      "Node 690: 0.0\n",
      "Node 691: 0.0\n",
      "Node 692: 0.0\n",
      "Node 693: 0.0\n",
      "Node 694: 0.00020003318786621094\n",
      "Node 695: 0.0\n",
      "Node 696: 0.0\n",
      "Node 697: 0.00010001659393310547\n",
      "Node 698: 0.0\n",
      "Node 699: 0.0\n",
      "Node 700: 0.0\n",
      "Node 701: 0.0\n",
      "Node 702: -0.0004999637603759766\n",
      "Node 703: -9.995698928833008e-05\n",
      "Node 704: -9.995698928833008e-05\n",
      "Node 705: -9.995698928833008e-05\n",
      "Node 706: -9.995698928833008e-05\n",
      "Node 707: -0.00019997358322143555\n",
      "Node 708: 0.0\n",
      "Node 709: 0.0\n",
      "Node 710: -0.0018000006675720215\n",
      "Node 711: -0.0012999773025512695\n",
      "Node 712: 0.00010001659393310547\n",
      "Node 713: 0.0\n",
      "Node 714: -0.00019997358322143555\n",
      "Node 715: 0.0\n",
      "Node 716: 0.0\n",
      "Node 717: 0.0\n",
      "Node 718: 0.0\n",
      "Node 719: 0.0\n",
      "Node 720: 0.0\n",
      "Node 721: -0.00019997358322143555\n",
      "Node 722: 0.0\n",
      "Node 723: 0.0\n",
      "Node 724: 0.0\n",
      "Node 725: 0.0\n",
      "Node 726: 0.0\n",
      "Node 727: 0.0\n",
      "Node 728: 0.0\n",
      "Node 729: 0.0\n",
      "Node 730: 0.0\n",
      "Node 731: -9.995698928833008e-05\n",
      "Node 732: 0.0\n",
      "Node 733: 0.0\n",
      "Node 734: -0.00019997358322143555\n",
      "Node 735: 0.0\n",
      "Node 736: 0.0\n",
      "Node 737: 0.000800013542175293\n",
      "Node 738: 0.0\n",
      "Node 739: -0.0040000081062316895\n",
      "Node 740: 0.000299990177154541\n",
      "Node 741: 0.0\n",
      "Node 742: 0.0\n",
      "Node 743: 0.0\n",
      "Node 744: -0.000299990177154541\n",
      "Node 745: -0.0018000006675720215\n",
      "Node 746: -0.0004999637603759766\n",
      "Node 747: 0.0\n",
      "Node 748: 0.0\n",
      "Node 749: 0.0\n",
      "Node 750: 0.0\n",
      "Node 751: 0.0\n",
      "Node 752: -0.0004000067710876465\n",
      "Node 753: 0.00010001659393310547\n",
      "Node 754: 0.0\n",
      "Node 755: 0.0\n",
      "Node 756: -0.002299964427947998\n",
      "Node 757: -9.995698928833008e-05\n",
      "Node 758: 0.0\n",
      "Node 759: -9.995698928833008e-05\n",
      "Node 760: 0.0\n",
      "Node 761: 0.0\n",
      "Node 762: 0.0\n",
      "Node 763: 0.0\n",
      "Node 764: -0.00019997358322143555\n",
      "Node 765: -9.995698928833008e-05\n",
      "Node 766: 0.00010001659393310547\n",
      "Node 767: 0.0\n",
      "Node 768: 0.0\n",
      "Node 769: -9.995698928833008e-05\n",
      "Node 770: 0.00020003318786621094\n",
      "Node 771: 0.00020003318786621094\n",
      "Node 772: 0.0\n",
      "Node 773: -9.995698928833008e-05\n",
      "Node 774: -9.995698928833008e-05\n",
      "Node 775: 0.00020003318786621094\n",
      "Node 776: 0.0\n",
      "Node 777: -9.995698928833008e-05\n",
      "Node 778: 0.00010001659393310547\n",
      "Node 779: 0.0\n",
      "Node 780: 0.00010001659393310547\n",
      "Node 781: 0.0\n",
      "Node 782: 0.0\n",
      "Node 783: 0.0\n",
      "Node 784: 0.0\n",
      "Node 785: 0.0\n",
      "Node 786: -0.0006999969482421875\n",
      "Node 787: 0.0\n",
      "Node 788: -0.003300011157989502\n",
      "Node 789: 0.00020003318786621094\n",
      "Node 790: 0.0\n",
      "Node 791: 0.0\n",
      "Node 792: 0.0\n",
      "Node 793: 0.0\n",
      "Node 794: 0.0\n",
      "Node 795: 0.0\n",
      "Node 796: 0.00010001659393310547\n",
      "Node 797: 0.0\n",
      "Node 798: 0.0\n",
      "Node 799: 0.0\n",
      "Node 800: 0.0\n",
      "Node 801: 0.0\n",
      "Node 802: 0.0\n",
      "Node 803: 0.0\n",
      "Node 804: 0.0\n",
      "Node 805: 0.00010001659393310547\n",
      "Node 806: 0.00010001659393310547\n",
      "Node 807: 0.0\n",
      "Node 808: 0.0\n",
      "Node 809: 0.0\n",
      "Node 810: 0.0\n",
      "Node 811: 0.0\n",
      "Node 812: -9.995698928833008e-05\n",
      "Node 813: 0.0\n",
      "Node 814: 0.0\n",
      "Node 815: 0.0\n",
      "Node 816: 0.0\n",
      "Node 817: 0.00010001659393310547\n",
      "Node 818: -9.995698928833008e-05\n",
      "Node 819: 0.0\n",
      "Node 820: 0.0\n",
      "Node 821: 0.0\n",
      "Node 822: 0.0\n",
      "Node 823: 0.0\n",
      "Node 824: -9.995698928833008e-05\n",
      "Node 825: 0.0\n",
      "Node 826: 0.00010001659393310547\n",
      "Node 827: 0.0\n",
      "Node 828: 0.0\n",
      "Node 829: 0.0\n",
      "Node 830: 0.0\n",
      "Node 831: 0.0\n",
      "Node 832: 0.0\n",
      "Node 833: 0.0\n",
      "Node 834: 0.0\n",
      "Node 835: 0.0\n",
      "Node 836: -9.995698928833008e-05\n",
      "Node 837: -0.006099998950958252\n",
      "Node 838: 0.0\n",
      "Node 839: 0.0\n",
      "Node 840: 0.0\n",
      "Node 841: 0.0\n",
      "Node 842: 0.00010001659393310547\n",
      "Node 843: 0.0\n",
      "Node 844: 0.0\n",
      "Node 845: 0.0\n",
      "Node 846: -9.995698928833008e-05\n",
      "Node 847: 0.0\n",
      "Node 848: 0.0\n",
      "Node 849: 0.0\n",
      "Node 850: 0.0\n",
      "Node 851: 0.0\n",
      "Node 852: 0.0\n",
      "Node 853: 0.0\n",
      "Node 854: 0.0\n",
      "Node 855: -9.995698928833008e-05\n",
      "Node 856: 0.0\n",
      "Node 857: 0.0\n",
      "Node 858: 0.0\n",
      "Node 859: 0.0\n",
      "Node 860: 0.0\n",
      "Node 861: -9.995698928833008e-05\n",
      "Node 862: 0.0\n",
      "Node 863: -0.00019997358322143555\n",
      "Node 864: 0.0\n",
      "Node 865: 0.0\n",
      "Node 866: 0.0\n",
      "Node 867: 0.0\n",
      "Node 868: 0.0\n",
      "Node 869: 0.0\n",
      "Node 870: -0.00019997358322143555\n",
      "Node 871: -9.995698928833008e-05\n",
      "Node 872: 0.0\n",
      "Node 873: 0.0\n",
      "Node 874: -0.0018000006675720215\n",
      "Node 875: 0.0\n",
      "Node 876: 0.0\n",
      "Node 877: 0.0\n",
      "Node 878: -9.995698928833008e-05\n",
      "Node 879: 0.0\n",
      "Node 880: 0.0\n",
      "Node 881: 0.0\n",
      "Node 882: 0.0\n",
      "Node 883: 0.0\n",
      "Node 884: 0.0\n",
      "Node 885: 0.0\n",
      "Node 886: -9.995698928833008e-05\n",
      "Node 887: -9.995698928833008e-05\n",
      "Node 888: 0.00020003318786621094\n",
      "Node 889: -0.00019997358322143555\n",
      "Node 890: 0.0\n",
      "Node 891: 0.0\n",
      "Node 892: 0.0\n",
      "Node 893: -0.00019997358322143555\n",
      "Node 894: 0.0\n",
      "Node 895: 0.0\n",
      "Node 896: 0.0\n",
      "Node 897: 0.0\n",
      "Node 898: -9.995698928833008e-05\n",
      "Node 899: -9.995698928833008e-05\n",
      "Node 900: -0.000599980354309082\n",
      "Node 901: -0.00019997358322143555\n",
      "Node 902: 0.0\n",
      "Node 903: -0.000899970531463623\n",
      "Node 904: -0.0004000067710876465\n",
      "Node 905: 0.0\n",
      "Node 906: 0.0\n",
      "Node 907: 0.0\n",
      "Node 908: -9.995698928833008e-05\n",
      "Node 909: 0.0\n",
      "Node 910: -0.00019997358322143555\n",
      "Node 911: -0.00019997358322143555\n",
      "Node 912: 0.0\n",
      "Node 913: -9.995698928833008e-05\n",
      "Node 914: -0.001100003719329834\n",
      "Node 915: 0.0\n",
      "Node 916: 0.0\n",
      "Node 917: 0.0\n",
      "Node 918: -0.0006999969482421875\n",
      "Node 919: -9.995698928833008e-05\n",
      "Node 920: 0.0\n",
      "Node 921: -0.00019997358322143555\n",
      "Node 922: -0.0012999773025512695\n",
      "Node 923: -0.000299990177154541\n",
      "Node 924: 0.0\n",
      "Node 925: 0.0\n",
      "Node 926: 0.0\n",
      "Node 927: 0.0009999871253967285\n",
      "Node 928: 0.00010001659393310547\n",
      "Node 929: 0.0\n",
      "Node 930: 0.0\n",
      "Node 931: 0.0\n",
      "Node 932: 0.0\n",
      "Node 933: 0.0\n",
      "Node 934: -9.995698928833008e-05\n",
      "Node 935: 0.0\n",
      "Node 936: 0.0\n",
      "Node 937: 0.0\n",
      "Node 938: 0.0\n",
      "Node 939: -9.995698928833008e-05\n",
      "Node 940: 0.0\n",
      "Node 941: -0.00019997358322143555\n",
      "Node 942: 0.0\n",
      "Node 943: -0.000299990177154541\n",
      "Node 944: 0.0\n",
      "Node 945: 0.0\n",
      "Node 946: -0.00019997358322143555\n",
      "Node 947: 0.0\n",
      "Node 948: 0.0\n",
      "Node 949: 0.0\n",
      "Node 950: 0.0\n",
      "Node 951: 0.0\n",
      "Node 952: -9.995698928833008e-05\n",
      "Node 953: 0.0\n",
      "Node 954: 0.0\n",
      "Node 955: 0.0\n",
      "Node 956: -0.001100003719329834\n",
      "Node 957: 0.0\n",
      "Node 958: 0.0\n",
      "Node 959: -0.0006999969482421875\n",
      "Node 960: 0.0\n",
      "Node 961: 0.0\n",
      "Node 962: 0.0\n",
      "Node 963: -0.0029999613761901855\n",
      "Node 964: 0.0\n",
      "Node 965: 0.0\n",
      "Node 966: 0.0\n",
      "Node 967: 0.0\n",
      "Node 968: 0.0\n",
      "Node 969: 0.0\n",
      "Node 970: 0.0\n",
      "Node 971: 0.0\n",
      "Node 972: 0.0\n",
      "Node 973: 0.0\n",
      "Node 974: 0.00020003318786621094\n",
      "Node 975: 0.0\n",
      "Node 976: 0.0\n",
      "Node 977: 0.0\n",
      "Node 978: -0.00019997358322143555\n",
      "Node 979: -0.0007999539375305176\n",
      "Node 980: 0.0\n",
      "Node 981: 0.0\n",
      "Node 982: 0.0\n",
      "Node 983: -9.995698928833008e-05\n",
      "Node 984: 0.0\n",
      "Node 985: 0.0\n",
      "Node 986: 0.0\n",
      "Node 987: 0.0006000399589538574\n",
      "Node 988: -9.995698928833008e-05\n",
      "Node 989: -0.0004000067710876465\n",
      "Node 990: -0.000899970531463623\n",
      "Node 991: 0.0\n",
      "Node 992: 0.0\n",
      "Node 993: 0.0\n",
      "Node 994: 0.00010001659393310547\n",
      "Node 995: -9.995698928833008e-05\n",
      "Node 996: 0.00010001659393310547\n",
      "Node 997: 0.0\n",
      "Node 998: -9.995698928833008e-05\n",
      "Node 999: 0.0\n",
      "Node 1000: -9.995698928833008e-05\n",
      "Node 1001: 0.0\n",
      "Node 1002: 0.00020003318786621094\n",
      "Node 1003: 0.00010001659393310547\n",
      "Node 1004: 0.0\n",
      "Node 1005: 0.00010001659393310547\n",
      "Node 1006: -0.0004000067710876465\n",
      "Node 1007: 0.00010001659393310547\n",
      "Node 1008: 0.00020003318786621094\n",
      "Node 1009: 0.0\n",
      "Node 1010: 0.0\n",
      "Node 1011: 0.00020003318786621094\n",
      "Node 1012: 0.0\n",
      "Node 1013: 0.0\n",
      "Node 1014: -0.000899970531463623\n",
      "Node 1015: 0.0\n",
      "Node 1016: 0.0\n",
      "Node 1017: -0.00019997358322143555\n",
      "Node 1018: -9.995698928833008e-05\n",
      "Node 1019: 0.0\n",
      "Node 1020: -0.003099977970123291\n",
      "Node 1021: 0.0\n",
      "Node 1022: -9.995698928833008e-05\n",
      "Node 1023: 0.0\n"
     ]
    }
   ],
   "source": [
    "l, a = model2.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights = model2.get_weights()\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "for i in range(size):\n",
    "    w = copy.deepcopy(or_weights)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model2.set_weights(w)\n",
    "    nl, na = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "    print(f\"Node {i}:\", 1.*(na - a) + 0*(l - nl))\n",
    "    change = l - nl\n",
    "    if change <= tol_high and change >= tol_low:\n",
    "        num_zeros += 1\n",
    "        z += [i]\n",
    "    elif change > 0:\n",
    "        num_worse += 1\n",
    "        wr += [i]\n",
    "    else:\n",
    "        num_important += 1\n",
    "        imp += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Nodes: 345\n",
      "Worse Nodes: 296\n",
      "Important Nodes: 383\n"
     ]
    }
   ],
   "source": [
    "print(\"Zero Nodes:\", num_zeros)\n",
    "print(\"Worse Nodes:\", num_worse)\n",
    "print(\"Important Nodes:\", num_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## IMPORTANT NODES ########\n",
      "Node 1: -0.00011387667655948785\n",
      "Node 3: -0.00010128152370458832\n",
      "Node 6: -3.623464107516128e-05\n",
      "Node 7: -0.001998766946792685\n",
      "Node 8: -0.001308627104759319\n",
      "Node 9: -0.0010139400839805912\n",
      "Node 11: -0.00024395577907565524\n",
      "Node 12: -8.690321445470328e-05\n",
      "Node 15: -0.00061111836433414\n",
      "Node 17: -0.0001145778417588339\n",
      "Node 21: -0.00017343966960914337\n",
      "Node 22: -6.050019264225437e-05\n",
      "Node 23: -0.0013901935100556484\n",
      "Node 28: -0.0004116062402725884\n",
      "Node 29: -0.0014539723873139287\n",
      "Node 32: -2.040214538578944e-05\n",
      "Node 33: -0.0019836359739303644\n",
      "Node 43: -0.0001338123559951887\n",
      "Node 45: -0.00010271003246309363\n",
      "Node 46: -8.029680252075178e-05\n",
      "Node 47: -1.0653376579350393e-05\n",
      "Node 49: -1.1082053184563456e-05\n",
      "Node 52: -0.002979259657859834\n",
      "Node 54: -0.0003367197751998896\n",
      "Node 55: -0.00018534734249120444\n",
      "Node 56: -3.561494350434735e-05\n",
      "Node 58: -0.004257445406913862\n",
      "Node 70: -2.6546597480803236e-05\n",
      "Node 72: -0.00643334937095652\n",
      "Node 77: -0.00010503306388864875\n",
      "Node 79: -0.00019823505878457937\n",
      "Node 81: -0.0014568001508713513\n",
      "Node 89: -0.0002669120788574286\n",
      "Node 90: -1.6926932334904166e-05\n",
      "Node 97: -3.8868212699938454e-05\n",
      "Node 99: -0.0004109164953232103\n",
      "Node 102: -0.0011747028112412128\n",
      "Node 103: -0.0004207309722901176\n",
      "Node 104: -0.010803828930854853\n",
      "Node 108: -9.642388820652048e-05\n",
      "Node 109: -0.0025575856447220824\n",
      "Node 110: -0.0003967954397201545\n",
      "Node 114: -0.00045332977771761485\n",
      "Node 116: -4.912278652191571e-05\n",
      "Node 122: -0.0036361079692841525\n",
      "Node 125: -7.12543249130615e-05\n",
      "Node 126: -6.661698818211104e-05\n",
      "Node 128: -0.0002673951625824511\n",
      "Node 129: -0.00012347278594970756\n",
      "Node 131: -0.005431611919403112\n",
      "Node 134: -0.00017966468334207342\n",
      "Node 137: -3.410708904272308e-05\n",
      "Node 138: -4.60501432418825e-05\n",
      "Node 140: -0.0001896016836167025\n",
      "Node 141: -6.267600059517431e-05\n",
      "Node 144: -0.0008142029762268077\n",
      "Node 149: -0.0011943308591843005\n",
      "Node 151: -0.00045097131729132656\n",
      "Node 153: -0.00018334259986885204\n",
      "Node 158: -0.006724979543686005\n",
      "Node 159: -0.00021240425109869676\n",
      "Node 161: -3.39410543441776e-05\n",
      "Node 169: -6.573789119723727e-05\n",
      "Node 178: -0.0006259119987488404\n",
      "Node 179: -0.0021497493505477916\n",
      "Node 180: -0.0002545802831650512\n",
      "Node 181: -0.00014678056240091664\n",
      "Node 182: -0.0006738871574402117\n",
      "Node 187: -0.0032814300775528826\n",
      "Node 189: -2.3518109321662983e-05\n",
      "Node 190: -0.0027910580158234666\n",
      "Node 192: -0.00021722815036784304\n",
      "Node 194: -0.00012967677116404452\n",
      "Node 195: -0.0006505080699921173\n",
      "Node 198: -0.002682304072380126\n",
      "Node 199: -0.0005816415548325349\n",
      "Node 202: -6.0871243476867676e-05\n",
      "Node 203: -0.0006169578552246424\n",
      "Node 204: -0.0007357247591018856\n",
      "Node 207: -2.800767421728967e-05\n",
      "Node 212: -0.0021058624267578097\n",
      "Node 213: -0.00026287157535553884\n",
      "Node 214: -0.00010458352565767548\n",
      "Node 217: -0.0005610178709030134\n",
      "Node 219: -0.0005337561368943256\n",
      "Node 220: -0.001347769379615782\n",
      "Node 221: -0.0012651754140854399\n",
      "Node 222: -0.005734951782226605\n",
      "Node 226: -0.00021180219650274168\n",
      "Node 233: -0.00012568337917329142\n",
      "Node 236: -0.0013174049854278724\n",
      "Node 237: -0.0055352200746536795\n",
      "Node 239: -8.966717720038098e-05\n",
      "Node 246: -2.729296684267357e-05\n",
      "Node 247: -0.0004491989374161287\n",
      "Node 248: -0.0002342150926590847\n",
      "Node 251: -3.648946285250698e-05\n",
      "Node 252: -4.97769117355773e-05\n",
      "Node 255: -0.0006763345956802835\n",
      "Node 256: -0.00018369758129122005\n",
      "Node 260: -0.026306548976898214\n",
      "Node 261: -0.0025464405536652635\n",
      "Node 263: -0.00027091605663309704\n",
      "Node 264: -0.00014762141704560694\n",
      "Node 266: -0.00012527043819432482\n",
      "Node 267: -0.00015138561725625\n",
      "Node 268: -0.00017402803897859087\n",
      "Node 274: -2.001430988318731e-05\n",
      "Node 275: -0.0006108304738998704\n",
      "Node 280: -8.276689052588715e-05\n",
      "Node 281: -0.002144640994071967\n",
      "Node 285: -6.553866863256985e-05\n",
      "Node 290: -1.436431407930705e-05\n",
      "Node 291: -0.00019640860557557094\n",
      "Node 292: -0.0005514681816101152\n",
      "Node 297: -0.0002980831623078384\n",
      "Node 302: -0.0007231146097184116\n",
      "Node 304: -0.00022009406089784633\n",
      "Node 306: -0.0004940855979920222\n",
      "Node 308: -0.010069247913360613\n",
      "Node 309: -0.0003546828985214745\n",
      "Node 312: -4.295327663428328e-05\n",
      "Node 314: -0.004780959272384644\n",
      "Node 317: -0.0008169394731522006\n",
      "Node 318: -0.00037398586273196255\n",
      "Node 322: -0.0025887368202209826\n",
      "Node 324: -0.00026664369106299546\n",
      "Node 325: -1.0324358940216882e-05\n",
      "Node 326: -0.00010159573555001966\n",
      "Node 327: -5.996282100684258e-05\n",
      "Node 339: -0.0010469065666198762\n",
      "Node 342: -0.0002665493726731194\n",
      "Node 348: -0.00350835092067725\n",
      "Node 349: -1.9838500022961725e-05\n",
      "Node 350: -8.477261066441866e-05\n",
      "Node 354: -0.0006554376363754333\n",
      "Node 358: -0.0007363051652908847\n",
      "Node 359: -0.00103150820732123\n",
      "Node 360: -8.32259178161765e-05\n",
      "Node 361: -6.502931118013144e-05\n",
      "Node 363: -0.0013999433755874646\n",
      "Node 368: -5.346803665162714e-05\n",
      "Node 370: -0.0017090142011643161\n",
      "Node 371: -0.00026183967590331925\n",
      "Node 377: -0.00017865617275247825\n",
      "Node 380: -1.1188459396405115e-05\n",
      "Node 382: -0.00030209181308749056\n",
      "Node 383: -0.0001553015470505681\n",
      "Node 386: -0.0023178384542466013\n",
      "Node 389: -0.0025883004426956457\n",
      "Node 390: -0.00030043394565582027\n",
      "Node 393: -4.07658576966341e-05\n",
      "Node 396: -0.001584532141685524\n",
      "Node 397: -9.533097743996688e-05\n",
      "Node 398: -0.0002805321455002785\n",
      "Node 399: -6.372516155250807e-05\n",
      "Node 403: -0.0009015366315842099\n",
      "Node 404: -0.004357924771308919\n",
      "Node 405: -0.002057216119766281\n",
      "Node 413: -0.0005432653903961748\n",
      "Node 414: -0.0004345122575760696\n",
      "Node 416: -0.00030869810581213297\n",
      "Node 419: -0.001637060451507577\n",
      "Node 423: -0.00023095185756683012\n",
      "Node 427: -1.5389847755509223e-05\n",
      "Node 429: -0.00027665183544167693\n",
      "Node 432: -0.0016936363458633918\n",
      "Node 437: -1.8862509727490817e-05\n",
      "Node 439: -0.0006717750549316559\n",
      "Node 442: -0.0002974601030349966\n",
      "Node 443: -0.0033606804609299656\n",
      "Node 444: -0.00039009313583382443\n",
      "Node 447: -1.705899238590991e-05\n",
      "Node 448: -0.00039219310283666875\n",
      "Node 449: -0.0004194704055786591\n",
      "Node 450: -0.0002585416555405118\n",
      "Node 451: -0.0024050504446030407\n",
      "Node 452: -0.0009370579481124963\n",
      "Node 455: -0.00024178054332735233\n",
      "Node 456: -0.0011699456214905313\n",
      "Node 460: -0.0006635210752488074\n",
      "Node 463: -0.00019702067375193533\n",
      "Node 464: -0.0046910986423492895\n",
      "Node 465: -0.00011959998607635747\n",
      "Node 466: -0.01503779160976415\n",
      "Node 467: -0.00021968665122995557\n",
      "Node 468: -9.495613574983519e-05\n",
      "Node 474: -0.1744079635381699\n",
      "Node 478: -0.0007634058713913427\n",
      "Node 479: -0.0014693282842637156\n",
      "Node 484: -0.0011475330352783697\n",
      "Node 485: -0.00055393612384802\n",
      "Node 488: -0.0006828382015229284\n",
      "Node 491: -0.00021197614669810072\n",
      "Node 495: -7.217595577246882e-05\n",
      "Node 498: -0.005349798178672871\n",
      "Node 499: -4.3731522560208624e-05\n",
      "Node 500: -0.0009327486276626828\n",
      "Node 501: -0.0010153387069702369\n",
      "Node 507: -2.5596880912859277e-05\n",
      "Node 508: -0.0012361897468566863\n",
      "Node 510: -0.0001306649923324965\n",
      "Node 512: -0.00036611154079446706\n",
      "Node 518: -0.0001155831098557103\n",
      "Node 519: -9.952678680424931e-05\n",
      "Node 521: -0.0001613806486130409\n",
      "Node 522: -0.0003856752157211929\n",
      "Node 529: -0.0023582089424133423\n",
      "Node 533: -0.0001813401460648567\n",
      "Node 536: -0.0005493505239486662\n",
      "Node 537: -1.5163016319319489e-05\n",
      "Node 539: -9.492156505586102e-05\n",
      "Node 540: -0.0022780153751373877\n",
      "Node 541: -0.014342165207862911\n",
      "Node 543: -0.0016917143821716385\n",
      "Node 545: -0.001251718902587906\n",
      "Node 547: -0.000177158546447842\n",
      "Node 553: -0.0003812254667282522\n",
      "Node 554: -2.9220461845502754e-05\n",
      "Node 557: -0.00040754230022432836\n",
      "Node 558: -0.0013722164869308662\n",
      "Node 562: -0.0023120203733444367\n",
      "Node 564: -0.004025861167907818\n",
      "Node 565: -0.0037310574054718604\n",
      "Node 566: -0.0013311229705811334\n",
      "Node 567: -1.2641835212767738e-05\n",
      "Node 568: -0.000119024586677563\n",
      "Node 572: -0.0012299029588699328\n",
      "Node 574: -0.0005031553745270534\n",
      "Node 576: -0.0005935272216797127\n",
      "Node 577: -4.631326198578645e-05\n",
      "Node 578: -5.639946460733736e-05\n",
      "Node 581: -0.00023171923160558716\n",
      "Node 582: -0.00013121354579925804\n",
      "Node 585: -0.005722388243675325\n",
      "Node 587: -0.00012534875869751883\n",
      "Node 589: -0.0003252710103989198\n",
      "Node 590: -0.0001554457902909201\n",
      "Node 592: -0.0013625964879989683\n",
      "Node 595: -0.0005815428972244518\n",
      "Node 596: -1.4364600181671072e-05\n",
      "Node 599: -0.0005145303010940516\n",
      "Node 602: -0.00022498517036440013\n",
      "Node 607: -0.0013544183492660844\n",
      "Node 611: -2.0492625236512296e-05\n",
      "Node 616: -0.00012662274837493825\n",
      "Node 618: -0.00010387132167821722\n",
      "Node 620: -9.467751979830297e-05\n",
      "Node 622: -0.0003855955600738925\n",
      "Node 623: -0.00011563651561741217\n",
      "Node 624: -9.969918727881222e-05\n",
      "Node 626: -0.05331242697238925\n",
      "Node 629: -0.00042933578491211044\n",
      "Node 632: -0.0008370521783829243\n",
      "Node 633: -0.00010615735054020359\n",
      "Node 634: -0.0012187806367874865\n",
      "Node 638: -0.0018341120481492057\n",
      "Node 645: -0.0013452542543411772\n",
      "Node 652: -7.765243053436954e-05\n",
      "Node 654: -0.00029676675796519447\n",
      "Node 655: -0.0013721809625626413\n",
      "Node 659: -4.540741443637142e-05\n",
      "Node 661: -3.815970420839587e-05\n",
      "Node 675: -0.0003980028867721641\n",
      "Node 678: -3.85227918625608e-05\n",
      "Node 679: -0.00024770328998569013\n",
      "Node 686: -4.711091518405652e-05\n",
      "Node 689: -0.00011992614269262791\n",
      "Node 690: -6.705377101901e-05\n",
      "Node 694: -0.0003076820373535405\n",
      "Node 696: -0.0021218869924546357\n",
      "Node 700: -6.474330425265684e-05\n",
      "Node 702: -0.0018901288986206133\n",
      "Node 704: -0.0005010874986649538\n",
      "Node 705: -0.00021394097805027545\n",
      "Node 706: -6.388456821448951e-05\n",
      "Node 707: -0.0003577786445617681\n",
      "Node 710: -0.005174541115760878\n",
      "Node 711: -0.002446896874904736\n",
      "Node 713: -0.00029424369335184775\n",
      "Node 716: -7.407567501072077e-05\n",
      "Node 717: -7.110955715183831e-05\n",
      "Node 723: -0.0005472489118576451\n",
      "Node 726: -1.0154604911827292e-05\n",
      "Node 728: -0.001398223161697465\n",
      "Node 729: -0.0002490278482437658\n",
      "Node 731: -0.00010224921703338996\n",
      "Node 733: -1.610200405122164e-05\n",
      "Node 734: -0.0006508929967881105\n",
      "Node 738: -1.649253368385395e-05\n",
      "Node 739: -0.02102575454711919\n",
      "Node 745: -0.0034961145639420543\n",
      "Node 746: -0.0005908359527588836\n",
      "Node 753: -9.30104494095696e-05\n",
      "Node 756: -0.007534383368492215\n",
      "Node 757: -0.0006345223665238375\n",
      "Node 759: -0.00028322312831885466\n",
      "Node 762: -0.0005578039407730939\n",
      "Node 771: -0.00018349657058724933\n",
      "Node 777: -3.750596046447896e-05\n",
      "Node 781: -0.0013530796289444869\n",
      "Node 784: -7.33799695968873e-05\n",
      "Node 786: -0.005411941957473854\n",
      "Node 788: -0.007630065178871215\n",
      "Node 789: -0.0008113770484924343\n",
      "Node 795: -0.00022045848369600662\n",
      "Node 797: -6.85939073563624e-05\n",
      "Node 798: -0.0002136872053146721\n",
      "Node 802: -0.00010785777568822486\n",
      "Node 803: -5.200431346896739e-05\n",
      "Node 804: -5.273225307467211e-05\n",
      "Node 808: -5.4729437828138217e-05\n",
      "Node 815: -0.0003251821279526279\n",
      "Node 820: -0.0004667472124100236\n",
      "Node 832: -0.003024027347564795\n",
      "Node 836: -0.000203189969062878\n",
      "Node 837: -0.012059053993225155\n",
      "Node 842: -1.3674330711421767e-05\n",
      "Node 843: -1.9838118553217043e-05\n",
      "Node 846: -0.00011786911487587926\n",
      "Node 850: -3.244023323067946e-05\n",
      "Node 853: -9.701936244965736e-05\n",
      "Node 854: -0.00014842326641084647\n",
      "Node 855: -0.0007906493902206657\n",
      "Node 861: -0.0029493047952652285\n",
      "Node 862: -0.00010272288322454859\n",
      "Node 870: -0.00022279727458962562\n",
      "Node 871: -0.0002185800790787118\n",
      "Node 879: -1.0376715660176394e-05\n",
      "Node 880: -0.00025754086971285783\n",
      "Node 886: -5.2196288108929956e-05\n",
      "Node 887: -0.00021199746131905783\n",
      "Node 890: -0.0007944013595581056\n",
      "Node 897: -0.00040017082691201633\n",
      "Node 899: -0.0003236591100693742\n",
      "Node 900: -0.0018895686149598134\n",
      "Node 901: -0.0005796073198318652\n",
      "Node 903: -0.003526886272430474\n",
      "Node 904: -0.0006245061397552876\n",
      "Node 908: -0.0002538523674011639\n",
      "Node 909: -0.0009883595228196151\n",
      "Node 911: -7.334432601935337e-05\n",
      "Node 913: -0.001263572072982888\n",
      "Node 914: -0.0013277548789978333\n",
      "Node 915: -0.0003226905584335915\n",
      "Node 917: -0.0003525235891342904\n",
      "Node 918: -0.0008025931358337512\n",
      "Node 919: -0.00033150944709781083\n",
      "Node 922: -0.007326039671897933\n",
      "Node 923: -1.5167355537415439e-05\n",
      "Node 929: -1.0232853889569071e-05\n",
      "Node 933: -0.0003768518209458227\n",
      "Node 934: -0.0028995786190033623\n",
      "Node 937: -6.489388942720886e-05\n",
      "Node 943: -0.0003212208271027306\n",
      "Node 945: -0.00012841486930847967\n",
      "Node 946: -1.3221573829658162e-05\n",
      "Node 949: -7.641427516946742e-05\n",
      "Node 952: -1.962342262273875e-05\n",
      "Node 956: -0.004848306846618744\n",
      "Node 957: -1.3528227806092197e-05\n",
      "Node 958: -0.0001737782001496324\n",
      "Node 959: -0.0013213812351227094\n",
      "Node 960: -1.4651989936886878e-05\n",
      "Node 963: -0.00587772367000583\n",
      "Node 978: -2.553174495700805e-05\n",
      "Node 979: -0.001914562320709301\n",
      "Node 981: -0.0004794481992722588\n",
      "Node 983: -0.0006952962636948001\n",
      "Node 989: -0.0002681946277618419\n",
      "Node 990: -0.00040919466018685124\n",
      "Node 995: -0.00032174880504609593\n",
      "Node 996: -7.281997203834045e-05\n",
      "Node 998: -4.7870802879379504e-05\n",
      "Node 1000: -0.00027327001094823444\n",
      "Node 1002: -0.00689141383171088\n",
      "Node 1006: -0.0016929648160934896\n",
      "Node 1014: -0.008062954616546714\n",
      "Node 1017: -0.0005469923257828135\n",
      "Node 1018: -0.0005640967130661201\n",
      "Node 1019: -0.000468229937553466\n",
      "Node 1020: -0.012319068634510044\n",
      "Node 1022: -3.421289920813031e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"######## IMPORTANT NODES ########\")\n",
    "for i in imp:\n",
    "    w = copy.deepcopy(or_weights)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model2.set_weights(w)\n",
    "    nl, na = tester_model2.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.*(na - a) + 1.0*(l - nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## WORSE NODES ########\n",
      "Node 0: 4.13911819457935e-05\n",
      "Node 4: 0.00023835399150839987\n",
      "Node 5: 0.00025930373668670015\n",
      "Node 10: 0.00020082871913906786\n",
      "Node 14: 0.00014250781536095936\n",
      "Node 18: 0.00026486358642574537\n",
      "Node 19: 0.0016526093482970294\n",
      "Node 24: 0.00011118769645690474\n",
      "Node 30: 4.8133587837173586e-05\n",
      "Node 31: 0.00013076276779167006\n",
      "Node 35: 0.00019327156543724744\n",
      "Node 37: 5.299410820003292e-05\n",
      "Node 39: 0.0006802566289900724\n",
      "Node 44: 0.0002921863317488782\n",
      "Node 51: 0.0002678472757339234\n",
      "Node 57: 0.0009409231901168491\n",
      "Node 59: 8.678886890400861e-05\n",
      "Node 60: 0.0002991854667663274\n",
      "Node 62: 0.00048684692382805483\n",
      "Node 63: 3.161346912383678e-05\n",
      "Node 65: 0.0010018054485320116\n",
      "Node 66: 0.0033053781270980798\n",
      "Node 67: 0.00037543771266934645\n",
      "Node 68: 0.0002577392339705442\n",
      "Node 74: 0.0006493238449095662\n",
      "Node 75: 0.002567309141158991\n",
      "Node 76: 1.1251425743097343e-05\n",
      "Node 78: 0.0003851246595382163\n",
      "Node 80: 0.0008524636268615327\n",
      "Node 85: 0.0014576805830001094\n",
      "Node 87: 6.532504558554031e-05\n",
      "Node 88: 0.0014497780323028397\n",
      "Node 91: 0.00035044147968288364\n",
      "Node 96: 4.5430445671068576e-05\n",
      "Node 98: 0.00010812244415281391\n",
      "Node 100: 6.118757724760115e-05\n",
      "Node 105: 0.0009176903724670282\n",
      "Node 107: 0.000367523407935999\n",
      "Node 111: 0.00028635733127591045\n",
      "Node 112: 0.00021024091243737608\n",
      "Node 124: 0.00012600877284996148\n",
      "Node 130: 0.0003954114675521758\n",
      "Node 139: 0.0017998697280883214\n",
      "Node 145: 1.5839791297822536e-05\n",
      "Node 152: 1.7416524887070572e-05\n",
      "Node 156: 0.002055597639083806\n",
      "Node 164: 0.00043657772541039197\n",
      "Node 165: 0.0009215997219085281\n",
      "Node 168: 0.0037831518411636234\n",
      "Node 176: 3.2941651344264855e-05\n",
      "Node 185: 9.969623088834645e-05\n",
      "Node 200: 8.718249797812216e-05\n",
      "Node 201: 0.0007271325349806901\n",
      "Node 208: 0.0004855483531951421\n",
      "Node 210: 0.00045331947803495254\n",
      "Node 211: 3.6400604248032487e-05\n",
      "Node 216: 3.507559299464891e-05\n",
      "Node 218: 7.922174930563575e-05\n",
      "Node 227: 0.001999206995964009\n",
      "Node 228: 0.00034620468616475275\n",
      "Node 229: 1.0802483558558507e-05\n",
      "Node 230: 0.00017989618778224692\n",
      "Node 231: 0.0006318882703780782\n",
      "Node 232: 4.1492915153429344e-05\n",
      "Node 238: 0.002481144523620582\n",
      "Node 240: 2.15317010878735e-05\n",
      "Node 241: 4.86538171767803e-05\n",
      "Node 242: 0.00033911952972409676\n",
      "Node 244: 0.00035349104404447385\n",
      "Node 250: 0.00018574163913720465\n",
      "Node 257: 0.00041221306324001894\n",
      "Node 262: 1.7911839485074132e-05\n",
      "Node 273: 9.118545055386562e-05\n",
      "Node 276: 0.00017344338893887645\n",
      "Node 278: 0.00010287659168239927\n",
      "Node 282: 0.00010137579441060751\n",
      "Node 283: 0.00025208647251129257\n",
      "Node 286: 0.002964880561828531\n",
      "Node 288: 0.000358235359191883\n",
      "Node 289: 0.0005076984167098342\n",
      "Node 296: 0.0004573179483413625\n",
      "Node 298: 0.0014981727600097416\n",
      "Node 299: 2.472517490381687e-05\n",
      "Node 300: 0.0007812189340591358\n",
      "Node 303: 0.0003814029932021068\n",
      "Node 307: 0.0043270287275314345\n",
      "Node 313: 0.00028956224918363915\n",
      "Node 319: 0.0002677806138992045\n",
      "Node 321: 0.00600872132778163\n",
      "Node 328: 8.84208917617535e-05\n",
      "Node 329: 0.0017624016523360986\n",
      "Node 331: 0.0005776397943496603\n",
      "Node 334: 0.00029026119709008036\n",
      "Node 335: 5.120480060571975e-05\n",
      "Node 337: 1.6799879073992052e-05\n",
      "Node 340: 7.576844692225304e-05\n",
      "Node 343: 0.0015987859249114722\n",
      "Node 344: 0.00013127658367151351\n",
      "Node 351: 4.551689624776589e-05\n",
      "Node 352: 0.001310991954803442\n",
      "Node 353: 0.0002689582586288264\n",
      "Node 355: 7.866890430441309e-05\n",
      "Node 356: 0.003077594375610282\n",
      "Node 366: 2.526724338525188e-05\n",
      "Node 367: 6.279137134546087e-05\n",
      "Node 369: 0.0003032266855239607\n",
      "Node 372: 8.522078990935711e-05\n",
      "Node 373: 3.0619263648890005e-05\n",
      "Node 376: 0.0010438749313353979\n",
      "Node 379: 0.0005421706438064611\n",
      "Node 381: 7.633097171777514e-05\n",
      "Node 384: 1.8918061256312235e-05\n",
      "Node 388: 0.00011255795955655845\n",
      "Node 391: 0.005345786523818896\n",
      "Node 394: 0.00025434920787803517\n",
      "Node 395: 8.52636575697785e-05\n",
      "Node 400: 0.0020637974500655742\n",
      "Node 401: 0.0002701601266860143\n",
      "Node 402: 0.0010452893972395794\n",
      "Node 406: 0.00013207218646993368\n",
      "Node 409: 0.0005846965312956698\n",
      "Node 410: 5.5252408981298906e-05\n",
      "Node 412: 0.00021118690967558695\n",
      "Node 417: 0.003609065747260942\n",
      "Node 418: 2.357723712920201e-05\n",
      "Node 421: 0.002697884368896397\n",
      "Node 424: 0.0006544918775558406\n",
      "Node 431: 0.00017799437046051025\n",
      "Node 433: 5.242507457725587e-05\n",
      "Node 434: 0.0003519457578659013\n",
      "Node 435: 0.002132174181938118\n",
      "Node 436: 0.0026913144350051432\n",
      "Node 445: 9.944219589230752e-05\n",
      "Node 446: 2.645084857932911e-05\n",
      "Node 457: 0.00015684478282917969\n",
      "Node 458: 1.3279414176836646e-05\n",
      "Node 459: 0.00023923540115355912\n",
      "Node 469: 0.00042355592250820084\n",
      "Node 470: 0.0004920930624007225\n",
      "Node 471: 0.00017214920520780552\n",
      "Node 472: 0.0014452532052993128\n",
      "Node 473: 0.0004491445779799541\n",
      "Node 475: 0.0019279115438460837\n",
      "Node 477: 4.228520393367852e-05\n",
      "Node 481: 0.0002682692766189376\n",
      "Node 482: 0.0008946705341338346\n",
      "Node 487: 0.0004971310138701668\n",
      "Node 494: 2.2142457961948914e-05\n",
      "Node 497: 0.00013684327602381252\n",
      "Node 503: 0.0001650688409804424\n",
      "Node 504: 0.0018082295894622513\n",
      "Node 506: 0.0004101706504821001\n",
      "Node 509: 2.4140572547848294e-05\n",
      "Node 511: 1.544420719146178e-05\n",
      "Node 514: 0.000625992178916901\n",
      "Node 517: 0.00042231993675223567\n",
      "Node 524: 0.0004236878633498975\n",
      "Node 526: 5.123858451838714e-05\n",
      "Node 531: 0.002863945174217175\n",
      "Node 532: 4.359757900229688e-05\n",
      "Node 538: 0.0007465235471725284\n",
      "Node 546: 0.00026416218280789217\n",
      "Node 548: 0.0004518362998962022\n",
      "Node 549: 0.0005998208284377915\n",
      "Node 550: 0.00047653393745417105\n",
      "Node 552: 0.0007035480022430374\n",
      "Node 555: 0.0001748155117033967\n",
      "Node 559: 0.0002133131742476646\n",
      "Node 561: 0.004913639140128989\n",
      "Node 571: 0.000543982386589037\n",
      "Node 575: 0.00032382311820977616\n",
      "Node 583: 0.001153769659996029\n",
      "Node 594: 3.399269580839359e-05\n",
      "Node 598: 0.00023459980487816612\n",
      "Node 604: 7.256453037252086e-05\n",
      "Node 605: 0.0001883919239044074\n",
      "Node 606: 0.00044850256443018655\n",
      "Node 609: 0.00016130890846244306\n",
      "Node 610: 0.00031856052875511853\n",
      "Node 612: 0.0001412982225417192\n",
      "Node 614: 0.00027561819553367783\n",
      "Node 617: 0.0017709749937057095\n",
      "Node 621: 0.000186425113677946\n",
      "Node 627: 0.0015385196924209144\n",
      "Node 628: 0.00021794588565826611\n",
      "Node 630: 7.929472923273728e-05\n",
      "Node 631: 0.00016685357093804143\n",
      "Node 637: 1.5553879737839615e-05\n",
      "Node 640: 0.00010785129070278732\n",
      "Node 641: 2.1201968192996468e-05\n",
      "Node 644: 0.00041337454318990385\n",
      "Node 650: 1.2127995490995858e-05\n",
      "Node 651: 0.0011394093275070016\n",
      "Node 653: 0.0017434208631514903\n",
      "Node 656: 0.0007393522500991345\n",
      "Node 658: 0.0003574560642242153\n",
      "Node 662: 0.0030352092027663424\n",
      "Node 666: 0.0005560476541518478\n",
      "Node 667: 4.752047061917786e-05\n",
      "Node 670: 0.00025858356952657147\n",
      "Node 674: 1.316599845879729e-05\n",
      "Node 676: 0.00016137878894795232\n",
      "Node 681: 0.0016762404680251697\n",
      "Node 682: 0.0005475329875945789\n",
      "Node 684: 0.0008913078546523012\n",
      "Node 685: 7.585213184346795e-05\n",
      "Node 697: 0.0013411007404326991\n",
      "Node 698: 1.2266778945835277e-05\n",
      "Node 703: 7.566139698023111e-05\n",
      "Node 712: 0.0017617330551147337\n",
      "Node 714: 0.0001844284772872795\n",
      "Node 718: 0.0002865415811538208\n",
      "Node 721: 0.0005201516389846672\n",
      "Node 722: 0.0006917467594146354\n",
      "Node 735: 6.782267093652372e-05\n",
      "Node 737: 0.008142872977256688\n",
      "Node 740: 0.0017054446458816264\n",
      "Node 744: 0.0003190123796462663\n",
      "Node 751: 0.0007566768646239952\n",
      "Node 752: 0.0006356782913207182\n",
      "Node 760: 7.494363784787961e-05\n",
      "Node 761: 0.00042064251899709415\n",
      "Node 764: 0.00041882114410396465\n",
      "Node 765: 0.0001133528470992351\n",
      "Node 766: 0.0010295860052108585\n",
      "Node 768: 7.565710544576998e-05\n",
      "Node 769: 0.0002531418085097714\n",
      "Node 770: 0.001012254834175108\n",
      "Node 773: 0.00014083192348479656\n",
      "Node 774: 1.9521975517244883e-05\n",
      "Node 775: 0.00040244061946859144\n",
      "Node 776: 5.027463436124702e-05\n",
      "Node 778: 0.00014749701023097916\n",
      "Node 780: 0.0007579753637313447\n",
      "Node 782: 1.561892032619916e-05\n",
      "Node 794: 0.00038334586620325517\n",
      "Node 796: 0.0001545632362365268\n",
      "Node 805: 8.523435592644457e-05\n",
      "Node 806: 0.0011661454915999903\n",
      "Node 809: 0.00046252548694603046\n",
      "Node 810: 0.0034908986806868603\n",
      "Node 816: 5.213305950158009e-05\n",
      "Node 817: 0.0004255003213882169\n",
      "Node 818: 0.0002924488306045303\n",
      "Node 819: 4.2090249061543794e-05\n",
      "Node 824: 2.3283100128113077e-05\n",
      "Node 826: 0.0014242475748061922\n",
      "Node 831: 0.00010899693965904511\n",
      "Node 833: 8.755943775173858e-05\n",
      "Node 858: 0.00014933416843410097\n",
      "Node 859: 0.00013874862194052096\n",
      "Node 863: 0.00010677680969228831\n",
      "Node 869: 0.0003427600145339271\n",
      "Node 872: 0.0007024865865706831\n",
      "Node 873: 5.779554843898804e-05\n",
      "Node 874: 0.0013038513898848691\n",
      "Node 875: 0.0004138479471206269\n",
      "Node 876: 0.00029632844924920576\n",
      "Node 878: 0.0002615249395370167\n",
      "Node 888: 0.0022491101503371302\n",
      "Node 889: 0.0014324817657470623\n",
      "Node 895: 0.0007838176965713206\n",
      "Node 898: 0.0006226968526840126\n",
      "Node 902: 0.0011526291131972455\n",
      "Node 906: 0.000151814007758988\n",
      "Node 910: 0.0005705785989761347\n",
      "Node 920: 3.1133675575167885e-05\n",
      "Node 921: 0.0015283572912215826\n",
      "Node 927: 0.007716363954544003\n",
      "Node 928: 0.00018569586277006334\n",
      "Node 930: 0.0002826450824736648\n",
      "Node 931: 5.6442856788629925e-05\n",
      "Node 932: 3.68940591811695e-05\n",
      "Node 939: 0.00026166357994072964\n",
      "Node 941: 0.00028655526638021733\n",
      "Node 942: 2.7054142951898896e-05\n",
      "Node 947: 0.001152093529701137\n",
      "Node 948: 6.098439693447144e-05\n",
      "Node 955: 0.0065513285398483\n",
      "Node 962: 0.00012712495326994144\n",
      "Node 968: 0.0005077872991561261\n",
      "Node 971: 0.0008215491294860655\n",
      "Node 974: 0.0003163160562514866\n",
      "Node 977: 4.409162998197935e-05\n",
      "Node 984: 0.00014138891696924283\n",
      "Node 987: 0.0027895522356032876\n",
      "Node 988: 4.200503826135016e-05\n",
      "Node 991: 2.1733975410387352e-05\n",
      "Node 993: 2.1077561378479714e-05\n",
      "Node 994: 0.0006747948169707474\n",
      "Node 1003: 0.0016339164972304365\n",
      "Node 1004: 0.0002991122961043535\n",
      "Node 1005: 0.0006535417318344061\n",
      "Node 1008: 3.147621154775404e-05\n",
      "Node 1011: 0.0010812390089034452\n",
      "Node 1015: 0.00042483484745015065\n",
      "0.20305821731089035\n",
      "0.0006860074909151701\n"
     ]
    }
   ],
   "source": [
    "print(\"######## WORSE NODES ########\")\n",
    "tot = 0\n",
    "for i in wr:\n",
    "    w = copy.deepcopy(or_weights)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model2.set_weights(w)\n",
    "    nl, na = tester_model2.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.*(na - a) + 1.0*(l - nl))\n",
    "    tot += (l - nl)\n",
    "print(tot)\n",
    "print(tot / num_worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.5534 - accuracy: 0.8573\n",
      "2.899231910711286e-05\n",
      "Found something better\n",
      "0.00016685455322269503\n",
      "Found something better\n",
      "0.0001815255498886281\n",
      "Found something better\n",
      "0.0013968333435058875\n",
      "Found something better\n",
      "0.002253782577514662\n",
      "Found something better\n",
      "0.002828232312202461\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.003118926424980206\n",
      "Found something better\n",
      "0.0044161053323746305\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.005940026946067834\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8581 --- Loss: 0.5452890724182129 --- Change: 0.005940026946067834 --- New tol: -1e-05\n",
      "1.4953613292068013e-08\n",
      "Found something better\n",
      "0.00016685455322269503\n",
      "Found something better\n",
      "0.00018178560256957297\n",
      "Found something better\n",
      "0.0012985332298279027\n",
      "Found something better\n",
      "0.0022349351835251217\n",
      "Found something better\n",
      "0.0029833795356750343\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0031984505271911547\n",
      "Found something better\n",
      "0.004052414884567234\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.005695710620880157\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8591 --- Loss: 0.537580908870697 --- Change: 0.005695710620880157 --- New tol: -1e-05\n",
      "0.0003577353096008262\n",
      "Found something better\n",
      "0.0013308223533629925\n",
      "Found something better\n",
      "0.0022347859811782667\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0022383183240890503\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0034243379354476967\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8584 --- Loss: 0.5323889988422393 --- Change: 0.0034243379354476967 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00016413619995110372\n",
      "Found something better\n",
      "0.0007969154405593315\n",
      "Found something better\n",
      "0.0021996321868895884\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8581 --- Loss: 0.5291180999279023 --- Change: 0.0021996321868895884 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001648165893554654\n",
      "Found something better\n",
      "0.000758025879859947\n",
      "Found something better\n",
      "0.0010388128757476743\n",
      "Found something better\n",
      "0.0012986025857925963\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0016754645824432533\n",
      "Found something better\n",
      "0.001931324658393907\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.002186171865463271\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0022479073095322153\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8585 --- Loss: 0.526078235244751 --- Change: 0.0022479073095322153 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001660342407226678\n",
      "Found something better\n",
      "0.00020075653076166987\n",
      "Found something better\n",
      "0.0007509319067000807\n",
      "Found something better\n",
      "0.0010572731113433263\n",
      "Found something better\n",
      "0.001224956507682795\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0012902332448958996\n",
      "Found something better\n",
      "0.0015952750492095656\n",
      "Found something better\n",
      "0.0018902382516860937\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.002199705386161821\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8588 --- Loss: 0.523064366197586 --- Change: 0.002199705386161821 --- New tol: -1e-05\n",
      "1.0681152073921396e-09\n",
      "Found something better\n",
      "0.0001660289001464754\n",
      "Found something better\n",
      "0.00020014556884765387\n",
      "Found something better\n",
      "0.0007483941316604458\n",
      "Found something better\n",
      "0.001057290992736759\n",
      "Found something better\n",
      "0.0012584125661849299\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0015454616117477225\n",
      "Found something better\n",
      "0.0018933852100372016\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0019265785932540244\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8586 --- Loss: 0.5202264081001282 --- Change: 0.0019265785932540244 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00017725158691405117\n",
      "Found something better\n",
      "0.000212702331542991\n",
      "Found something better\n",
      "0.0007590260839462459\n",
      "Found something better\n",
      "0.0010563940525054937\n",
      "Found something better\n",
      "0.0012484104442597064\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00125532381534581\n",
      "Found something better\n",
      "0.0015576121997833647\n",
      "Found something better\n",
      "0.0018639647531509705\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8584 --- Loss: 0.517477872800827 --- Change: 0.0018639647531509705 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001590744018554302\n",
      "Found something better\n",
      "0.000212702331542991\n",
      "Found something better\n",
      "0.0007540113353728838\n",
      "Found something better\n",
      "0.001056472816467302\n",
      "Found something better\n",
      "0.0012453319263458162\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.001563743801116968\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8587 --- Loss: 0.5153725203037262 --- Change: 0.001563743801116968 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001590348815917464\n",
      "Found something better\n",
      "0.0002127044677734058\n",
      "Found something better\n",
      "0.00044266582965846174\n",
      "Found something better\n",
      "0.0004986951589584177\n",
      "Found something better\n",
      "0.0006179459190368663\n",
      "Found something better\n",
      "0.0006219426918029458\n",
      "Found something better\n",
      "0.0010534244155883886\n",
      "Found something better\n",
      "0.001235338573455791\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0013371213817596095\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0015770443725585336\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8587 --- Loss: 0.5131195997714997 --- Change: 0.0015770443725585336 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001591203308105915\n",
      "Found something better\n",
      "0.0002127044677734835\n",
      "Found something better\n",
      "0.0004543488740921586\n",
      "Found something better\n",
      "0.0004987026357651025\n",
      "Found something better\n",
      "0.0006179480552673588\n",
      "Found something better\n",
      "0.0006219426918030235\n",
      "Found something better\n",
      "0.001053426551818881\n",
      "Found something better\n",
      "0.0011987759208679782\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0013371235179901796\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.859 --- Loss: 0.5113380160808563 --- Change: 0.0013371235179901796 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001591577148437051\n",
      "Found something better\n",
      "0.00021269912719721338\n",
      "Found something better\n",
      "0.0005088438892364211\n",
      "Found something better\n",
      "0.0006177958822250029\n",
      "Found something better\n",
      "0.0007210932064055653\n",
      "Found something better\n",
      "0.0009763286876677934\n",
      "Found something better\n",
      "0.0011514042854308348\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0011862955522536798\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8587 --- Loss: 0.509514715385437 --- Change: 0.0011862955522536798 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015913314819339107\n",
      "Found something better\n",
      "0.00021269805908200599\n",
      "Found something better\n",
      "0.0005086782312392901\n",
      "Found something better\n",
      "0.0006178059959412007\n",
      "Found something better\n",
      "0.0007211936092377025\n",
      "Found something better\n",
      "0.000975109691619902\n",
      "Found something better\n",
      "0.001151909227371195\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8588 --- Loss: 0.5079119950294495 --- Change: 0.001151909227371195 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015911392211919172\n",
      "Found something better\n",
      "0.00021412719726563443\n",
      "Found something better\n",
      "0.0005125437402725307\n",
      "Found something better\n",
      "0.0006467669248581442\n",
      "Found something better\n",
      "0.0009966779041290284\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0010632702302933227\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0011130169296265091\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8591 --- Loss: 0.5064505380630493 --- Change: 0.0011130169296265091 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015900604248044736\n",
      "Found something better\n",
      "0.00024720727920528595\n",
      "Found something better\n",
      "0.0005125255823135389\n",
      "Found something better\n",
      "0.0006467701292037664\n",
      "Found something better\n",
      "0.0010021263599395702\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8595 --- Loss: 0.5051903604507446 --- Change: 0.0010021263599395702 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015904663085933855\n",
      "Found something better\n",
      "0.00024721582412717824\n",
      "Found something better\n",
      "0.0002523019027710238\n",
      "Found something better\n",
      "0.0005207201623916857\n",
      "Found something better\n",
      "0.0006340691709518564\n",
      "Found something better\n",
      "0.0006630939817428127\n",
      "Found something better\n",
      "0.000752959489822369\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0009587743949890369\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.000978926544189418\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0009834184217452967\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8593 --- Loss: 0.5036997740268707 --- Change: 0.0009834184217452967 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015904769897462367\n",
      "Found something better\n",
      "0.0002471947383880457\n",
      "Found something better\n",
      "0.0004816484069824267\n",
      "Found something better\n",
      "0.0006991919183731098\n",
      "Found something better\n",
      "0.000788695406913764\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0009219700479507708\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0009572982263564956\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0009919148254394217\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8593 --- Loss: 0.5022827528476715 --- Change: 0.0009919148254394217 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015904556274413115\n",
      "Found something better\n",
      "0.00024719473838812344\n",
      "Found something better\n",
      "0.0004817285156250017\n",
      "Found something better\n",
      "0.0006992073726654224\n",
      "Found something better\n",
      "0.0008571225881576438\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0008979950141906667\n",
      "Found something better\n",
      "0.000905836353302003\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0009630629777908406\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8595 --- Loss: 0.5009926515579224 --- Change: 0.0009630629777908406 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015890991210937289\n",
      "Found something better\n",
      "0.0002473568153381289\n",
      "Found something better\n",
      "0.0004900865173339985\n",
      "Found something better\n",
      "0.0006249543905258669\n",
      "Found something better\n",
      "0.000886139574050937\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0009031177854538052\n",
      "Found something better\n",
      "0.0009065424442291302\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8595 --- Loss: 0.49969759092330934 --- Change: 0.0009065424442291302 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015882553100585173\n",
      "Found something better\n",
      "0.0002473557472229215\n",
      "Found something better\n",
      "0.0004900929260253983\n",
      "Found something better\n",
      "0.0006249479818344283\n",
      "Found something better\n",
      "0.0010119555234909038\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8599 --- Loss: 0.49842337164878847 --- Change: 0.0010119555234909038 --- New tol: -1e-05\n",
      "1.0681152462499454e-09\n",
      "Found something better\n",
      "0.00015943969726564533\n",
      "Found something better\n",
      "0.00020286584377289806\n",
      "Found something better\n",
      "0.0002473568153381289\n",
      "Found something better\n",
      "0.0004898162841796893\n",
      "Found something better\n",
      "0.0006249506187439025\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0010058211326599395\n",
      "Found something better\n",
      "0.0015290636682510459\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8596 --- Loss: 0.4961104277610779 --- Change: 0.0015290636682510459 --- New tol: -1e-05\n",
      "4.788892745971052e-05\n",
      "Found something better\n",
      "0.00015861083984376466\n",
      "Found something better\n",
      "0.00023180398941040069\n",
      "Found something better\n",
      "0.0002887895011901953\n",
      "Found something better\n",
      "0.0004423609924316618\n",
      "Found something better\n",
      "0.0006956871175765877\n",
      "Found something better\n",
      "0.0012224960756301805\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8604 --- Loss: 0.49470686774253847 --- Change: 0.0012224960756301805 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015897506713869467\n",
      "Found something better\n",
      "0.00021314910411835417\n",
      "Found something better\n",
      "0.0002690846920013645\n",
      "Found something better\n",
      "0.000698779711723324\n",
      "Found something better\n",
      "0.000854660463333129\n",
      "Found something better\n",
      "0.0015838525867462427\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00288306291103364\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8605 --- Loss: 0.49063104515075684 --- Change: 0.00288306291103364 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015895904541015637\n",
      "Found something better\n",
      "0.00021314319610597486\n",
      "Found something better\n",
      "0.00022901300430299698\n",
      "Found something better\n",
      "0.0005146862363815397\n",
      "Found something better\n",
      "0.000696535701751727\n",
      "Found something better\n",
      "0.0009660093641281063\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0010239136743545483\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8603 --- Loss: 0.4890826083660126 --- Change: 0.0010239136743545483 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001591416931152445\n",
      "Found something better\n",
      "0.0002131426954269544\n",
      "Found something better\n",
      "0.00022878763198852514\n",
      "Found something better\n",
      "0.00044385615348816463\n",
      "Found something better\n",
      "0.0006965287923813068\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00087840866565706\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0009698646545410305\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8603 --- Loss: 0.48769708743095397 --- Change: 0.0009698646545410305 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001630200195312459\n",
      "Found something better\n",
      "0.00041018506050110036\n",
      "Found something better\n",
      "0.00041917217731473607\n",
      "Found something better\n",
      "0.0007297802877426207\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0007877523326873525\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8584 --- Loss: 0.4857574338436127 --- Change: 0.0007877523326873525 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00016093612670901034\n",
      "Found something better\n",
      "0.00022166578769685638\n",
      "Found something better\n",
      "0.0003628605222702308\n",
      "Found something better\n",
      "0.0006763394641876419\n",
      "Found something better\n",
      "0.000777278742790255\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0007825855731964147\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0008501093482971234\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8592 --- Loss: 0.4848858548641205 --- Change: 0.0008501093482971234 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001610301208496312\n",
      "Found something better\n",
      "0.0006252915000915737\n",
      "Found something better\n",
      "0.0006749370288848976\n",
      "Found something better\n",
      "0.0007534706163406679\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0007571631908416698\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8595 --- Loss: 0.48393276038169863 --- Change: 0.0007571631908416698 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00016136016845703092\n",
      "Found something better\n",
      "0.0001772587966919148\n",
      "Found something better\n",
      "0.00018279083251955262\n",
      "Found something better\n",
      "0.0006744720363617063\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0006829482793808349\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0007266957902908543\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0007394619703292915\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8596 --- Loss: 0.48291925039291383 --- Change: 0.0007394619703292915 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00015772644042970073\n",
      "Found something better\n",
      "0.00017725138664247407\n",
      "Found something better\n",
      "0.0001828399658203361\n",
      "Found something better\n",
      "0.0006438015174865863\n",
      "Found something better\n",
      "0.0007040807199478248\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0007106516790390204\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0007270119524002082\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8597 --- Loss: 0.4819235261440277 --- Change: 0.0007270119524002082 --- New tol: -1e-05\n",
      "1.0681152462499454e-09\n",
      "Found something better\n",
      "0.0001577413940429928\n",
      "Found something better\n",
      "0.00017725943088530704\n",
      "Found something better\n",
      "0.00018284317016603602\n",
      "Found something better\n",
      "0.000643807926177986\n",
      "Found something better\n",
      "0.0006997412443160932\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0007023374700546535\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.86 --- Loss: 0.481048754119873 --- Change: 0.0007023374700546535 --- New tol: -1e-05\n",
      "7.4768066071762e-09\n",
      "Found something better\n",
      "0.00015952087402342773\n",
      "Found something better\n",
      "0.00017725452423092978\n",
      "Found something better\n",
      "0.00018283035278319758\n",
      "Found something better\n",
      "0.0007513350725173617\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.861 --- Loss: 0.4804039842128754 --- Change: 0.0007513350725173617 --- New tol: -1e-05\n",
      "7.476806646034007e-09\n",
      "Found something better\n",
      "0.00015890457153321935\n",
      "Found something better\n",
      "0.00017725836277009965\n",
      "Found something better\n",
      "0.00018284851074218953\n",
      "Found something better\n",
      "0.00028213411331178825\n",
      "Found something better\n",
      "0.00046868216991427514\n",
      "Found something better\n",
      "0.0006383991575241221\n",
      "Found something better\n",
      "0.0007274571895599656\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8614 --- Loss: 0.47953619112968443 --- Change: 0.0007274571895599656 --- New tol: -1e-05\n",
      "6.408691399784061e-09\n",
      "Found something better\n",
      "0.0001597323608398149\n",
      "Found something better\n",
      "0.00020784377574917798\n",
      "Found something better\n",
      "0.00030358214378357613\n",
      "Found something better\n",
      "0.0005445258665084828\n",
      "Found something better\n",
      "0.0006428222227096491\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0006445493507384936\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8615 --- Loss: 0.4786582705974579 --- Change: 0.0006445493507384936 --- New tol: -1e-05\n",
      "6.408691438641867e-09\n",
      "Found something better\n",
      "0.0001597216796875078\n",
      "Found something better\n",
      "0.00020784591197970935\n",
      "Found something better\n",
      "0.0003035800075531225\n",
      "Found something better\n",
      "0.0005445090532302965\n",
      "Found something better\n",
      "0.0006428022050857629\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8612 --- Loss: 0.4776113889694214 --- Change: 0.0006428022050857629 --- New tol: -1e-05\n",
      "7.476806646034007e-09\n",
      "Found something better\n",
      "0.00015668182373048677\n",
      "Found something better\n",
      "0.00020783786773683754\n",
      "Found something better\n",
      "0.0002823989391326997\n",
      "Found something better\n",
      "0.0005904638671875128\n",
      "Found something better\n",
      "0.0006166600084305085\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8612 --- Loss: 0.47673044610023496 --- Change: 0.0006166600084305085 --- New tol: -1e-05\n",
      "6.408691399784061e-09\n",
      "Found something better\n",
      "0.00015665298461911003\n",
      "Found something better\n",
      "0.0001944270467757969\n",
      "Found something better\n",
      "0.0002824118232727046\n",
      "Found something better\n",
      "0.0005904655027389459\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.000601302618980376\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8608 --- Loss: 0.4757000364303589 --- Change: 0.000601302618980376 --- New tol: -1e-05\n",
      "6.408691438641867e-09\n",
      "Found something better\n",
      "0.00015665191650390264\n",
      "Found something better\n",
      "0.00019442647933960998\n",
      "Found something better\n",
      "0.0002812323331833122\n",
      "Found something better\n",
      "0.0003803462362289511\n",
      "Found something better\n",
      "0.0005922329425812001\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0005952925968170153\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8608 --- Loss: 0.47484961843490603 --- Change: 0.0005952925968170153 --- New tol: -1e-05\n",
      "6.408691438641867e-09\n",
      "Found something better\n",
      "0.00015638381958008594\n",
      "Found something better\n",
      "0.00019442704677583576\n",
      "Found something better\n",
      "0.00028210227966310447\n",
      "Found something better\n",
      "0.00031984336376191804\n",
      "Found something better\n",
      "0.0006299040365219277\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8608 --- Loss: 0.473949755525589 --- Change: 0.0006299040365219277 --- New tol: -1e-05\n",
      "6.408691399784061e-09\n",
      "Found something better\n",
      "0.00015753631591796657\n",
      "Found something better\n",
      "0.00019443075180055613\n",
      "Found something better\n",
      "0.0003699633026122861\n",
      "Found something better\n",
      "0.00038999156951905476\n",
      "Found something better\n",
      "0.0005445316696167046\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0005863119840621989\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.000604252877235395\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8609 --- Loss: 0.4731293758392334 --- Change: 0.000604252877235395 --- New tol: -1e-05\n",
      "6.408691438641867e-09\n",
      "Found something better\n",
      "0.0001536077880859743\n",
      "Found something better\n",
      "0.00019441636562348983\n",
      "Found something better\n",
      "0.00037113852024081014\n",
      "Found something better\n",
      "0.00041076966762544355\n",
      "Found something better\n",
      "0.0005309182643890408\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0006674230384826895\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.861 --- Loss: 0.4722187786102295 --- Change: 0.0006674230384826895 --- New tol: -1e-05\n",
      "6.408691399784061e-09\n",
      "Found something better\n",
      "0.00015360992431638908\n",
      "Found something better\n",
      "0.0001944185018539435\n",
      "Found something better\n",
      "0.0003714835214614631\n",
      "Found something better\n",
      "0.00041609529018402645\n",
      "Found something better\n",
      "0.0005308961105346621\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.000653305425643924\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8611 --- Loss: 0.47132834939956664 --- Change: 0.000653305425643924 --- New tol: -1e-05\n",
      "6.408691399784061e-09\n",
      "Found something better\n",
      "0.0001347822570800683\n",
      "Found something better\n",
      "0.0001944185018539435\n",
      "Found something better\n",
      "0.0003681475734710726\n",
      "Found something better\n",
      "0.000416093153953534\n",
      "Found something better\n",
      "0.0005309009504317952\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.861 --- Loss: 0.4705270552158356 --- Change: 0.0005309009504317952 --- New tol: -1e-05\n",
      "6.408691438641867e-09\n",
      "Found something better\n",
      "0.0001347811889648609\n",
      "Found something better\n",
      "0.00019442170619968224\n",
      "Found something better\n",
      "0.00036816004753114636\n",
      "Found something better\n",
      "0.00041374593734744833\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00046488146781922254\n",
      "Found something better\n",
      "0.000483337383270277\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0005096422910690456\n",
      "Found something better\n",
      "0.0005161346435546887\n",
      "Found something better\n",
      "0.0006520164394378691\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8612 --- Loss: 0.46968130612373354 --- Change: 0.0006520164394378691 --- New tol: -1e-05\n",
      "2.136230492499891e-09\n",
      "Found something better\n",
      "0.00014455657958985512\n",
      "Found something better\n",
      "0.00019441900253296395\n",
      "Found something better\n",
      "0.0003162102985382331\n",
      "Found something better\n",
      "0.0003681707286834923\n",
      "Found something better\n",
      "0.00041375020980835564\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004559934043884328\n",
      "Found something better\n",
      "0.0005545423650741898\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8612 --- Loss: 0.46888910274505613 --- Change: 0.0005545423650741898 --- New tol: -1e-05\n",
      "5.340576153534115e-09\n",
      "Found something better\n",
      "0.00014455444335936262\n",
      "Found something better\n",
      "0.00019442327499387123\n",
      "Found something better\n",
      "0.0002488349723815675\n",
      "Found something better\n",
      "0.0003683320140838497\n",
      "Found something better\n",
      "0.00041281454086301573\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004515959739685049\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0005096444272994993\n",
      "Found something better\n",
      "0.0005161479616165088\n",
      "Found something better\n",
      "0.000526498036384565\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8614 --- Loss: 0.4682226912021637 --- Change: 0.000526498036384565 --- New tol: -1e-05\n",
      "5.340576192391921e-09\n",
      "Found something better\n",
      "0.00014455337524415523\n",
      "Found something better\n",
      "0.00019442384243013587\n",
      "Found something better\n",
      "0.000248993897438049\n",
      "Found something better\n",
      "0.0003683600616455007\n",
      "Found something better\n",
      "0.00042020376205442744\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004548742961883567\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.000509643359184253\n",
      "Found something better\n",
      "0.00051617202758788\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0005227252435684226\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8612 --- Loss: 0.4673902123451233 --- Change: 0.0005227252435684226 --- New tol: -1e-05\n",
      "5.340576153534115e-09\n",
      "Found something better\n",
      "0.00014548690795900264\n",
      "Found something better\n",
      "0.00019459901332853912\n",
      "Found something better\n",
      "0.00024847828865052764\n",
      "Found something better\n",
      "0.00036817720413209737\n",
      "Found something better\n",
      "0.0004202128744125455\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0006076356220245382\n",
      "Found something better\n",
      "0.0006438150835037127\n",
      "Found something better\n",
      "0.000678213424682611\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0007019684886932331\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0007439614439010422\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8615 --- Loss: 0.4664560030460358 --- Change: 0.0007439614439010422 --- New tol: -1e-05\n",
      "4.272460946141976e-09\n",
      "Found something better\n",
      "0.00014545166015626497\n",
      "Found something better\n",
      "0.00019459367275238558\n",
      "Found something better\n",
      "0.00027826245784760004\n",
      "Found something better\n",
      "0.0003684573268890656\n",
      "Found something better\n",
      "0.00042204283237460834\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0005859467506408977\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8619 --- Loss: 0.46579034218788146 --- Change: 0.0005859467506408977 --- New tol: -1e-05\n",
      "4.272460946141976e-09\n",
      "Found something better\n",
      "0.00014404708862302828\n",
      "Found something better\n",
      "0.00019457017421720124\n",
      "Found something better\n",
      "0.00024886976718901385\n",
      "Found something better\n",
      "0.000365235114097584\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00039356840133665804\n",
      "Found something better\n",
      "0.0004089394521712974\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00043759394645688233\n",
      "Found something better\n",
      "0.0005174689388275034\n",
      "Found something better\n",
      "0.0005445251464843703\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8619 --- Loss: 0.4650124491214752 --- Change: 0.0005445251464843703 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "0.0001455125427246018\n",
      "Found something better\n",
      "0.00019456590175629394\n",
      "Found something better\n",
      "0.000358147101402273\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004024094295501657\n",
      "Found something better\n",
      "0.0004088604116439687\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00043759287834167494\n",
      "Found something better\n",
      "0.0004918886470794637\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4643526153087616 --- Change: 0.0004918886470794637 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.0001455125427246018\n",
      "Found something better\n",
      "0.0001945680379867476\n",
      "Found something better\n",
      "0.0003581470346450677\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004048578262328905\n",
      "Found something better\n",
      "0.0004087909841537396\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00043759444713594163\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004684427833557158\n",
      "Found something better\n",
      "0.0004865217161178769\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.46365758428573606 --- Change: 0.0004865217161178769 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00014551467895505542\n",
      "Found something better\n",
      "0.0001946481466293226\n",
      "Found something better\n",
      "0.0003581523752212212\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00042239577770230565\n",
      "Found something better\n",
      "0.0004285217094421367\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00047394404411313926\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00048703629970547327\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8618 --- Loss: 0.4628761151790619 --- Change: 0.00048703629970547327 --- New tol: -1e-05\n",
      "-2.136230453642085e-09\n",
      "Found something better\n",
      "0.00014488662719729372\n",
      "Found something better\n",
      "0.00015644266605379653\n",
      "Found something better\n",
      "0.00018543975830079826\n",
      "Found something better\n",
      "0.000358125948905974\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00039579805850984974\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.000457869410514844\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00046842490196228324\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004726216125488514\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8617 --- Loss: 0.4621580771923065 --- Change: 0.0004726216125488514 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00014478622436523423\n",
      "Found something better\n",
      "0.00015644166469571674\n",
      "Found something better\n",
      "0.00018523788452147192\n",
      "Found something better\n",
      "0.0003627526617050014\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00039526136398315703\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004411699962615756\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004684427833557158\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8618 --- Loss: 0.4615317374706268 --- Change: 0.0004684427833557158 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00014478836059568788\n",
      "Found something better\n",
      "0.00015644380092620923\n",
      "Found something better\n",
      "0.00018588729858396433\n",
      "Found something better\n",
      "0.0003542974615096838\n",
      "Found something better\n",
      "0.00035813128948208867\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003952592277526645\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004411721324920681\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8622 --- Loss: 0.46107292304039 --- Change: 0.0004411721324920681 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00014182113647459582\n",
      "Found something better\n",
      "0.00014606008529663428\n",
      "Found something better\n",
      "0.00018588729858396433\n",
      "Found something better\n",
      "0.0003542931890487377\n",
      "Found something better\n",
      "0.0003905667829513515\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8622 --- Loss: 0.46051497049331663 --- Change: 0.0003905667829513515 --- New tol: -1e-05\n",
      "1.0681152073921396e-09\n",
      "Found something better\n",
      "0.00010168884277341194\n",
      "Found something better\n",
      "0.00014341706752776795\n",
      "Found something better\n",
      "0.00018540771484372163\n",
      "Found something better\n",
      "0.0003555125427245786\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0003632904529571546\n",
      "Found something better\n",
      "0.00038550455093382395\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.00038607902526853195\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4598777005195618 --- Change: 0.00038607902526853195 --- New tol: -1e-05\n",
      "1.0681152462499454e-09\n",
      "Found something better\n",
      "0.00010168670654299715\n",
      "Found something better\n",
      "0.00014342184066773456\n",
      "Found something better\n",
      "0.00018541198730470665\n",
      "Found something better\n",
      "0.00034553740024568234\n",
      "Found something better\n",
      "0.00034852134704591026\n",
      "Found something better\n",
      "0.0003730339050293074\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4593447949409485 --- Change: 0.0003730339050293074 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00010177749633787924\n",
      "Found something better\n",
      "0.0001326531028747635\n",
      "Found something better\n",
      "0.00018699493408204647\n",
      "Found something better\n",
      "0.00035507780551910506\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0003644154024124046\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8621 --- Loss: 0.4588670657634735 --- Change: 0.0003644154024124046 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.907623291015022e-05\n",
      "Found something better\n",
      "0.00013536775112151788\n",
      "Found something better\n",
      "0.00018743392944335912\n",
      "Found something better\n",
      "0.00035374886989595895\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0003602175378799477\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4583096050262451 --- Change: 0.0003602175378799477 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "9.906768798829679e-05\n",
      "Found something better\n",
      "0.00012877050399781598\n",
      "Found something better\n",
      "0.00018743179321290546\n",
      "Found something better\n",
      "0.00035372694015504134\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8624 --- Loss: 0.4579757122993469 --- Change: 0.00035372694015504134 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "9.906341552735064e-05\n",
      "Found something better\n",
      "0.0001287758445739695\n",
      "Found something better\n",
      "0.00018762619018554693\n",
      "Found something better\n",
      "0.0002880043220520101\n",
      "Found something better\n",
      "0.00034948855876922865\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00035207942008971257\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0003873733663558865\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8623 --- Loss: 0.4573794575214386 --- Change: 0.0003873733663558865 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.913177490233349e-05\n",
      "Found something better\n",
      "0.00012880895614625354\n",
      "Found something better\n",
      "0.00018759307861330175\n",
      "Found something better\n",
      "0.00028882406711577265\n",
      "Found something better\n",
      "0.0003494858551025492\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0003709059190749986\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4567210247039795 --- Change: 0.0003709059190749986 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.912750244142621e-05\n",
      "Found something better\n",
      "0.0001288004112244001\n",
      "Found something better\n",
      "0.0001875952148437554\n",
      "Found something better\n",
      "0.00025421280860903896\n",
      "Found something better\n",
      "0.00034943408489229163\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0003687513494491867\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8621 --- Loss: 0.456237101316452 --- Change: 0.0003687513494491867 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "0.00012699996948242753\n",
      "Found something better\n",
      "0.00012879884243009452\n",
      "Found something better\n",
      "0.00017977874755858436\n",
      "Found something better\n",
      "0.00025421501159665904\n",
      "Found something better\n",
      "0.00034954787254333757\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8622 --- Loss: 0.45578061146736143 --- Change: 0.00034954787254333757 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "0.00012699890136718127\n",
      "Found something better\n",
      "0.00012879563808439464\n",
      "Found something better\n",
      "0.00017977661132809186\n",
      "Found something better\n",
      "0.0002749422216415276\n",
      "Found something better\n",
      "0.0002794986200332583\n",
      "Found something better\n",
      "0.00029723944187163285\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003433629989623765\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8622 --- Loss: 0.4552900928974152 --- Change: 0.0003433629989623765 --- New tol: -1e-05\n",
      "-1.0681152073921396e-09\n",
      "Found something better\n",
      "0.0001275148010254079\n",
      "Found something better\n",
      "0.00012879877567292808\n",
      "Found something better\n",
      "0.0001797766113281307\n",
      "Found something better\n",
      "0.0002749486303329662\n",
      "Found something better\n",
      "0.00027950396060945066\n",
      "Found something better\n",
      "0.0002977094125747759\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003036822509765613\n",
      "Found something better\n",
      "0.0003328567504882862\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8622 --- Loss: 0.4548145832538605 --- Change: 0.0003328567504882862 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "0.00012751266479491541\n",
      "Found something better\n",
      "0.00012879877567292808\n",
      "Found something better\n",
      "0.0001797766113281307\n",
      "Found something better\n",
      "0.000275198569297791\n",
      "Found something better\n",
      "0.00029770514011382977\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00030362991333007794\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003142544555663984\n",
      "Found something better\n",
      "0.00033248329162597343\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8624 --- Loss: 0.454425310087204 --- Change: 0.00033248329162597343 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00011500076293945448\n",
      "Found something better\n",
      "0.00012997804164885938\n",
      "Found something better\n",
      "0.00018032989501954864\n",
      "Found something better\n",
      "0.000275111618042001\n",
      "Found something better\n",
      "0.0003088485765457227\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0003142544555663984\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003335490036010968\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4537773800373077 --- Change: 0.0003335490036010968 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.834777832029017e-05\n",
      "Found something better\n",
      "0.0002071520042419117\n",
      "Found something better\n",
      "0.0002751692295074326\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003036288452148317\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003142576599120983\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4533284405231476 --- Change: 0.0003142576599120983 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.834777832032903e-05\n",
      "Found something better\n",
      "0.0002071509361267043\n",
      "Found something better\n",
      "0.00027397507667541453\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0003036277770996243\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.862 --- Loss: 0.4528946865558624 --- Change: 0.0003036277770996243 --- New tol: -1e-05\n",
      "-2.136230492499891e-09\n",
      "Found something better\n",
      "9.834884643553642e-05\n",
      "Found something better\n",
      "0.0002071509361267043\n",
      "Found something better\n",
      "0.00027395157814023017\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0002794786930084225\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00028995023250578673\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8622 --- Loss: 0.4525662004470825 --- Change: 0.00028995023250578673 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.835311889648257e-05\n",
      "Found something better\n",
      "0.000208286910057065\n",
      "Found something better\n",
      "0.0002115198230743298\n",
      "Found something better\n",
      "0.00024665464401244506\n",
      "Found something better\n",
      "0.0002484628295898461\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0004675433254242023\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8621 --- Loss: 0.45185541715621946 --- Change: 0.0004675433254242023 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.836273193358224e-05\n",
      "Found something better\n",
      "0.000202145247459401\n",
      "Found something better\n",
      "0.0002458663749694556\n",
      "Found something better\n",
      "0.00027085693359372163\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8621 --- Loss: 0.451468478679657 --- Change: 0.00027085693359372163 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.659286499024699e-05\n",
      "Found something better\n",
      "0.00018914521694184393\n",
      "Found something better\n",
      "0.0002458332633972493\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0002587384128570791\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8623 --- Loss: 0.4511845553398132 --- Change: 0.0002587384128570791 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "9.659179687500074e-05\n",
      "Found something better\n",
      "0.0002015438985824458\n",
      "Found something better\n",
      "0.00024583326339721044\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0002459078979492213\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8623 --- Loss: 0.4508332583427429 --- Change: 0.0002459078979492213 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "9.65907287597545e-05\n",
      "Found something better\n",
      "0.00020389588832853687\n",
      "Found something better\n",
      "0.00024584714889525625\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8623 --- Loss: 0.4504820481300354 --- Change: 0.00024584714889525625 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "9.65907287597545e-05\n",
      "Found something better\n",
      "0.00020389695644378313\n",
      "Found something better\n",
      "0.00021215300559996873\n",
      "Found something better\n",
      "0.00023750707626342128\n",
      "Found something better\n",
      "0.00023845886230468015\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0002469266700744432\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8626 --- Loss: 0.45025788850784304 --- Change: 0.0002469266700744432 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00011366989135743187\n",
      "Found something better\n",
      "0.00021916115760804987\n",
      "Found something better\n",
      "0.00022469936370851927\n",
      "Found something better\n",
      "0.00024220472335818586\n",
      "Found something better\n",
      "0.00024430687427524076\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0002551240921020492\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00029167247772217685\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8628 --- Loss: 0.44992691650390626 --- Change: 0.00029167247772217685 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "0.00011367950439453155\n",
      "Found something better\n",
      "0.00013069244384767973\n",
      "Found something better\n",
      "0.0001792372131347586\n",
      "Found something better\n",
      "0.0002633145904541278\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8629 --- Loss: 0.44959361705780027 --- Change: 0.0002633145904541278 --- New tol: -1e-05\n",
      "0.0\n",
      "Found something better\n",
      "0.00011367950439449269\n",
      "Found something better\n",
      "0.0001306977176665891\n",
      "Found something better\n",
      "0.000179238281249966\n",
      "Found something better\n",
      "0.0001817301559448092\n",
      "Found something better\n",
      "0.00021436371326444267\n",
      "Found something better\n",
      "0.00024219210624692455\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8631 --- Loss: 0.44933333129882813 --- Change: 0.00024219210624692455 --- New tol: -1e-05\n",
      "-2.136230453642085e-09\n",
      "Found something better\n",
      "0.00011368057250977781\n",
      "Found something better\n",
      "0.0001306955814361743\n",
      "Found something better\n",
      "0.0001792372131347586\n",
      "Found something better\n",
      "0.00018179526805877088\n",
      "Found something better\n",
      "0.0001896520376205357\n",
      "Found something better\n",
      "0.00019375273227692502\n",
      "Found something better\n",
      "0.00019940925121306183\n",
      "Found something better\n",
      "0.00023792907714844657\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8631 --- Loss: 0.4489934326171875 --- Change: 0.00023792907714844657 --- New tol: -1e-05\n",
      "-2.136230492499891e-09\n",
      "Found something better\n",
      "0.00011368057250977781\n",
      "Found something better\n",
      "0.00013069558143613547\n",
      "Found something better\n",
      "0.00017866683959959494\n",
      "Found something better\n",
      "0.00018179526805877088\n",
      "Found something better\n",
      "0.0001896520376205357\n",
      "Found something better\n",
      "0.00019375273227692502\n",
      "Found something better\n",
      "0.00019940925121306183\n",
      "Found something better\n",
      "0.00023646268844603746\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8631 --- Loss: 0.4486556287765503 --- Change: 0.00023646268844603746 --- New tol: -1e-05\n",
      "-1.0681152462499454e-09\n",
      "Found something better\n",
      "0.00011412811279298273\n",
      "Found something better\n",
      "0.00013120350360869558\n",
      "Found something better\n",
      "0.0001786700439453337\n",
      "Found something better\n",
      "0.00021661715507507727\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.00022803565979004345\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00028296514034270576\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8632 --- Loss: 0.44829425711631776 --- Change: 0.00028296514034270576 --- New tol: -1e-05\n",
      "-2.136230453642085e-09\n",
      "Found something better\n",
      "0.00011301406860353968\n",
      "Found something better\n",
      "0.00012960485458373693\n",
      "Found something better\n",
      "0.0001786700439453337\n",
      "Found something better\n",
      "0.00019503445148468798\n",
      "Found something better\n",
      "0.0002292251062393391\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8638 --- Loss: 0.448223927116394 --- Change: 0.0002292251062393391 --- New tol: -1e-05\n",
      "-2.136230492499891e-09\n",
      "Found something better\n",
      "0.0001134829711913976\n",
      "Found something better\n",
      "0.00012688059329984402\n",
      "Found something better\n",
      "0.00017866683959959494\n",
      "Found something better\n",
      "0.0001939098119735416\n",
      "Found something better\n",
      "0.00019947811126706515\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00021574645996091932\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00036054975509642315\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8634 --- Loss: 0.4475374245643616 --- Change: 0.00036054975509642315 --- New tol: -1e-05\n",
      "-2.136230453642085e-09\n",
      "Found something better\n",
      "0.00011348190307619021\n",
      "Found something better\n",
      "0.0001268817281722956\n",
      "Found something better\n",
      "0.0001786614990234414\n",
      "Found something better\n",
      "0.00022033738613127518\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0002250230407714715\n",
      "Found something better\n",
      "Did 20 iterations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-275-a1822c5e48a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mtester_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mnl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtester_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moa\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mol\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_change\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mbest_change\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moa\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mol\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     dataset = dataset.map(\n\u001b[1;32m--> 390\u001b[1;33m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[1;32m-> 1591\u001b[1;33m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3926\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[0;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    981\u001b[0m       \u001b[1;31m# TensorArrays and `None`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m       func_outputs = nest.map_structure(convert, func_outputs,\n\u001b[1;32m--> 983\u001b[1;33m                                         expand_composites=True)\n\u001b[0m\u001b[0;32m    984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m       \u001b[0mcheck_mutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_args_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    943\u001b[0m               (str(python_func), type(x)))\n\u001b[0;32m    944\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeps_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36mmark_as_return\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;31m# of a new identity operation that the stateful operations definitely don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;31m# depend on.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_returned_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3827\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3828\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m-> 3829\u001b[1;33m         \"Identity\", input=input, name=name)\n\u001b[0m\u001b[0;32m   3830\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3831\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3322\u001b[0m           op_def=op_def)\n\u001b[1;32m-> 3323\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[1;34m(self, op, compute_device)\u001b[0m\n\u001b[0;32m   3376\u001b[0m     \u001b[1;31m# Apply a kernel label if one has been specified for this op type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3377\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3378\u001b[1;33m       \u001b[0mkernel_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op_to_kernel_label_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3379\u001b[0m       op._set_attr(\"_kernel\",  # pylint: disable=protected-access\n\u001b[0;32m   3380\u001b[0m                    attr_value_pb2.AttrValue(s=compat.as_bytes(kernel_label)))\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\master\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2218\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2219\u001b[0m     \u001b[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2220\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2222\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, acc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "end_not_reached = True\n",
    "improved = False\n",
    "tol = -1e-5\n",
    "current_pos = 0\n",
    "best_pos = -1\n",
    "best_change = tol\n",
    "original2 = model2.get_weights()\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights2 = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "while end_not_reached or improved:\n",
    "    if not(end_not_reached):\n",
    "        end_not_reached = True\n",
    "        improved = False\n",
    "        current_pos = 0\n",
    "        size -= 1\n",
    "        nodes_removed2 += [best_pos]\n",
    "        best_weights2[0][:,best_pos] = 0\n",
    "        best_weights2[1][best_pos] = 0\n",
    "        best_weights2[2][best_pos,:] = 0\n",
    "        best_pos = -1\n",
    "        #tol -= best_change\n",
    "        ol = best_loss\n",
    "        oa = best_acc\n",
    "        bas2 += [best_acc]\n",
    "        bls2 += [best_loss]\n",
    "        print(\"Improvement has occured!! Accuracy:\", best_acc, \"--- Loss:\", best_loss, '--- Change:', best_change, '--- New tol:', tol)\n",
    "        best_change = tol\n",
    "        num_removed2 += 1\n",
    "    if current_pos in nodes_removed2:\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            end_not_reached = False\n",
    "        continue\n",
    "    w = copy.deepcopy(best_weights2)\n",
    "    w[0][:,current_pos] = 0\n",
    "    w[1][current_pos] = 0\n",
    "    w[2][current_pos,:] = 0\n",
    "    tester_model2.set_weights(w)\n",
    "    nl, na = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=512)\n",
    "    if 0.3*(na - oa) + 0.7*(ol - nl) > best_change:\n",
    "        best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "        print(best_change)\n",
    "        best_pos = current_pos\n",
    "        improved = True\n",
    "        best_acc = na\n",
    "        best_loss = nl\n",
    "        print(\"Found something better\")\n",
    "    current_pos += 1\n",
    "    if current_pos - num_removed2 >= size:\n",
    "        end_not_reached = False\n",
    "    if current_pos%200 == 0:\n",
    "        print(\"Did 200 iterations\")\n",
    "\n",
    "tester_model2.set_weights(best_weights2)\n",
    "loss2, acc2 = tester_model2.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_removed2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.get_weights()[0][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = model.get_weights()\n",
    "old[0][:,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "old[1][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "old[2][0,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(old[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.0739 - accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0738823753261473, 0.9776]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 170us/sample - loss: 0.0671 - accuracy: 0.9793\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0588 - accuracy: 0.9810\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0539 - accuracy: 0.9820\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0495 - accuracy: 0.9834\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0446 - accuracy: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a95d055888>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 0s - loss: 0.0727 - accuracy: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07267645624614087, 0.9798]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('master': conda)",
   "language": "python",
   "name": "python37664bitmastercondad556aba890334ca2b025f74f5b164268"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}