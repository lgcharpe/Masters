{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Filters from Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import tqdm\n",
    "import IProgress\n",
    "from hfunc import models\n",
    "from hfunc import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10\n",
    "class_accuracy = metrics.ClassAccuracy()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "y_train, y_test = tf.one_hot(y_train.flatten(), 10), tf.one_hot(y_test.flatten(), 10)\n",
    "y_test_flat = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "    \n",
    "tester_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7206 - accuracy: 0.3680 - auc_4: 0.8209\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3715 - accuracy: 0.4994 - auc_4: 0.8929\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.2415 - accuracy: 0.5440 - auc_4: 0.9128\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1366 - accuracy: 0.5842 - auc_4: 0.9271\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0497 - accuracy: 0.6189 - auc_4: 0.9378\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9677 - accuracy: 0.6463 - auc_4: 0.9471\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8923 - accuracy: 0.6777 - auc_4: 0.9548\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.8185 - accuracy: 0.7022 - auc_4: 0.96 - 9s 6ms/step - loss: 0.8186 - accuracy: 0.7021 - auc_4: 0.9618\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7567 - accuracy: 0.7261 - auc_4: 0.9673\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6943 - accuracy: 0.7482 - auc_4: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f55aa05248>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 1.3278 - accuracy: 0.5847 - auc_4: 0.9126\n",
      "Node 0: -0.0014998912811279297\n",
      "Node 1: 9.822845458984375e-05\n",
      "Node 2: 0.0001723766326904297\n",
      "Node 3: 0.0\n",
      "Node 4: -0.0011245012283325195\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0005205869674682617\n",
      "Node 7: 0.003957152366638184\n",
      "Node 8: 0.004172801971435547\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0009812116622924805\n",
      "Node 11: 0.0\n",
      "Node 12: 0.007231950759887695\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: -0.048014163970947266\n",
      "Node 16: 0.0\n",
      "Node 17: 2.7298927307128906e-05\n",
      "Node 18: -0.02084052562713623\n",
      "Node 19: 0.0\n",
      "Node 20: -0.026913046836853027\n",
      "Node 21: 0.00026988983154296875\n",
      "Node 22: -0.0019714832305908203\n",
      "Node 23: 0.0\n",
      "Node 24: 0.008685946464538574\n",
      "Node 25: -0.0003619194030761719\n",
      "Node 26: 6.556510925292969e-06\n",
      "Node 27: 0.016967415809631348\n",
      "Node 28: 1.1920928955078125e-07\n",
      "Node 29: -0.009758353233337402\n",
      "Node 30: 0.0011377334594726562\n",
      "Node 31: 0.0\n",
      "Node 32: 0.00048220157623291016\n",
      "Node 33: -1.5497207641601562e-06\n",
      "Node 34: 0.001094222068786621\n",
      "Node 35: 0.0026895999908447266\n",
      "Node 36: -0.0025289058685302734\n",
      "Node 37: -0.03654742240905762\n",
      "Node 38: 7.033348083496094e-05\n",
      "Node 39: 1.1920928955078125e-07\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: -0.004094362258911133\n",
      "Node 43: 0.0\n",
      "Node 44: 0.005756735801696777\n",
      "Node 45: -1.1920928955078125e-06\n",
      "Node 46: 4.220008850097656e-05\n",
      "Node 47: 0.0003744363784790039\n",
      "Node 48: -0.013466715812683105\n",
      "Node 49: 0.0023827552795410156\n",
      "Node 50: 0.008487224578857422\n",
      "Node 51: -0.006354331970214844\n",
      "Node 52: 0.007519841194152832\n",
      "Node 53: -0.07084643840789795\n",
      "Node 54: -1.1920928955078125e-06\n",
      "Node 55: 0.0\n",
      "Node 56: -0.0018187761306762695\n",
      "Node 57: 0.000579833984375\n",
      "Node 58: 0.0006616115570068359\n",
      "Node 59: -0.00035703182220458984\n",
      "Node 60: 0.004546165466308594\n",
      "Node 61: -0.0027458667755126953\n",
      "Node 62: -0.0012902021408081055\n",
      "Node 63: 0.00834512710571289\n",
      "Node 64: 0.0010380744934082031\n",
      "Node 65: 0.0\n",
      "Node 66: -0.00705111026763916\n",
      "Node 67: -0.010856270790100098\n",
      "Node 68: 0.005580306053161621\n",
      "Node 69: -0.0019620656967163086\n",
      "Node 70: 0.006988406181335449\n",
      "Node 71: -0.10985243320465088\n",
      "Node 72: -0.022123336791992188\n",
      "Node 73: -0.0063784122467041016\n",
      "Node 74: 0.0024548768997192383\n",
      "Node 75: -0.1535269021987915\n",
      "Node 76: -0.008829951286315918\n",
      "Node 77: 3.743171691894531e-05\n",
      "Node 78: 0.011450648307800293\n",
      "Node 79: 0.0001150369644165039\n",
      "Node 80: 0.004777789115905762\n",
      "Node 81: -2.384185791015625e-07\n",
      "Node 82: 0.0\n",
      "Node 83: -5.841255187988281e-06\n",
      "Node 84: 0.001359701156616211\n",
      "Node 85: -7.987022399902344e-06\n",
      "Node 86: 0.0025240182876586914\n",
      "Node 87: -3.5762786865234375e-07\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0002847909927368164\n",
      "Node 90: 0.0007767677307128906\n",
      "Node 91: -0.0039212703704833984\n",
      "Node 92: -3.5762786865234375e-07\n",
      "Node 93: -0.06218063831329346\n",
      "Node 94: -0.006334662437438965\n",
      "Node 95: -0.019425272941589355\n",
      "Node 96: 0.0\n",
      "Node 97: 0.000655055046081543\n",
      "Node 98: 0.0\n",
      "Node 99: -0.0035638809204101562\n",
      "Node 100: 0.0\n",
      "Node 101: -0.04783987998962402\n",
      "Node 102: -0.06660258769989014\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: -0.010961413383483887\n",
      "Node 106: 0.00620114803314209\n",
      "Node 107: -0.034975290298461914\n",
      "Node 108: -0.05547201633453369\n",
      "Node 109: 0.0\n",
      "Node 110: -0.03499937057495117\n",
      "Node 111: -0.08441221714019775\n",
      "Node 112: 0.0\n",
      "Node 113: -0.0004379749298095703\n",
      "Node 114: -0.003833174705505371\n",
      "Node 115: 0.002426624298095703\n",
      "Node 116: 0.001537919044494629\n",
      "Node 117: -0.0005725622177124023\n",
      "Node 118: 0.0006412267684936523\n",
      "Node 119: -0.0014963150024414062\n",
      "Node 120: 0.0\n",
      "Node 121: -0.038939476013183594\n",
      "Node 122: -0.02510058879852295\n",
      "Node 123: -0.009965300559997559\n",
      "Node 124: 0.0\n",
      "Node 125: 7.581710815429688e-05\n",
      "Node 126: 0.002235889434814453\n",
      "Node 127: 0.0\n",
      "Node 0: 0.0\n",
      "Node 1: 0.0\n",
      "Node 2: 0.0\n",
      "Node 3: 0.0\n",
      "Node 4: -1.4571449756622314\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: -2.6450798511505127\n",
      "Node 26: -0.28355729579925537\n",
      "Node 27: -1.3392333984375\n",
      "Node 28: 0.0\n",
      "Node 29: 0.0\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: -1.7498373985290527\n",
      "Node 38: 0.0004483461380004883\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: -1.1809196472167969\n",
      "Node 45: 0.0\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: 0.0\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0\n",
      "Node 59: 0.0\n",
      "Node 60: 0.0\n",
      "Node 61: 0.0\n",
      "Node 62: 0.0\n",
      "Node 63: 0.0\n"
     ]
    }
   ],
   "source": [
    "l, a, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights = model.get_weights()\n",
    "weight_len = len(or_weights) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [128]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "\n",
    "#For-loop over convolutional layers\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[conv_len - (2*layer+1)][:, :, :, i] = 0\n",
    "        w[conv_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))\n",
    "\n",
    "#For-loop over dense layers\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 57\n",
      "Worse Nodes: 1\n",
      "Important Nodes: 6\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 38\n",
      "Worse Nodes: 45\n",
      "Important Nodes: 45\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 1.3278 - accuracy: 0.5847 - auc_4: 0.9126\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.5846999883651733 --- Loss: 1.3277618885040283 --- Change: -8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5846999883651733 --- Loss: 1.3277618885040283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5846999883651733 --- Loss: 1.3272415399551392 --- Change: 0.0003642439842224121 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827000141143799 --- Loss: 1.3232766389846802 --- Change: 0.002175438404083252 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5828999876976013 --- Loss: 1.3231372833251953 --- Change: 0.00015754103660583496 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.3188047409057617 --- Change: 0.002942782640457153 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.3188047409057617 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5824000239372253 --- Loss: 1.3178369998931885 --- Change: 0.0006174266338348388 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5824000239372253 --- Loss: 1.3178369998931885 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.005999547243118286 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827000141143799 --- Loss: 1.3093243837356567 --- Change: 4.9281120300292966e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827000141143799 --- Loss: 1.3093243837356567 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827999711036682 --- Loss: 1.3092992305755615 --- Change: 4.7594308853149414e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827999711036682 --- Loss: 1.3092992305755615 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5842999815940857 --- Loss: 1.3062597513198853 --- Change: 0.0025776386260986323 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.583899974822998 --- Loss: 1.3047467470169067 --- Change: 0.000939100980758667 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.583899974822998 --- Loss: 1.3047391176223755 --- Change: 5.340576171875e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2851483821868896 --- Change: 0.014253515005111694 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2851481437683105 --- Change: 1.6689300537109374e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2851481437683105 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5861999988555908 --- Loss: 1.2843248844146729 --- Change: 0.0007262885570526122 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5861999988555908 --- Loss: 1.2843255996704102 --- Change: -5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840553522109985 --- Change: 9.179115295410156e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840396165847778 --- Change: 1.1014938354492186e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849000215530396 --- Loss: 1.2777714729309082 --- Change: 0.004177200794219971 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2777471542358398 --- Change: 0.0002570092678070068 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2777488231658936 --- Change: -1.1682510375976561e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5864999890327454 --- Loss: 1.2771539688110352 --- Change: 0.0006564021110534668 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2739813327789307 --- Change: 0.0019808411598205566 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867000222206116 --- Loss: 1.2724426984786987 --- Change: 0.0013770580291748045 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.26910400390625 --- Change: 0.002457088232040405 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2691041231155396 --- Change: -8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2691041231155396 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2688368558883667 --- Change: 0.00018708705902099608 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5878000259399414 --- Loss: 1.268312931060791 --- Change: 0.0005767464637756348 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867999792098999 --- Loss: 1.2667078971862793 --- Change: 0.0008235096931457519 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867999792098999 --- Loss: 1.2667078971862793 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5874000191688538 --- Loss: 1.2650362253189087 --- Change: 0.0013501822948455808 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2625014781951904 --- Change: 0.0016843259334564206 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5873000025749207 --- Loss: 1.2625792026519775 --- Change: 5.584955215454104e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5873000025749207 --- Loss: 1.2622408866882324 --- Change: 0.00023682117462158202 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5873000025749207 --- Loss: 1.2622110843658447 --- Change: 2.086162567138672e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867999792098999 --- Loss: 1.2590197324752808 --- Change: 0.0020839393138885496 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.258271336555481 --- Change: 7.389187812805169e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.2582714557647705 --- Change: -8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.2582714557647705 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.258270263671875 --- Change: 8.344650268554688e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2572377920150757 --- Change: 0.0007827222347259521 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2572380304336548 --- Change: -1.6689300537109374e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2572380304336548 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.257238507270813 --- Change: -3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.257238507270813 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.252395749092102 --- Change: 0.0035099327564239503 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: -1.5854835510253905e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5843999981880188 --- Loss: 1.2504163980484009 --- Change: 0.0009371280670166014 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5828999876976013 --- Loss: 1.2496907711029053 --- Change: 5.793571472167969e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5843999981880188 --- Loss: 1.2493844032287598 --- Change: 0.0006644606590270996 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5845999717712402 --- Loss: 1.248937726020813 --- Change: 0.00037266612052917476 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849999785423279 --- Loss: 1.2491222620010376 --- Change: -9.173154830932609e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849999785423279 --- Loss: 1.2491222620010376 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849999785423279 --- Loss: 1.2491222620010376 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0006455957889556884 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 4.072189331054687e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model.get_weights()\n",
    "tol = -1e-5\n",
    "layer_sizes = [64]\n",
    "conv_layer_sizes = [128]\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights4 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts3 = []\n",
    "places3 = []\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights4)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights4[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights4[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts3.append(num_removed2)\n",
    "    places3.append(nodes_removed2)\n",
    "\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights4)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights4[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights4[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts3.append(num_removed2)\n",
    "    places3.append(nodes_removed2)\n",
    "\n",
    "tester_model.set_weights(best_weights4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 1.2484 - accuracy: 0.5855 - auc_5: 0.9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2483556270599365, 0.5855000019073486, 0.9177575707435608]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.638, 0.642, 0.434, 0.404, 0.582, 0.413, 0.676, 0.621, 0.741, 0.696]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.643, 0.746, 0.381, 0.357, 0.59, 0.45, 0.709, 0.644, 0.689, 0.646]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76, 58]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4804 - accuracy: 0.4579 - auc_4: 0.8714 - val_loss: 1.1099 - val_accuracy: 0.6023 - val_auc_4: 0.9299\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9963 - accuracy: 0.6485 - auc_4: 0.9435 - val_loss: 0.9322 - val_accuracy: 0.6728 - val_auc_4: 0.9510\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7969 - accuracy: 0.7199 - auc_4: 0.9632 - val_loss: 0.9150 - val_accuracy: 0.6867 - val_auc_4: 0.9516\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6647 - accuracy: 0.7676 - auc_4: 0.9740 - val_loss: 0.8108 - val_accuracy: 0.7237 - val_auc_4: 0.9614\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5525 - accuracy: 0.8058 - auc_4: 0.9815 - val_loss: 0.7984 - val_accuracy: 0.7392 - val_auc_4: 0.9628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ed8c240548>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7984 - accuracy: 0.7392 - auc_4: 0.9628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7984009981155396, 0.7391999959945679, 0.9627801775932312]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 - 1s - loss: 0.4232 - accuracy: 0.8534 - auc_4: 0.9893\n",
      "Node 0: 0.0\n",
      "Node 1: 0.0\n",
      "Node 2: 0.0013521015644073486\n",
      "Node 3: -3.5762786865234375e-07\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: -0.07783263921737671\n",
      "Node 8: -1.8596649169921875e-05\n",
      "Node 9: 0.0\n",
      "Node 10: -1.4901161193847656e-07\n",
      "Node 11: -0.018270939588546753\n",
      "Node 12: 0.0\n",
      "Node 13: -0.0499434769153595\n",
      "Node 14: -0.14751791954040527\n",
      "Node 15: -0.2636755704879761\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: -0.18876487016677856\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: -7.331371307373047e-06\n",
      "Node 24: 0.0\n",
      "Node 25: -9.08970832824707e-06\n",
      "Node 26: 0.0\n",
      "Node 27: -0.06157222390174866\n",
      "Node 28: 0.0\n",
      "Node 29: -8.383393287658691e-05\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: -0.2191484570503235\n",
      "Node 35: 0.0\n",
      "Node 36: -0.0007567107677459717\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: -0.10855263471603394\n",
      "Node 44: -0.3271961808204651\n",
      "Node 45: -0.27273261547088623\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: -0.26343172788619995\n",
      "Node 51: -6.258487701416016e-07\n",
      "Node 52: 0.0\n",
      "Node 53: -0.06599682569503784\n",
      "Node 54: 0.0\n",
      "Node 55: -0.27704674005508423\n",
      "Node 56: 0.0\n",
      "Node 57: -0.000689089298248291\n",
      "Node 58: -0.00011795759201049805\n",
      "Node 59: -3.5315752029418945e-05\n",
      "Node 60: 0.0\n",
      "Node 61: 0.0\n",
      "Node 62: 0.0\n",
      "Node 63: 0.0\n",
      "Node 0: -0.0005040168762207031\n",
      "Node 1: -0.003500223159790039\n",
      "Node 2: 0.0\n",
      "Node 3: -1.1622905731201172e-06\n",
      "Node 4: -7.152557373046875e-06\n",
      "Node 5: 0.00060996413230896\n",
      "Node 6: -0.0011338591575622559\n",
      "Node 7: -0.002590268850326538\n",
      "Node 8: -0.0007088780403137207\n",
      "Node 9: -0.0018631219863891602\n",
      "Node 10: -1.7285346984863281e-06\n",
      "Node 11: -0.0014049112796783447\n",
      "Node 12: -0.002091020345687866\n",
      "Node 13: -0.0027763545513153076\n",
      "Node 14: -0.0030696988105773926\n",
      "Node 15: -0.001302182674407959\n",
      "Node 16: -0.0013506114482879639\n",
      "Node 17: -0.0011172890663146973\n",
      "Node 18: 0.0\n",
      "Node 19: -4.023313522338867e-06\n",
      "Node 20: 0.0\n",
      "Node 21: -0.0025820136070251465\n",
      "Node 22: -0.0005322694778442383\n",
      "Node 23: -0.0013357698917388916\n",
      "Node 24: 0.0016115307807922363\n",
      "Node 25: -0.0025205016136169434\n",
      "Node 26: -0.002890259027481079\n",
      "Node 27: 0.0\n",
      "Node 28: 5.3942203521728516e-05\n",
      "Node 29: -0.0039771199226379395\n",
      "Node 30: -0.0014171004295349121\n",
      "Node 31: 8.940696716308594e-08\n",
      "Node 32: 0.0\n",
      "Node 33: -0.0019071698188781738\n",
      "Node 34: -1.2814998626708984e-06\n",
      "Node 35: -2.9802322387695312e-08\n",
      "Node 36: -0.0014747679233551025\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0009432137012481689\n",
      "Node 39: -0.0015484392642974854\n",
      "Node 40: -0.0008994936943054199\n",
      "Node 41: -0.0028920769691467285\n",
      "Node 42: -0.0020131170749664307\n",
      "Node 43: 0.0\n",
      "Node 44: -0.0002880394458770752\n",
      "Node 45: -0.0019057393074035645\n",
      "Node 46: -0.0002918839454650879\n",
      "Node 47: 0.0\n",
      "Node 48: -0.0002536773681640625\n",
      "Node 49: -0.0009399652481079102\n",
      "Node 50: 0.0\n",
      "Node 51: -0.002204686403274536\n",
      "Node 52: -0.0005132853984832764\n",
      "Node 53: -8.940696716308594e-08\n",
      "Node 54: 0.0\n",
      "Node 55: -0.0009551644325256348\n",
      "Node 56: -0.0015131235122680664\n",
      "Node 57: -0.001622527837753296\n",
      "Node 58: -0.0009731650352478027\n",
      "Node 59: -0.0041713714599609375\n",
      "Node 60: -0.001848667860031128\n",
      "Node 61: -0.005128085613250732\n",
      "Node 62: -0.0014369189739227295\n",
      "Node 63: -0.0011883974075317383\n",
      "Node 64: -0.005432248115539551\n",
      "Node 65: 0.0\n",
      "Node 66: 0.0\n",
      "Node 67: -5.960464477539062e-07\n",
      "Node 68: -0.000760197639465332\n",
      "Node 69: 0.0\n",
      "Node 70: -0.0016381144523620605\n",
      "Node 71: -0.0033257901668548584\n",
      "Node 72: -0.0007368326187133789\n",
      "Node 73: 0.0\n",
      "Node 74: -0.0004343390464782715\n",
      "Node 75: 0.0005745291709899902\n",
      "Node 76: -1.0132789611816406e-06\n",
      "Node 77: 0.0\n",
      "Node 78: 0.0\n",
      "Node 79: -0.0012166798114776611\n",
      "Node 80: -0.00014972686767578125\n",
      "Node 81: -0.001115262508392334\n",
      "Node 82: 0.0012983977794647217\n",
      "Node 83: 0.0003294646739959717\n",
      "Node 84: 0.0\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: -0.001516193151473999\n",
      "Node 88: 0.00018808245658874512\n",
      "Node 89: -2.9802322387695312e-08\n",
      "Node 90: -0.002256244421005249\n",
      "Node 91: -0.0030793845653533936\n",
      "Node 92: -0.0007765591144561768\n",
      "Node 93: -0.0010057389736175537\n",
      "Node 94: 0.0002111494541168213\n",
      "Node 95: -0.003379344940185547\n",
      "Node 96: 0.0\n",
      "Node 97: -0.0014612078666687012\n",
      "Node 98: 0.0\n",
      "Node 99: -0.0012485086917877197\n",
      "Node 100: -0.0004869401454925537\n",
      "Node 101: 0.0\n",
      "Node 102: -0.0018114745616912842\n",
      "Node 103: -0.0016598105430603027\n",
      "Node 104: 0.0\n",
      "Node 105: -5.066394805908203e-07\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0018524527549743652\n",
      "Node 108: 0.0\n",
      "Node 109: -0.0018687546253204346\n",
      "Node 110: -0.003041476011276245\n",
      "Node 111: 0.00020515918731689453\n",
      "Node 112: -0.00018528103828430176\n",
      "Node 113: -0.0027299225330352783\n",
      "Node 114: -0.0014698505401611328\n",
      "Node 115: -0.0010254979133605957\n",
      "Node 116: -0.0008705556392669678\n",
      "Node 117: -0.0006078779697418213\n",
      "Node 118: -0.00030690431594848633\n",
      "Node 119: -0.0006461739540100098\n",
      "Node 120: 0.0\n",
      "Node 121: -0.0004106760025024414\n",
      "Node 122: -0.00022330880165100098\n",
      "Node 123: -0.003067612648010254\n",
      "Node 124: -0.0004870593547821045\n",
      "Node 125: -0.0008786022663116455\n",
      "Node 126: -0.0018548071384429932\n",
      "Node 127: -0.0004245340824127197\n",
      "Node 128: 0.0003553628921508789\n",
      "Node 129: -0.005039095878601074\n",
      "Node 130: -0.003448277711868286\n",
      "Node 131: 0.0\n",
      "Node 132: -0.00249558687210083\n",
      "Node 133: -0.002127528190612793\n",
      "Node 134: -0.0032663345336914062\n",
      "Node 135: -0.0027578771114349365\n",
      "Node 136: -0.0020661354064941406\n",
      "Node 137: 0.0\n",
      "Node 138: -0.0008827447891235352\n",
      "Node 139: -4.538893699645996e-05\n",
      "Node 140: 0.0\n",
      "Node 141: -2.8133392333984375e-05\n",
      "Node 142: -8.940696716308594e-08\n",
      "Node 143: -0.0028717219829559326\n",
      "Node 144: -0.0006638765335083008\n",
      "Node 145: 0.0\n",
      "Node 146: 0.0005339384078979492\n",
      "Node 147: -2.4437904357910156e-05\n",
      "Node 148: -0.003769010305404663\n",
      "Node 149: -0.0031836330890655518\n",
      "Node 150: -0.0006935596466064453\n",
      "Node 151: 0.0\n",
      "Node 152: 2.086162567138672e-07\n",
      "Node 153: -2.384185791015625e-07\n",
      "Node 154: 0.0\n",
      "Node 155: -0.0075563788414001465\n",
      "Node 156: 0.0\n",
      "Node 157: -0.0036211013793945312\n",
      "Node 158: -0.0025259852409362793\n",
      "Node 159: -0.0012029409408569336\n",
      "Node 160: 0.00010839104652404785\n",
      "Node 161: -0.0023080408573150635\n",
      "Node 162: -0.001986086368560791\n",
      "Node 163: -0.0024718642234802246\n",
      "Node 164: -0.0020658671855926514\n",
      "Node 165: -0.003598034381866455\n",
      "Node 166: -0.005419909954071045\n",
      "Node 167: 0.0\n",
      "Node 168: 0.0\n",
      "Node 169: 0.0\n",
      "Node 170: 0.0\n",
      "Node 171: 0.0\n",
      "Node 172: -0.0019556283950805664\n",
      "Node 173: 2.175569534301758e-06\n",
      "Node 174: -0.0025828182697296143\n",
      "Node 175: -0.002034127712249756\n",
      "Node 176: -0.0006369054317474365\n",
      "Node 177: -0.0017982721328735352\n",
      "Node 178: -0.00017303228378295898\n",
      "Node 179: -0.0011839568614959717\n",
      "Node 180: -0.0005984008312225342\n",
      "Node 181: 0.00020891427993774414\n",
      "Node 182: 0.0\n",
      "Node 183: -0.0014660060405731201\n",
      "Node 184: 0.0\n",
      "Node 185: 0.0007293522357940674\n",
      "Node 186: -5.364418029785156e-07\n",
      "Node 187: 0.0\n",
      "Node 188: 0.0004876554012298584\n",
      "Node 189: -0.0021678805351257324\n",
      "Node 190: -2.9802322387695312e-08\n",
      "Node 191: -0.004750192165374756\n",
      "Node 192: -1.4901161193847656e-07\n",
      "Node 193: -1.7881393432617188e-07\n",
      "Node 194: -0.0007176101207733154\n",
      "Node 195: 0.00019103288650512695\n",
      "Node 196: -0.003178209066390991\n",
      "Node 197: 0.0\n",
      "Node 198: -0.002330958843231201\n",
      "Node 199: 0.0\n",
      "Node 200: -0.0031678378582000732\n",
      "Node 201: -0.0070788562297821045\n",
      "Node 202: -0.0018282830715179443\n",
      "Node 203: -0.00026413798332214355\n",
      "Node 204: 0.0\n",
      "Node 205: 2.9802322387695312e-08\n",
      "Node 206: 0.00030875205993652344\n",
      "Node 207: -0.001644045114517212\n",
      "Node 208: -0.0005442202091217041\n",
      "Node 209: -0.0006201565265655518\n",
      "Node 210: -0.0006835460662841797\n",
      "Node 211: -0.0017864108085632324\n",
      "Node 212: 0.0\n",
      "Node 213: -0.0006373822689056396\n",
      "Node 214: -1.1920928955078125e-07\n",
      "Node 215: 0.0\n",
      "Node 216: -0.004441201686859131\n",
      "Node 217: -0.0020587146282196045\n",
      "Node 218: -0.0006463229656219482\n",
      "Node 219: 0.0\n",
      "Node 220: 0.0\n",
      "Node 221: -0.00031694769859313965\n",
      "Node 222: 0.0\n",
      "Node 223: -0.00045621395111083984\n",
      "Node 224: -0.004156261682510376\n",
      "Node 225: 0.0018064379692077637\n",
      "Node 226: -2.9802322387695312e-08\n",
      "Node 227: -0.0002816915512084961\n",
      "Node 228: -0.0008938908576965332\n",
      "Node 229: -0.00292050838470459\n",
      "Node 230: 0.004138976335525513\n",
      "Node 231: -4.941225051879883e-05\n",
      "Node 232: -0.0027875006198883057\n",
      "Node 233: -4.649162292480469e-06\n",
      "Node 234: 0.0\n",
      "Node 235: -0.0019334852695465088\n",
      "Node 236: -0.003627777099609375\n",
      "Node 237: 0.0\n",
      "Node 238: 0.0\n",
      "Node 239: -0.0003783106803894043\n",
      "Node 240: -0.002102404832839966\n",
      "Node 241: 0.0\n",
      "Node 242: 0.0\n",
      "Node 243: 0.0\n",
      "Node 244: -0.0024751126766204834\n",
      "Node 245: 0.0\n",
      "Node 246: 0.0\n",
      "Node 247: -0.0010548830032348633\n",
      "Node 248: 0.0\n",
      "Node 249: 0.0\n",
      "Node 250: -0.0016336441040039062\n",
      "Node 251: -0.0003866851329803467\n",
      "Node 252: -0.0020935237407684326\n",
      "Node 253: 0.0009052455425262451\n",
      "Node 254: 0.0\n",
      "Node 255: -0.0019350051879882812\n"
     ]
    }
   ],
   "source": [
    "l, a, auc = model2.evaluate(x_train, y_train, verbose=2, batch_size=256)\n",
    "or_weights2 = model2.get_weights()\n",
    "weight_len = len(or_weights2) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "\n",
    "#For-loop over dense layers\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights2)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_train, y_train, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))\n",
    "    \n",
    "#For-loop over convolutional layers\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights2)\n",
    "        w[conv_len - (2*layer+1)][:, :, :, i] = 0\n",
    "        w[conv_len - 2*layer][i] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_train, y_train, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 84\n",
      "Worse Nodes: 22\n",
      "Important Nodes: 150\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 43\n",
      "Worse Nodes: 1\n",
      "Important Nodes: 20\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 0.7984 - accuracy: 0.7392 - auc_4: 0.9628\n",
      "Node 0: 0.0\n",
      "Node 1: 0.0\n",
      "Node 2: 0.008527517318725586\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: -0.061670780181884766\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: -0.009320676326751709\n",
      "Node 12: 0.0\n",
      "Node 13: -0.029446899890899658\n",
      "Node 14: -0.11987334489822388\n",
      "Node 15: -0.2563565969467163\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: -0.1815524697303772\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: -5.960464477539063e-08\n",
      "Node 24: 0.0\n",
      "Node 25: -2.9206275939941406e-06\n",
      "Node 26: 0.0\n",
      "Node 27: -0.052210450172424316\n",
      "Node 28: 0.0\n",
      "Node 29: -0.00041747093200683594\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: -0.2058887481689453\n",
      "Node 35: 0.0\n",
      "Node 36: -0.00036913156509399414\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: -0.09705352783203125\n",
      "Node 44: -0.31480371952056885\n",
      "Node 45: -0.27714407444000244\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: -0.2665746212005615\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: -0.05614393949508667\n",
      "Node 54: 0.0\n",
      "Node 55: -0.26064491271972656\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0013911724090576172\n",
      "Node 58: -0.00012260675430297852\n",
      "Node 59: 2.384185791015625e-07\n",
      "Node 60: 0.0\n",
      "Node 61: 0.0\n",
      "Node 62: 0.0\n",
      "Node 63: 0.0\n",
      "Node 0: 0.0009682774543762207\n",
      "Node 1: -0.0012143850326538086\n",
      "Node 2: 0.0\n",
      "Node 3: -1.3709068298339844e-06\n",
      "Node 4: -4.76837158203125e-07\n",
      "Node 5: 0.002109527587890625\n",
      "Node 6: -0.0006174445152282715\n",
      "Node 7: -0.0009421110153198242\n",
      "Node 8: 0.00134199857711792\n",
      "Node 9: 0.0005490779876708984\n",
      "Node 10: 2.3245811462402344e-06\n",
      "Node 11: 0.0018274188041687012\n",
      "Node 12: -0.0002834200859069824\n",
      "Node 13: -0.0012377500534057617\n",
      "Node 14: -0.0014548301696777344\n",
      "Node 15: 0.0020037293434143066\n",
      "Node 16: 0.002504289150238037\n",
      "Node 17: -0.000713646411895752\n",
      "Node 18: 0.0\n",
      "Node 19: -1.7881393432617188e-07\n",
      "Node 20: 0.0\n",
      "Node 21: -0.0009124279022216797\n",
      "Node 22: 0.00018072128295898438\n",
      "Node 23: 0.0030825138092041016\n",
      "Node 24: 0.003656744956970215\n",
      "Node 25: -0.0018599629402160645\n",
      "Node 26: 0.0006195902824401855\n",
      "Node 27: 0.0\n",
      "Node 28: 0.0030388236045837402\n",
      "Node 29: -0.0038657784461975098\n",
      "Node 30: 0.0016228556632995605\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: -0.0008481144905090332\n",
      "Node 34: 2.4437904357910156e-06\n",
      "Node 35: 0.0\n",
      "Node 36: 0.00010591745376586914\n",
      "Node 37: 0.0\n",
      "Node 38: 0.0021004080772399902\n",
      "Node 39: -0.0004532337188720703\n",
      "Node 40: 0.0005960464477539062\n",
      "Node 41: 0.0013800263404846191\n",
      "Node 42: -0.00013190507888793945\n",
      "Node 43: 0.0\n",
      "Node 44: 0.0007298588752746582\n",
      "Node 45: -0.0020411014556884766\n",
      "Node 46: 0.003000319004058838\n",
      "Node 47: 0.0\n",
      "Node 48: -4.458427429199219e-05\n",
      "Node 49: 0.00010132789611816406\n",
      "Node 50: 0.0\n",
      "Node 51: -0.0011252164840698242\n",
      "Node 52: 0.003157198429107666\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0001717209815979004\n",
      "Node 56: 0.0005467534065246582\n",
      "Node 57: -0.001015782356262207\n",
      "Node 58: 0.0005988478660583496\n",
      "Node 59: -0.0018669366836547852\n",
      "Node 60: 0.0008248686790466309\n",
      "Node 61: -0.0011690258979797363\n",
      "Node 62: -0.0013782978057861328\n",
      "Node 63: -0.0008027553558349609\n",
      "Node 64: -0.004641354084014893\n",
      "Node 65: 0.0\n",
      "Node 66: 0.0\n",
      "Node 67: -1.1920928955078125e-07\n",
      "Node 68: 1.2814998626708984e-05\n",
      "Node 69: 0.0\n",
      "Node 70: 0.0007742047309875488\n",
      "Node 71: -0.002827465534210205\n",
      "Node 72: 0.001790165901184082\n",
      "Node 73: 0.0\n",
      "Node 74: 0.00013464689254760742\n",
      "Node 75: 0.0024019479751586914\n",
      "Node 76: 1.0728836059570312e-06\n",
      "Node 77: 0.0\n",
      "Node 78: 0.0\n",
      "Node 79: 0.0031707286834716797\n",
      "Node 80: 0.0019205808639526367\n",
      "Node 81: 0.0009453296661376953\n",
      "Node 82: 0.003175973892211914\n",
      "Node 83: 0.0011268258094787598\n",
      "Node 84: 0.0\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0007243156433105469\n",
      "Node 88: 0.004002571105957031\n",
      "Node 89: 0.0\n",
      "Node 90: -0.000748753547668457\n",
      "Node 91: -0.0018592476844787598\n",
      "Node 92: 0.0016663074493408203\n",
      "Node 93: -0.00037729740142822266\n",
      "Node 94: 0.0009607672691345215\n",
      "Node 95: -0.0006386637687683105\n",
      "Node 96: 0.0\n",
      "Node 97: -3.987550735473633e-05\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0006838440895080566\n",
      "Node 100: 0.0003960132598876953\n",
      "Node 101: 0.0\n",
      "Node 102: -0.00022619962692260742\n",
      "Node 103: 0.0009683370590209961\n",
      "Node 104: 0.0\n",
      "Node 105: 9.5367431640625e-07\n",
      "Node 106: 0.0\n",
      "Node 107: 0.005772113800048828\n",
      "Node 108: 0.0\n",
      "Node 109: 0.0019865036010742188\n",
      "Node 110: -0.0031145215034484863\n",
      "Node 111: 0.0008632540702819824\n",
      "Node 112: -0.00010454654693603516\n",
      "Node 113: -0.0018517374992370605\n",
      "Node 114: 0.0018416047096252441\n",
      "Node 115: -0.0004469156265258789\n",
      "Node 116: 0.0006337761878967285\n",
      "Node 117: 0.0007991790771484375\n",
      "Node 118: 0.0023860931396484375\n",
      "Node 119: 0.0014865994453430176\n",
      "Node 120: 0.0\n",
      "Node 121: 0.00043570995330810547\n",
      "Node 122: 0.0015957951545715332\n",
      "Node 123: -0.0023772120475769043\n",
      "Node 124: 0.00011897087097167969\n",
      "Node 125: 0.0036144256591796875\n",
      "Node 126: -0.0008551478385925293\n",
      "Node 127: 0.0025874972343444824\n",
      "Node 128: 0.0022816061973571777\n",
      "Node 129: -0.0010287761688232422\n",
      "Node 130: -0.0018775463104248047\n",
      "Node 131: 0.0\n",
      "Node 132: -0.0012935400009155273\n",
      "Node 133: -0.0013509392738342285\n",
      "Node 134: -0.001116931438446045\n",
      "Node 135: -0.00025200843811035156\n",
      "Node 136: 0.0004159212112426758\n",
      "Node 137: 0.0\n",
      "Node 138: 0.0028277039527893066\n",
      "Node 139: 4.291534423828125e-05\n",
      "Node 140: 0.0\n",
      "Node 141: 1.537799835205078e-05\n",
      "Node 142: 0.0\n",
      "Node 143: 0.0009360313415527344\n",
      "Node 144: -0.00030618906021118164\n",
      "Node 145: 0.0\n",
      "Node 146: 0.003912448883056641\n",
      "Node 147: 0.0028171539306640625\n",
      "Node 148: -0.0021300315856933594\n",
      "Node 149: -0.0007762312889099121\n",
      "Node 150: -0.00045734643936157227\n",
      "Node 151: 0.0\n",
      "Node 152: 0.0\n",
      "Node 153: 1.7881393432617188e-07\n",
      "Node 154: 0.0\n",
      "Node 155: -0.006265223026275635\n",
      "Node 156: 0.0\n",
      "Node 157: -0.0006244778633117676\n",
      "Node 158: -0.0003898739814758301\n",
      "Node 159: 0.0011244416236877441\n",
      "Node 160: 0.004308044910430908\n",
      "Node 161: -0.0009355545043945312\n",
      "Node 162: 0.00015223026275634766\n",
      "Node 163: -0.0005397200584411621\n",
      "Node 164: -0.000798642635345459\n",
      "Node 165: 0.00021475553512573242\n",
      "Node 166: -0.003906965255737305\n",
      "Node 167: 0.0\n",
      "Node 168: 0.0\n",
      "Node 169: 0.0\n",
      "Node 170: 0.0\n",
      "Node 171: 0.0\n",
      "Node 172: 0.00040215253829956055\n",
      "Node 173: -2.384185791015625e-07\n",
      "Node 174: -0.0005967020988464355\n",
      "Node 175: -0.0013284087181091309\n",
      "Node 176: 0.0015502572059631348\n",
      "Node 177: -0.0005164146423339844\n",
      "Node 178: 5.781650543212891e-06\n",
      "Node 179: -0.000870048999786377\n",
      "Node 180: 0.0008842349052429199\n",
      "Node 181: 0.0018088817596435547\n",
      "Node 182: 0.0\n",
      "Node 183: 0.0021439194679260254\n",
      "Node 184: 0.0\n",
      "Node 185: 0.0027934908866882324\n",
      "Node 186: 0.0\n",
      "Node 187: 0.0\n",
      "Node 188: 0.003582298755645752\n",
      "Node 189: -0.0014906525611877441\n",
      "Node 190: 1.1920928955078125e-07\n",
      "Node 191: -0.004227995872497559\n",
      "Node 192: 0.0\n",
      "Node 193: 4.172325134277344e-07\n",
      "Node 194: 0.0010071992874145508\n",
      "Node 195: 0.001291811466217041\n",
      "Node 196: -0.0009177327156066895\n",
      "Node 197: 0.0\n",
      "Node 198: -0.0008745193481445312\n",
      "Node 199: 0.0\n",
      "Node 200: -0.0015906095504760742\n",
      "Node 201: -0.0061130523681640625\n",
      "Node 202: -0.00022721290588378906\n",
      "Node 203: 0.0019069314002990723\n",
      "Node 204: 0.0\n",
      "Node 205: 0.0\n",
      "Node 206: 0.0013720989227294922\n",
      "Node 207: -2.2232532501220703e-05\n",
      "Node 208: 0.002117931842803955\n",
      "Node 209: 0.002322971820831299\n",
      "Node 210: 0.002773761749267578\n",
      "Node 211: -0.0009404420852661133\n",
      "Node 212: 0.0\n",
      "Node 213: 0.0013885498046875\n",
      "Node 214: 0.0\n",
      "Node 215: 0.0\n",
      "Node 216: -0.0029489994049072266\n",
      "Node 217: -0.0003535747528076172\n",
      "Node 218: -0.0001615285873413086\n",
      "Node 219: 0.0\n",
      "Node 220: 0.0\n",
      "Node 221: 0.0017009973526000977\n",
      "Node 222: 0.0\n",
      "Node 223: -0.0002605915069580078\n",
      "Node 224: -0.0035677552223205566\n",
      "Node 225: 0.00523144006729126\n",
      "Node 226: 0.0\n",
      "Node 227: 0.0008458495140075684\n",
      "Node 228: -0.0005245804786682129\n",
      "Node 229: -0.0013633966445922852\n",
      "Node 230: 0.006519794464111328\n",
      "Node 231: -3.838539123535156e-05\n",
      "Node 232: -0.0007848739624023438\n",
      "Node 233: -2.7418136596679688e-06\n",
      "Node 234: 0.0\n",
      "Node 235: -0.0006982684135437012\n",
      "Node 236: -0.002147495746612549\n",
      "Node 237: 0.0\n",
      "Node 238: 0.0\n",
      "Node 239: 0.0005933642387390137\n",
      "Node 240: 0.0007533431053161621\n",
      "Node 241: 0.0\n",
      "Node 242: 0.0\n",
      "Node 243: 0.0\n",
      "Node 244: -0.0005186796188354492\n",
      "Node 245: 0.0\n",
      "Node 246: 0.0\n",
      "Node 247: 0.0025309324264526367\n",
      "Node 248: 0.0\n",
      "Node 249: 0.0\n",
      "Node 250: -0.0009960532188415527\n",
      "Node 251: -0.0003472566604614258\n",
      "Node 252: -0.0010825395584106445\n",
      "Node 253: 0.0021359920501708984\n",
      "Node 254: 0.0\n",
      "Node 255: -0.0014472603797912598\n"
     ]
    }
   ],
   "source": [
    "l, a, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights2 = model2.get_weights()\n",
    "weight_len = len(or_weights2) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "\n",
    "#For-loop over dense layers\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights2)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))\n",
    "    \n",
    "#For-loop over convolutional layers\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights2)\n",
    "        w[conv_len - (2*layer+1)][:, :, :, i] = 0\n",
    "        w[conv_len - 2*layer][i] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 85\n",
      "Worse Nodes: 90\n",
      "Important Nodes: 81\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 45\n",
      "Worse Nodes: 2\n",
      "Important Nodes: 17\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7811 - accuracy: 0.7343 - auc: 0.9641\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7811414003372192 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7791403532028198 --- Change: 0.0012207210063934326 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7791403532028198 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7333999872207642 --- Loss: 0.7788729667663574 --- Change: 9.717345237731933e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7333999872207642 --- Loss: 0.7788729667663574 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7716400623321533 --- Change: 0.0053630292415618895 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.001049727201461792 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 3.808140754699707e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700837254524231 --- Change: 1.6272068023681639e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688760161399841 --- Change: 0.0006654024124145508 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688760161399841 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688760161399841 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: -1.043081283569336e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688751220703125 --- Change: 1.6689300537109375e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688751220703125 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688727378845215 --- Change: 1.6689300537109375e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688727378845215 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0005234420299530029 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0003397107124328613 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671268582344055 --- Change: -1.043081283569336e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7661057114601135 --- Change: 0.0007148027420043945 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7322999835014343 --- Loss: 0.7657871842384338 --- Change: 0.00013297200202941892 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7322999835014343 --- Loss: 0.7657871842384338 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7322999835014343 --- Loss: 0.7657871842384338 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7644184827804565 --- Change: 0.001558101177215576 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.736299991607666 --- Loss: 0.7636383771896362 --- Change: 0.0011460661888122556 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0014307498931884766 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7329999804496765 --- Loss: 0.7599818706512451 --- Change: 0.00013880133628845208 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7330999970436096 --- Loss: 0.7598163485527039 --- Change: 0.00014587044715881346 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.732699990272522 --- Loss: 0.7596465945243835 --- Change: -1.174211502075198e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7335000038146973 --- Loss: 0.7598873376846313 --- Change: 7.148385047912599e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7335000038146973 --- Loss: 0.7598873376846313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7335000038146973 --- Loss: 0.7598873376846313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7333999872207642 --- Loss: 0.7562488317489624 --- Change: 0.0025169491767883297 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.00037263631820678707 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: -1.6689300537109374e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7551780939102173 --- Change: 0.0004670441150665283 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336000204086304 --- Loss: 0.7549936771392822 --- Change: 9.910464286804198e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7547692656517029 --- Change: 0.000187075138092041 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7547692656517029 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0007869541645050049 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.001280355453491211 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 9.145736694335937e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7516787648200989 --- Change: 0.00018459558486938477 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7353000044822693 --- Loss: 0.751879870891571 --- Change: 0.00015922188758850096 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7371000051498413 --- Loss: 0.7502419948577881 --- Change: 0.0016865134239196777 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.737500011920929 --- Loss: 0.75013267993927 --- Change: 0.00019652247428894042 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.737500011920929 --- Loss: 0.7501348257064819 --- Change: -1.5020370483398437e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 5.632638931274414e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7383999824523926 --- Loss: 0.7503913044929504 --- Change: 8.482336997985839e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 4.27842140197754e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394000291824341 --- Loss: 0.7506734132766724 --- Change: 5.97536563873291e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394000291824341 --- Loss: 0.7506734132766724 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 3.303289413452148e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7504774928092957 --- Change: 0.0001340985298156738 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0004308104515075683 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7401999831199646 --- Loss: 0.7500375509262085 --- Change: 8.714795112609861e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7398999929428101 --- Loss: 0.7498258352279663 --- Change: 5.820393562316893e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7398999929428101 --- Loss: 0.7498258352279663 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7400000095367432 --- Loss: 0.7498515248298645 --- Change: 1.2022256851196288e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 2.4086236953735373e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502560019493103 --- Change: 2.2780895233154296e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502560019493103 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 7.105469703674316e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 5.757808685302734e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7415000200271606 --- Loss: 0.7496580481529236 --- Change: 0.00046175718307495117 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0002638101577758789 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493680715560913 --- Change: -8.344650268554688e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493683695793152 --- Change: -2.086162567138672e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493864297866821 --- Change: 1.7362833023071286e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493864297866821 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: -3.880262374877929e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 1.8942356109619138e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493222951889038 --- Change: 2.9832124710083008e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493222951889038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_weights4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-efc1a00f7d00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mplaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_removed2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mtester_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_weights4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_weights4' is not defined"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "tol = -1e-5\n",
    "layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.806, 0.92, 0.672, 0.477, 0.694, 0.608, 0.772, 0.805, 0.852, 0.737]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.777, 0.837, 0.626, 0.467, 0.751, 0.67, 0.812, 0.803, 0.867, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7744 - accuracy: 0.7343 - auc: 0.9650\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.013817846107482955 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536677002906799 --- Change: 0.05909848084449766 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 1.3107061386108399e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0008232743263244635 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.897 --- Loss: 1.1212269067764282 --- Change: 0.048295856666564954 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.121446132659912 --- Change: 0.0041342322349548375 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.006408405303955078 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.902 --- Loss: 1.0952787399291992 --- Change: 0.0007418125152587884 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0015961946487426768 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0027626694679260266 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.907 --- Loss: 1.0889948606491089 --- Change: 0.001026299667358399 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.908 --- Loss: 1.088670253753662 --- Change: 0.0007973820686340337 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0902987718582153 --- Change: 0.0009114445686340343 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0899895429611206 --- Change: 9.276866912841796e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0030559185028076192 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.090509295463562 --- Change: 0.0002881557464599615 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.0894718170166016 --- Change: 0.0003112435340881348 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0015142022132873523 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.917 --- Loss: 1.0784984827041626 --- Change: 0.003177798080444338 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.00047988004684448296 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0002508909225463873 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.08073091506958 --- Change: -5.006790161132813e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.92 --- Loss: 1.0827049016952515 --- Change: 0.00010780401229858454 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0836422443389893 --- Change: 0.00041879720687866263 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0834838151931763 --- Change: 4.752874374389648e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0004411697387695312 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0808788537979126 --- Change: 0.0003403186798095703 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0007631720542907726 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.926 --- Loss: 1.085551381111145 --- Change: 0.0013350697517395035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0872161388397217 --- Change: 0.0002005726814270025 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.000680989170074464 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 1.7881393432617188e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0895538330078125 --- Change: 1.7523765563964844e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0848712921142578 --- Change: 4.76226806640518e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0842589139938354 --- Change: 0.00018371343612670897 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0870466232299805 --- Change: 0.0005636872291564952 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.931 --- Loss: 1.0885720252990723 --- Change: 0.0009423793792724621 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.933 --- Loss: 1.0920823812484741 --- Change: 0.0003468932151794444 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0923717021942139 --- Change: 0.0006132037162780767 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0916045904159546 --- Change: 0.0002301335334777832 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.095198631286621 --- Change: 0.00032178773880005003 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: -5.0067901611328125e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0005976676940917968 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0006748795509338379 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: -1.430511474609375e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909996032714844 --- Change: -7.68899917602539e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 3.247261047363281e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.935 --- Loss: 1.0879974365234375 --- Change: 0.00016817741394042917 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.937 --- Loss: 1.0896828174591064 --- Change: 0.0008943857192993175 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.938 --- Loss: 1.092002034187317 --- Change: 4.2349815367881405e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.939 --- Loss: 1.0930116176605225 --- Change: 0.00039712495803833063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.941 --- Loss: 1.0943353176116943 --- Change: 0.0010028900146484385 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0956448316574097 --- Change: 0.0003071457862854009 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961695909500122 --- Change: 0.0005425722122192388 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961976051330566 --- Change: -8.404254913330078e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0938652753829956 --- Change: -3.010749816899881e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.096069097518921 --- Change: 3.885335922241271e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0941189527511597 --- Change: 0.0005850434303283691 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.944 --- Loss: 1.0953398942947388 --- Change: 0.0003337175369262701 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.946 --- Loss: 1.097354531288147 --- Change: 0.0007956089019775401 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.947 --- Loss: 1.0990265607833862 --- Change: 0.00019839115142822317 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.948 --- Loss: 1.0994091033935547 --- Change: 0.0005852372169494634 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.101301670074463 --- Change: 0.00013222999572753958 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.1008661985397339 --- Change: 0.00013064146041870116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.0969769954681396 --- Change: 0.0011667609214782715 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0005680502891540538 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0006372961997985868 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 4.874467849731445e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 1.1086463928222655e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: -2.5033950805664064e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 1.5020370483398437e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 1.0728836059570311e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 2.1457672119140625e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.00014959573745727537 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.00038502216339111325 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 2.1457672119140623e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: -3.2186508178710934e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073286533355713 --- Change: 1.2874603271484374e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0005374581336975103 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078757047653198 --- Change: -1.5735626220703124e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078639030456543 --- Change: 3.540515899658203e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.955 --- Loss: 1.1005418300628662 --- Change: 0.0007966218948364245 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "oa = min(acc)\n",
    "ia = np.argmin(acc)\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 90.17798533837001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.782, 0.861, 0.71, 0.589, 0.574, 0.641, 0.783, 0.728, 0.877, 0.798]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.764, 0.702, 0.177, 0.193, 0.955, 0.619, 0.7, 0.377, 0.737, 0.862]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7744 - accuracy: 0.7343 - auc: 0.9650\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.013817846107482955 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536677002906799 --- Change: 0.05909848084449766 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 1.3107061386108399e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0008232743263244635 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.897 --- Loss: 1.1212269067764282 --- Change: 0.048295856666564954 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.121446132659912 --- Change: 0.0041342322349548375 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.006408405303955078 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.902 --- Loss: 1.0952787399291992 --- Change: 0.0007418125152587884 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0015961946487426768 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0027626694679260266 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.907 --- Loss: 1.0889948606491089 --- Change: 0.001026299667358399 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.908 --- Loss: 1.088670253753662 --- Change: 0.0007973820686340337 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0902987718582153 --- Change: 0.0009114445686340343 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0899895429611206 --- Change: 9.276866912841796e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0030559185028076192 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.090509295463562 --- Change: 0.0002881557464599615 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.0894718170166016 --- Change: 0.0003112435340881348 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0015142022132873523 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.917 --- Loss: 1.0784984827041626 --- Change: 0.003177798080444338 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.00047988004684448296 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0002508909225463873 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.08073091506958 --- Change: -5.006790161132813e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.92 --- Loss: 1.0827049016952515 --- Change: 0.00010780401229858454 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0836422443389893 --- Change: 0.00041879720687866263 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0834838151931763 --- Change: 4.752874374389648e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0004411697387695312 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0808788537979126 --- Change: 0.0003403186798095703 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0007631720542907726 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.926 --- Loss: 1.085551381111145 --- Change: 0.0013350697517395035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0872161388397217 --- Change: 0.0002005726814270025 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.000680989170074464 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 1.7881393432617188e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0895538330078125 --- Change: 1.7523765563964844e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0848712921142578 --- Change: 4.76226806640518e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0842589139938354 --- Change: 0.00018371343612670897 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0870466232299805 --- Change: 0.0005636872291564952 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.931 --- Loss: 1.0885720252990723 --- Change: 0.0009423793792724621 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.933 --- Loss: 1.0920823812484741 --- Change: 0.0003468932151794444 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0923717021942139 --- Change: 0.0006132037162780767 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0916045904159546 --- Change: 0.0002301335334777832 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.095198631286621 --- Change: 0.00032178773880005003 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: -5.0067901611328125e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0005976676940917968 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0006748795509338379 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: -1.430511474609375e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909996032714844 --- Change: -7.68899917602539e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 3.247261047363281e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.935 --- Loss: 1.0879974365234375 --- Change: 0.00016817741394042917 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.937 --- Loss: 1.0896828174591064 --- Change: 0.0008943857192993175 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.938 --- Loss: 1.092002034187317 --- Change: 4.2349815367881405e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.939 --- Loss: 1.0930116176605225 --- Change: 0.00039712495803833063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.941 --- Loss: 1.0943353176116943 --- Change: 0.0010028900146484385 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0956448316574097 --- Change: 0.0003071457862854009 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961695909500122 --- Change: 0.0005425722122192388 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961976051330566 --- Change: -8.404254913330078e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0938652753829956 --- Change: -3.010749816899881e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.096069097518921 --- Change: 3.885335922241271e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0941189527511597 --- Change: 0.0005850434303283691 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.944 --- Loss: 1.0953398942947388 --- Change: 0.0003337175369262701 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.946 --- Loss: 1.097354531288147 --- Change: 0.0007956089019775401 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.947 --- Loss: 1.0990265607833862 --- Change: 0.00019839115142822317 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.948 --- Loss: 1.0994091033935547 --- Change: 0.0005852372169494634 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.101301670074463 --- Change: 0.00013222999572753958 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.1008661985397339 --- Change: 0.00013064146041870116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.0969769954681396 --- Change: 0.0011667609214782715 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0005680502891540538 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0006372961997985868 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 4.874467849731445e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 1.1086463928222655e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: -2.5033950805664064e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 1.5020370483398437e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 1.0728836059570311e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 2.1457672119140625e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.00014959573745727537 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.00038502216339111325 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 2.1457672119140623e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: -3.2186508178710934e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073286533355713 --- Change: 1.2874603271484374e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0005374581336975103 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078757047653198 --- Change: -1.5735626220703124e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078639030456543 --- Change: 3.540515899658203e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.955 --- Loss: 1.1005418300628662 --- Change: 0.0007966218948364245 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "oa = min(acc)\n",
    "ia = np.argmin(acc)\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7605 - accuracy: 0.7492 - auc: 0.9653\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605359554290771 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 8.344650268554688e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: -2.0027160644531247e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0005593955516815185 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 2.1195411682128904e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596960663795471 --- Change: 0.00012850165367126463 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: -6.258487701416016e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: -4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 8.131861686706543e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0016119062900543212 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.004112809896469116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.000922095775604248 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 9.938478469848632e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7501125931739807 --- Change: 0.00012117028236389161 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7494815587997437 --- Change: 0.000501716136932373 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.00027591586112976076 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0019015729427337646 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7460865378379822 --- Change: 4.903674125671387e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.746314287185669 --- Change: -9.43541526794432e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463226914405823 --- Change: -6.133317947387695e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7457163333892822 --- Change: 0.0003644585609436035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457607388496399 --- Change: 2.8908252716064453e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457746863365173 --- Change: -9.763240814208983e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.00021591782569885254 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.00023236870765686035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.744422435760498 --- Change: 0.0001982927322387695 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7428980469703674 --- Change: 0.001157069206237793 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7425541281700134 --- Change: 3.074407577514647e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0001464664936065674 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426019310951233 --- Change: 8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7421849370002747 --- Change: 0.000171893835067749 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7391334176063538 --- Change: 0.002346062660217285 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7394374012947083 --- Change: 2.7197599411010726e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 4.941225051879883e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0010465919971466063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7381114959716797 --- Change: 0.00013164281845092773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: -8.225440979003906e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380809187889099 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7381917834281921 --- Change: 0.0001623988151550293 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7378065586090088 --- Change: 0.0001796603202819824 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0001419425010681152 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 2.5451183319091794e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377282381057739 --- Change: 3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: -8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7374983429908752 --- Change: 0.00013179779052734374 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7371328473091125 --- Change: 7.58528709411621e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.737308144569397 --- Change: 8.729100227355957e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 4.234910011291504e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7364974021911621 --- Change: 0.0005251705646514892 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 2.8067827224731444e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.00012409687042236328 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: -5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7360435724258423 --- Change: 0.00032130479812622067 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 7.528662681579589e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361932396888733 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: -2.8371810913085934e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 6.712079048156738e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361469864845276 --- Change: -1.543760299682617e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361464500427246 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361381649971008 --- Change: 5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 2.515316009521487e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: -1.251697540283203e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362189888954163 --- Change: 5.424022674560547e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 4.342794418334961e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7541999816894531 --- Loss: 0.7359620332717896 --- Change: 0.0001964330673217773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359568476676941 --- Change: 3.363490104675293e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359568476676941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 2.7954578399658202e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.00033099055290222165 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0001447677612304687 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344157099723816 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "frac = 0.3\n",
    "ignore_tol = -1e-2\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 110.70612378517787\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.787, 0.897, 0.654, 0.513, 0.729, 0.624, 0.842, 0.789, 0.849, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.783, 0.867, 0.637, 0.535, 0.727, 0.668, 0.81, 0.792, 0.871, 0.833]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7605 - accuracy: 0.7492 - auc: 0.9653\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: -2.0027160644531247e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0005593955516815185 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 2.1195411682128904e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596960663795471 --- Change: 0.00012850165367126463 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: -6.258487701416016e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: -4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 8.131861686706543e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0016119062900543212 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.004112809896469116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.000922095775604248 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 9.938478469848632e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7501125931739807 --- Change: 0.00012117028236389161 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7494815587997437 --- Change: 0.000501716136932373 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.00027591586112976076 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0019015729427337646 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7460865378379822 --- Change: 4.903674125671387e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.746314287185669 --- Change: -9.43541526794432e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463226914405823 --- Change: -6.133317947387695e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7457163333892822 --- Change: 0.0003644585609436035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457607388496399 --- Change: 2.8908252716064453e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457746863365173 --- Change: -9.763240814208983e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.00021591782569885254 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.00023236870765686035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.744422435760498 --- Change: 0.0001982927322387695 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7428980469703674 --- Change: 0.001157069206237793 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7425541281700134 --- Change: 3.074407577514647e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0001464664936065674 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426019310951233 --- Change: 8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7421849370002747 --- Change: 0.000171893835067749 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7391334176063538 --- Change: 0.002346062660217285 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7394374012947083 --- Change: 2.7197599411010726e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 4.941225051879883e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0010465919971466063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7381114959716797 --- Change: 0.00013164281845092773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: -8.225440979003906e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380809187889099 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7381917834281921 --- Change: 0.0001623988151550293 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7378065586090088 --- Change: 0.0001796603202819824 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0001419425010681152 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 2.5451183319091794e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377282381057739 --- Change: 3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: -8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7374983429908752 --- Change: 0.00013179779052734374 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7371328473091125 --- Change: 7.58528709411621e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.737308144569397 --- Change: 8.729100227355957e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 4.234910011291504e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7364974021911621 --- Change: 0.0005251705646514892 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 2.8067827224731444e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.00012409687042236328 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: -5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7360435724258423 --- Change: 0.00032130479812622067 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 7.528662681579589e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361932396888733 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: -2.8371810913085934e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 6.712079048156738e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361469864845276 --- Change: -1.543760299682617e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361464500427246 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361381649971008 --- Change: 5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 2.515316009521487e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: -1.251697540283203e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362189888954163 --- Change: 5.424022674560547e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 4.342794418334961e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7541999816894531 --- Loss: 0.7359620332717896 --- Change: 0.0001964330673217773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359568476676941 --- Change: 3.363490104675293e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 2.7954578399658202e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.00033099055290222165 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0001447677612304687 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344157099723816 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "frac = 0.3\n",
    "ignore_tol = -1e-2\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 47.23251302242279\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.787, 0.897, 0.654, 0.513, 0.729, 0.624, 0.842, 0.789, 0.849, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.783, 0.867, 0.637, 0.535, 0.727, 0.668, 0.81, 0.792, 0.871, 0.833]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7605 - accuracy: 0.7492 - auc: 0.9653\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: -2.0027160644531247e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0005593955516815185 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 2.1195411682128904e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596960663795471 --- Change: 0.00012850165367126463 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: -6.258487701416016e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: -4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 8.131861686706543e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0016119062900543212 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.004112809896469116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.000922095775604248 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 9.938478469848632e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7501125931739807 --- Change: 0.00012117028236389161 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7494815587997437 --- Change: 0.000501716136932373 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.00027591586112976076 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.7482198476791382 --- Change: 0.00042728781700134276 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470213770866394 --- Change: 0.0009289264678955078 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470213770866394 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470208406448364 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470208406448364 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.746618926525116 --- Change: 0.00028133988380432126 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7465864419937134 --- Change: 8.27491283416748e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7465864419937134 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7465864419937134 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7466005682945251 --- Change: -9.888410568237303e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7466089129447937 --- Change: -5.841255187988281e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7464084029197693 --- Change: 0.00017034411430358885 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7458956837654114 --- Change: 0.00023890137672424313 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7454628348350525 --- Change: 0.0002129971981048584 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0001942157745361328 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7480999827384949 --- Loss: 0.7441245913505554 --- Change: 0.00020255446434020992 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7442034482955933 --- Change: 0.000154799222946167 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7442034482955933 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7442033886909485 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7433729767799377 --- Change: 0.0005812883377075195 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.7434123158454895 --- Change: 2.467632293701174e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401872277259827 --- Change: 0.0021075546741485597 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401872277259827 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401856184005737 --- Change: 1.1265277862548827e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401856184005737 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.7387703061103821 --- Change: 0.001140725612640381 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.7387703061103821 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7490000128746033 --- Loss: 0.7386330366134644 --- Change: 0.00012609362602233887 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7490000128746033 --- Loss: 0.7386330366134644 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7387228608131409 --- Change: 0.00020711421966552735 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.00033801794052124023 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387546896934509 --- Change: -2.9206275939941403e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387546896934509 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7509999871253967 --- Loss: 0.7383127808570862 --- Change: 0.0002793312072753906 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7377004027366638 --- Change: 0.00015867352485656735 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7377004027366638 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.737541913986206 --- Change: 5.093216896057129e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.737541913986206 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375418543815613 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 1.043081283569336e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375417947769165 --- Change: -1.0013580322265623e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.737116813659668 --- Change: -2.926588058471647e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7493000030517578 --- Loss: 0.737097978591919 --- Change: 0.00013318657875061035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7506999969482422 --- Loss: 0.737538754940033 --- Change: 0.00011145472526550291 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.00015570521354675294 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7509999871253967 --- Loss: 0.7369372844696045 --- Change: 0.0003553211688995361 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7516999840736389 --- Loss: 0.7372460961341858 --- Change: -6.16908073425293e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7516999840736389 --- Loss: 0.7372574210166931 --- Change: -7.927417755126953e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7370762228965759 --- Change: 0.0002468407154083252 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7375624179840088 --- Change: 1.9669532775878906e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7375624179840088 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.737557590007782 --- Change: 3.379583358764648e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 7.998943328857438e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372033596038818 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 1.4185905456542967e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372083067893982 --- Change: -4.881620407104492e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372087836265564 --- Change: -3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372087836265564 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372079491615295 --- Change: 5.841255187988281e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7371987104415894 --- Change: 6.467103958129883e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.00015337467193603515 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: -1.251697540283203e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7534000277519226 --- Loss: 0.7372525930404663 --- Change: 7.866621017456056e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7534000277519226 --- Loss: 0.7372525930404663 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7534000277519226 --- Loss: 0.7372525930404663 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371766567230225 --- Change: 2.3150444030761717e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 5.841255187988281e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.00024483203887939455 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7366942167282104 --- Change: 3.228187561035156e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7366942167282104 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.7365595698356628 --- Change: 0.00012425780296325683 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 7.008910179138183e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0005673468112945557 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0003720462322235107 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7342455983161926 --- Change: 0.0005802929401397704 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7342453002929688 --- Change: 2.086162567138672e-07 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "frac = 0.3\n",
    "ignore_tol = -1e-3\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 12.586614600817363\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.787, 0.897, 0.654, 0.513, 0.729, 0.624, 0.842, 0.789, 0.849, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.766, 0.87, 0.632, 0.547, 0.711, 0.669, 0.828, 0.795, 0.876, 0.837]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "ignore_tol = -1e-2\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "ofp = fp.result().numpy()\n",
    "otp = tp.result().numpy()\n",
    "\n",
    "fp.reset_states()\n",
    "tp.reset_states()\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "\n",
    "        fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        \n",
    "        nfp = fp.result().numpy()\n",
    "        ntp = tp.result().numpy()\n",
    "        fp.reset_states()\n",
    "        tp.reset_states()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.5*(ntp - otp) + 0.5*(ofp - nfp) >= tol:\n",
    "            best_change = 0.5*(ntp - otp) + 0.5*(ofp - nfp)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            otp = ntp\n",
    "            ofp = nfp\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! True Positives:\", ntp, \"--- False Positives:\", nfp, \"--- Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif 0.5*(ntp - otp) + 0.5*(ofp - nfp) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "\n",
    "        fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        \n",
    "        nfp = fp.result().numpy()\n",
    "        ntp = tp.result().numpy()\n",
    "        fp.reset_states()\n",
    "        tp.reset_states()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.5*(ntp - otp) + 0.5*(ofp - nfp) >= tol:\n",
    "            best_change = 0.5*(ntp - otp) + 0.5*(ofp - nfp)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            otp = ntp\n",
    "            ofp = nfp\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! True Positives:\", ntp, \"--- False Positives:\", nfp, \"--- Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif 0.5*(ntp - otp) + 0.5*(ofp - nfp) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test_flat[y_test_flat==4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 6.011486442883809\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205.0\n",
      "626.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = tester_model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9405\n",
      "[ True  True  True ...  True False  True]\n",
      "323.0\n",
      "728.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = np.array([1 if y == 4 else 0 for y in y_pred_flat])\n",
    "y_test_class_5 = np.array([1 if y == 4 else 0 for y in y_test_flat])\n",
    "\n",
    "acc = (y_pred_class_5 == y_test_class_5).mean()\n",
    "print(acc)\n",
    "print(y_pred_class_5 == y_test_class_5)\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36692652a799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moriginal2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mweight_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfrac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model2' is not defined"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "frac = 0.75\n",
    "tol = -1e-5\n",
    "ignore_tol = -1e-3\n",
    "k = 9\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "num_removed2 = 0\n",
    "amount = []\n",
    "place = []\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class = np.array([1 if y == k else 0 for y in y_pred_flat])\n",
    "y_test_class = np.array([1 if y == k else 0 for y in y_test_flat])\n",
    "\n",
    "oa = (y_pred_class == y_test_class).mean()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class = np.array([1 if y == k else 0 for y in y_pred_flat])\n",
    "\n",
    "        na = (y_pred_class == y_test_class).mean()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amount.append(num_removed2)\n",
    "    place.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class = np.array([1 if y == k else 0 for y in y_pred_flat])\n",
    "\n",
    "        na = (y_pred_class == y_test_class).mean()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388.0\n",
      "731.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = tester_model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442.0\n",
      "774.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = np.array([1 if y == 4 else 0 for y in y_pred_flat])\n",
    "y_test_class_5 = np.array([1 if y == 4 else 0 for y in y_test_flat])\n",
    "\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.716, 0.765, 0.574, 0.653, 0.774, 0.559, 0.821, 0.775, 0.888, 0.838]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.773, 0.856, 0.599, 0.577, 0.731, 0.608, 0.816, 0.79, 0.848, 0.812]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Specific removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class3 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class4 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class5 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class6 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class7 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class8 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class9 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class10 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 116, 22, 7, 3],\n",
       " [[0,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   25,\n",
       "   26,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   37,\n",
       "   39,\n",
       "   40,\n",
       "   42,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   56,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61],\n",
       "  [0,\n",
       "   4,\n",
       "   5,\n",
       "   2,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   10,\n",
       "   13,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   23,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   19,\n",
       "   21,\n",
       "   9,\n",
       "   30,\n",
       "   37,\n",
       "   40,\n",
       "   45,\n",
       "   48,\n",
       "   25,\n",
       "   39,\n",
       "   24,\n",
       "   31,\n",
       "   43,\n",
       "   32,\n",
       "   51,\n",
       "   53,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   69,\n",
       "   70,\n",
       "   52,\n",
       "   58,\n",
       "   57,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   79,\n",
       "   80,\n",
       "   86,\n",
       "   92,\n",
       "   99,\n",
       "   102,\n",
       "   110,\n",
       "   114,\n",
       "   116,\n",
       "   118,\n",
       "   119,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   128,\n",
       "   129,\n",
       "   134,\n",
       "   138,\n",
       "   144,\n",
       "   145,\n",
       "   153,\n",
       "   156,\n",
       "   83,\n",
       "   123,\n",
       "   94,\n",
       "   78,\n",
       "   159,\n",
       "   162,\n",
       "   163,\n",
       "   165,\n",
       "   167,\n",
       "   169,\n",
       "   171,\n",
       "   182,\n",
       "   183,\n",
       "   181,\n",
       "   184,\n",
       "   185,\n",
       "   187,\n",
       "   188,\n",
       "   192,\n",
       "   194,\n",
       "   195,\n",
       "   203,\n",
       "   205,\n",
       "   210,\n",
       "   211,\n",
       "   215,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   225,\n",
       "   226,\n",
       "   232,\n",
       "   208,\n",
       "   82,\n",
       "   89,\n",
       "   204,\n",
       "   229,\n",
       "   200,\n",
       "   87,\n",
       "   111,\n",
       "   149,\n",
       "   238,\n",
       "   240,\n",
       "   241,\n",
       "   244,\n",
       "   252,\n",
       "   253,\n",
       "   255],\n",
       "  [9,\n",
       "   12,\n",
       "   19,\n",
       "   20,\n",
       "   23,\n",
       "   27,\n",
       "   33,\n",
       "   39,\n",
       "   42,\n",
       "   48,\n",
       "   57,\n",
       "   63,\n",
       "   69,\n",
       "   74,\n",
       "   76,\n",
       "   88,\n",
       "   94,\n",
       "   98,\n",
       "   102,\n",
       "   113,\n",
       "   120,\n",
       "   122],\n",
       "  [4, 6, 8, 23, 37, 50, 56],\n",
       "  [9, 11, 25]]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 111, 22, 7, 1],\n",
       " [[0,\n",
       "   3,\n",
       "   2,\n",
       "   4,\n",
       "   7,\n",
       "   6,\n",
       "   10,\n",
       "   9,\n",
       "   11,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   23,\n",
       "   22,\n",
       "   26,\n",
       "   25,\n",
       "   29,\n",
       "   28,\n",
       "   30,\n",
       "   33,\n",
       "   35,\n",
       "   34,\n",
       "   40,\n",
       "   37,\n",
       "   39,\n",
       "   45,\n",
       "   42,\n",
       "   50,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   52,\n",
       "   59,\n",
       "   58,\n",
       "   56,\n",
       "   60,\n",
       "   61],\n",
       "  [0,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   2,\n",
       "   10,\n",
       "   13,\n",
       "   1,\n",
       "   9,\n",
       "   16,\n",
       "   17,\n",
       "   21,\n",
       "   23,\n",
       "   18,\n",
       "   26,\n",
       "   27,\n",
       "   31,\n",
       "   32,\n",
       "   37,\n",
       "   40,\n",
       "   48,\n",
       "   36,\n",
       "   45,\n",
       "   53,\n",
       "   12,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   51,\n",
       "   5,\n",
       "   30,\n",
       "   58,\n",
       "   65,\n",
       "   69,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   86,\n",
       "   87,\n",
       "   92,\n",
       "   102,\n",
       "   99,\n",
       "   110,\n",
       "   114,\n",
       "   116,\n",
       "   118,\n",
       "   119,\n",
       "   38,\n",
       "   15,\n",
       "   39,\n",
       "   19,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   128,\n",
       "   129,\n",
       "   144,\n",
       "   134,\n",
       "   138,\n",
       "   24,\n",
       "   145,\n",
       "   153,\n",
       "   159,\n",
       "   162,\n",
       "   163,\n",
       "   167,\n",
       "   165,\n",
       "   169,\n",
       "   182,\n",
       "   184,\n",
       "   171,\n",
       "   185,\n",
       "   188,\n",
       "   187,\n",
       "   194,\n",
       "   192,\n",
       "   195,\n",
       "   205,\n",
       "   210,\n",
       "   211,\n",
       "   221,\n",
       "   215,\n",
       "   222,\n",
       "   223,\n",
       "   226,\n",
       "   225,\n",
       "   229,\n",
       "   94,\n",
       "   200,\n",
       "   85,\n",
       "   14,\n",
       "   66,\n",
       "   105,\n",
       "   166,\n",
       "   106,\n",
       "   82,\n",
       "   109,\n",
       "   161,\n",
       "   209,\n",
       "   232,\n",
       "   241,\n",
       "   238,\n",
       "   240,\n",
       "   252,\n",
       "   244,\n",
       "   253,\n",
       "   255],\n",
       "  [12,\n",
       "   19,\n",
       "   9,\n",
       "   20,\n",
       "   23,\n",
       "   42,\n",
       "   27,\n",
       "   33,\n",
       "   48,\n",
       "   57,\n",
       "   94,\n",
       "   63,\n",
       "   69,\n",
       "   74,\n",
       "   76,\n",
       "   98,\n",
       "   88,\n",
       "   112,\n",
       "   39,\n",
       "   102,\n",
       "   113,\n",
       "   122],\n",
       "  [4, 37, 8, 6, 23, 50, 56],\n",
       "  [9]]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 116, 24, 8, 1],\n",
       " [[0,\n",
       "   3,\n",
       "   2,\n",
       "   4,\n",
       "   7,\n",
       "   6,\n",
       "   10,\n",
       "   9,\n",
       "   11,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   23,\n",
       "   22,\n",
       "   26,\n",
       "   25,\n",
       "   29,\n",
       "   28,\n",
       "   30,\n",
       "   33,\n",
       "   35,\n",
       "   34,\n",
       "   40,\n",
       "   37,\n",
       "   39,\n",
       "   44,\n",
       "   42,\n",
       "   45,\n",
       "   50,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   52,\n",
       "   59,\n",
       "   58,\n",
       "   56,\n",
       "   60,\n",
       "   61],\n",
       "  [0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   5,\n",
       "   10,\n",
       "   16,\n",
       "   12,\n",
       "   13,\n",
       "   17,\n",
       "   18,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   32,\n",
       "   21,\n",
       "   23,\n",
       "   9,\n",
       "   19,\n",
       "   28,\n",
       "   30,\n",
       "   31,\n",
       "   22,\n",
       "   37,\n",
       "   36,\n",
       "   15,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   43,\n",
       "   45,\n",
       "   48,\n",
       "   24,\n",
       "   53,\n",
       "   54,\n",
       "   60,\n",
       "   63,\n",
       "   65,\n",
       "   72,\n",
       "   69,\n",
       "   74,\n",
       "   76,\n",
       "   79,\n",
       "   80,\n",
       "   92,\n",
       "   86,\n",
       "   102,\n",
       "   99,\n",
       "   110,\n",
       "   114,\n",
       "   118,\n",
       "   116,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   128,\n",
       "   129,\n",
       "   138,\n",
       "   134,\n",
       "   145,\n",
       "   153,\n",
       "   144,\n",
       "   159,\n",
       "   162,\n",
       "   163,\n",
       "   165,\n",
       "   180,\n",
       "   169,\n",
       "   171,\n",
       "   182,\n",
       "   34,\n",
       "   78,\n",
       "   123,\n",
       "   75,\n",
       "   58,\n",
       "   103,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   188,\n",
       "   187,\n",
       "   195,\n",
       "   192,\n",
       "   194,\n",
       "   200,\n",
       "   2,\n",
       "   66,\n",
       "   59,\n",
       "   94,\n",
       "   164,\n",
       "   64,\n",
       "   205,\n",
       "   210,\n",
       "   211,\n",
       "   215,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   225,\n",
       "   226,\n",
       "   238,\n",
       "   241,\n",
       "   240,\n",
       "   244,\n",
       "   245,\n",
       "   87,\n",
       "   181,\n",
       "   167,\n",
       "   203,\n",
       "   232,\n",
       "   252,\n",
       "   253,\n",
       "   255],\n",
       "  [12,\n",
       "   5,\n",
       "   9,\n",
       "   19,\n",
       "   20,\n",
       "   23,\n",
       "   33,\n",
       "   27,\n",
       "   57,\n",
       "   42,\n",
       "   48,\n",
       "   74,\n",
       "   63,\n",
       "   69,\n",
       "   76,\n",
       "   88,\n",
       "   94,\n",
       "   112,\n",
       "   24,\n",
       "   98,\n",
       "   102,\n",
       "   113,\n",
       "   120,\n",
       "   122],\n",
       "  [4, 37, 8, 6, 23, 50, 25, 56],\n",
       "  [9]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.save_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
