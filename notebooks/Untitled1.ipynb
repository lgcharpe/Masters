{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Filters from Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import tqdm\n",
    "import IProgress\n",
    "from hfunc import models\n",
    "from hfunc import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10\n",
    "class_accuracy = metrics.ClassAccuracy()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)\n",
    "y_train, y_test = tf.one_hot(y_train.flatten(), 10), tf.one_hot(y_test.flatten(), 10)\n",
    "y_test_flat = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "    \n",
    "tester_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7206 - accuracy: 0.3680 - auc_4: 0.8209\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3715 - accuracy: 0.4994 - auc_4: 0.8929\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.2415 - accuracy: 0.5440 - auc_4: 0.9128\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1366 - accuracy: 0.5842 - auc_4: 0.9271\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0497 - accuracy: 0.6189 - auc_4: 0.9378\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9677 - accuracy: 0.6463 - auc_4: 0.9471\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8923 - accuracy: 0.6777 - auc_4: 0.9548\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.8185 - accuracy: 0.7022 - auc_4: 0.96 - 9s 6ms/step - loss: 0.8186 - accuracy: 0.7021 - auc_4: 0.9618\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7567 - accuracy: 0.7261 - auc_4: 0.9673\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6943 - accuracy: 0.7482 - auc_4: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f55aa05248>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 1.3278 - accuracy: 0.5847 - auc_4: 0.9126\n",
      "Node 0: -0.0014998912811279297\n",
      "Node 1: 9.822845458984375e-05\n",
      "Node 2: 0.0001723766326904297\n",
      "Node 3: 0.0\n",
      "Node 4: -0.0011245012283325195\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0005205869674682617\n",
      "Node 7: 0.003957152366638184\n",
      "Node 8: 0.004172801971435547\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0009812116622924805\n",
      "Node 11: 0.0\n",
      "Node 12: 0.007231950759887695\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: -0.048014163970947266\n",
      "Node 16: 0.0\n",
      "Node 17: 2.7298927307128906e-05\n",
      "Node 18: -0.02084052562713623\n",
      "Node 19: 0.0\n",
      "Node 20: -0.026913046836853027\n",
      "Node 21: 0.00026988983154296875\n",
      "Node 22: -0.0019714832305908203\n",
      "Node 23: 0.0\n",
      "Node 24: 0.008685946464538574\n",
      "Node 25: -0.0003619194030761719\n",
      "Node 26: 6.556510925292969e-06\n",
      "Node 27: 0.016967415809631348\n",
      "Node 28: 1.1920928955078125e-07\n",
      "Node 29: -0.009758353233337402\n",
      "Node 30: 0.0011377334594726562\n",
      "Node 31: 0.0\n",
      "Node 32: 0.00048220157623291016\n",
      "Node 33: -1.5497207641601562e-06\n",
      "Node 34: 0.001094222068786621\n",
      "Node 35: 0.0026895999908447266\n",
      "Node 36: -0.0025289058685302734\n",
      "Node 37: -0.03654742240905762\n",
      "Node 38: 7.033348083496094e-05\n",
      "Node 39: 1.1920928955078125e-07\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: -0.004094362258911133\n",
      "Node 43: 0.0\n",
      "Node 44: 0.005756735801696777\n",
      "Node 45: -1.1920928955078125e-06\n",
      "Node 46: 4.220008850097656e-05\n",
      "Node 47: 0.0003744363784790039\n",
      "Node 48: -0.013466715812683105\n",
      "Node 49: 0.0023827552795410156\n",
      "Node 50: 0.008487224578857422\n",
      "Node 51: -0.006354331970214844\n",
      "Node 52: 0.007519841194152832\n",
      "Node 53: -0.07084643840789795\n",
      "Node 54: -1.1920928955078125e-06\n",
      "Node 55: 0.0\n",
      "Node 56: -0.0018187761306762695\n",
      "Node 57: 0.000579833984375\n",
      "Node 58: 0.0006616115570068359\n",
      "Node 59: -0.00035703182220458984\n",
      "Node 60: 0.004546165466308594\n",
      "Node 61: -0.0027458667755126953\n",
      "Node 62: -0.0012902021408081055\n",
      "Node 63: 0.00834512710571289\n",
      "Node 64: 0.0010380744934082031\n",
      "Node 65: 0.0\n",
      "Node 66: -0.00705111026763916\n",
      "Node 67: -0.010856270790100098\n",
      "Node 68: 0.005580306053161621\n",
      "Node 69: -0.0019620656967163086\n",
      "Node 70: 0.006988406181335449\n",
      "Node 71: -0.10985243320465088\n",
      "Node 72: -0.022123336791992188\n",
      "Node 73: -0.0063784122467041016\n",
      "Node 74: 0.0024548768997192383\n",
      "Node 75: -0.1535269021987915\n",
      "Node 76: -0.008829951286315918\n",
      "Node 77: 3.743171691894531e-05\n",
      "Node 78: 0.011450648307800293\n",
      "Node 79: 0.0001150369644165039\n",
      "Node 80: 0.004777789115905762\n",
      "Node 81: -2.384185791015625e-07\n",
      "Node 82: 0.0\n",
      "Node 83: -5.841255187988281e-06\n",
      "Node 84: 0.001359701156616211\n",
      "Node 85: -7.987022399902344e-06\n",
      "Node 86: 0.0025240182876586914\n",
      "Node 87: -3.5762786865234375e-07\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0002847909927368164\n",
      "Node 90: 0.0007767677307128906\n",
      "Node 91: -0.0039212703704833984\n",
      "Node 92: -3.5762786865234375e-07\n",
      "Node 93: -0.06218063831329346\n",
      "Node 94: -0.006334662437438965\n",
      "Node 95: -0.019425272941589355\n",
      "Node 96: 0.0\n",
      "Node 97: 0.000655055046081543\n",
      "Node 98: 0.0\n",
      "Node 99: -0.0035638809204101562\n",
      "Node 100: 0.0\n",
      "Node 101: -0.04783987998962402\n",
      "Node 102: -0.06660258769989014\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: -0.010961413383483887\n",
      "Node 106: 0.00620114803314209\n",
      "Node 107: -0.034975290298461914\n",
      "Node 108: -0.05547201633453369\n",
      "Node 109: 0.0\n",
      "Node 110: -0.03499937057495117\n",
      "Node 111: -0.08441221714019775\n",
      "Node 112: 0.0\n",
      "Node 113: -0.0004379749298095703\n",
      "Node 114: -0.003833174705505371\n",
      "Node 115: 0.002426624298095703\n",
      "Node 116: 0.001537919044494629\n",
      "Node 117: -0.0005725622177124023\n",
      "Node 118: 0.0006412267684936523\n",
      "Node 119: -0.0014963150024414062\n",
      "Node 120: 0.0\n",
      "Node 121: -0.038939476013183594\n",
      "Node 122: -0.02510058879852295\n",
      "Node 123: -0.009965300559997559\n",
      "Node 124: 0.0\n",
      "Node 125: 7.581710815429688e-05\n",
      "Node 126: 0.002235889434814453\n",
      "Node 127: 0.0\n",
      "Node 0: 0.0\n",
      "Node 1: 0.0\n",
      "Node 2: 0.0\n",
      "Node 3: 0.0\n",
      "Node 4: -1.4571449756622314\n",
      "Node 5: 0.0\n",
      "Node 6: 0.0\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: 0.0\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: 0.0\n",
      "Node 17: 0.0\n",
      "Node 18: 0.0\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: 0.0\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: -2.6450798511505127\n",
      "Node 26: -0.28355729579925537\n",
      "Node 27: -1.3392333984375\n",
      "Node 28: 0.0\n",
      "Node 29: 0.0\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0\n",
      "Node 32: 0.0\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 0.0\n",
      "Node 36: 0.0\n",
      "Node 37: -1.7498373985290527\n",
      "Node 38: 0.0004483461380004883\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: -1.1809196472167969\n",
      "Node 45: 0.0\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: 0.0\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0\n",
      "Node 59: 0.0\n",
      "Node 60: 0.0\n",
      "Node 61: 0.0\n",
      "Node 62: 0.0\n",
      "Node 63: 0.0\n"
     ]
    }
   ],
   "source": [
    "l, a, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights = model.get_weights()\n",
    "weight_len = len(or_weights) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [128]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "\n",
    "#For-loop over convolutional layers\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[conv_len - (2*layer+1)][:, :, :, i] = 0\n",
    "        w[conv_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))\n",
    "\n",
    "#For-loop over dense layers\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 57\n",
      "Worse Nodes: 1\n",
      "Important Nodes: 6\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 38\n",
      "Worse Nodes: 45\n",
      "Important Nodes: 45\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 1.3278 - accuracy: 0.5847 - auc_4: 0.9126\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.5846999883651733 --- Loss: 1.3277618885040283 --- Change: -8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5846999883651733 --- Loss: 1.3277618885040283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5846999883651733 --- Loss: 1.3272415399551392 --- Change: 0.0003642439842224121 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827000141143799 --- Loss: 1.3232766389846802 --- Change: 0.002175438404083252 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5828999876976013 --- Loss: 1.3231372833251953 --- Change: 0.00015754103660583496 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.3188047409057617 --- Change: 0.002942782640457153 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.3188047409057617 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5824000239372253 --- Loss: 1.3178369998931885 --- Change: 0.0006174266338348388 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5824000239372253 --- Loss: 1.3178369998931885 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.005999547243118286 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5825999975204468 --- Loss: 1.309351921081543 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827000141143799 --- Loss: 1.3093243837356567 --- Change: 4.9281120300292966e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827000141143799 --- Loss: 1.3093243837356567 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827999711036682 --- Loss: 1.3092992305755615 --- Change: 4.7594308853149414e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5827999711036682 --- Loss: 1.3092992305755615 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5842999815940857 --- Loss: 1.3062597513198853 --- Change: 0.0025776386260986323 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.583899974822998 --- Loss: 1.3047467470169067 --- Change: 0.000939100980758667 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.583899974822998 --- Loss: 1.3047391176223755 --- Change: 5.340576171875e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2851483821868896 --- Change: 0.014253515005111694 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2851481437683105 --- Change: 1.6689300537109374e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2851481437683105 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5861999988555908 --- Loss: 1.2843248844146729 --- Change: 0.0007262885570526122 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5861999988555908 --- Loss: 1.2843255996704102 --- Change: -5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840553522109985 --- Change: 9.179115295410156e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840396165847778 --- Change: 1.1014938354492186e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856000185012817 --- Loss: 1.2840389013290405 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849000215530396 --- Loss: 1.2777714729309082 --- Change: 0.004177200794219971 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2777471542358398 --- Change: 0.0002570092678070068 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2777488231658936 --- Change: -1.1682510375976561e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5864999890327454 --- Loss: 1.2771539688110352 --- Change: 0.0006564021110534668 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5856999754905701 --- Loss: 1.2739813327789307 --- Change: 0.0019808411598205566 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867000222206116 --- Loss: 1.2724426984786987 --- Change: 0.0013770580291748045 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.26910400390625 --- Change: 0.002457088232040405 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2691041231155396 --- Change: -8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2691041231155396 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2688368558883667 --- Change: 0.00018708705902099608 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5878000259399414 --- Loss: 1.268312931060791 --- Change: 0.0005767464637756348 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867999792098999 --- Loss: 1.2667078971862793 --- Change: 0.0008235096931457519 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867999792098999 --- Loss: 1.2667078971862793 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5874000191688538 --- Loss: 1.2650362253189087 --- Change: 0.0013501822948455808 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5871000289916992 --- Loss: 1.2625014781951904 --- Change: 0.0016843259334564206 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5873000025749207 --- Loss: 1.2625792026519775 --- Change: 5.584955215454104e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5873000025749207 --- Loss: 1.2622408866882324 --- Change: 0.00023682117462158202 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5873000025749207 --- Loss: 1.2622110843658447 --- Change: 2.086162567138672e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5867999792098999 --- Loss: 1.2590197324752808 --- Change: 0.0020839393138885496 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.258271336555481 --- Change: 7.389187812805169e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.2582714557647705 --- Change: -8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.2582714557647705 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5853000283241272 --- Loss: 1.258270263671875 --- Change: 8.344650268554688e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2572377920150757 --- Change: 0.0007827222347259521 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2572380304336548 --- Change: -1.6689300537109374e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2572380304336548 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.257238507270813 --- Change: -3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.257238507270813 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.252395749092102 --- Change: 0.0035099327564239503 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: -1.5854835510253905e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5859000086784363 --- Loss: 1.2523980140686035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5843999981880188 --- Loss: 1.2504163980484009 --- Change: 0.0009371280670166014 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5828999876976013 --- Loss: 1.2496907711029053 --- Change: 5.793571472167969e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5843999981880188 --- Loss: 1.2493844032287598 --- Change: 0.0006644606590270996 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5845999717712402 --- Loss: 1.248937726020813 --- Change: 0.00037266612052917476 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849999785423279 --- Loss: 1.2491222620010376 --- Change: -9.173154830932609e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849999785423279 --- Loss: 1.2491222620010376 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5849999785423279 --- Loss: 1.2491222620010376 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0006455957889556884 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2484142780303955 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 4.072189331054687e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.5855000019073486 --- Loss: 1.2483561038970947 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model.get_weights()\n",
    "tol = -1e-5\n",
    "layer_sizes = [64]\n",
    "conv_layer_sizes = [128]\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights4 = model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts3 = []\n",
    "places3 = []\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights4)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights4[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights4[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts3.append(num_removed2)\n",
    "    places3.append(nodes_removed2)\n",
    "\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights4)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na, nauc = tester_model.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights4[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights4[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts3.append(num_removed2)\n",
    "    places3.append(nodes_removed2)\n",
    "\n",
    "tester_model.set_weights(best_weights4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 1.2484 - accuracy: 0.5855 - auc_5: 0.9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2483556270599365, 0.5855000019073486, 0.9177575707435608]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.638, 0.642, 0.434, 0.404, 0.582, 0.413, 0.676, 0.621, 0.741, 0.696]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.643, 0.746, 0.381, 0.357, 0.59, 0.45, 0.709, 0.644, 0.689, 0.646]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76, 58]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "tester_model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4227 - accuracy: 0.4825 - auc: 0.8822 - val_loss: 1.0835 - val_accuracy: 0.6148 - val_auc: 0.9332\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9401 - accuracy: 0.6679 - auc: 0.9495 - val_loss: 0.8804 - val_accuracy: 0.6925 - val_auc: 0.9551\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7424 - accuracy: 0.7390 - auc: 0.9678 - val_loss: 0.8214 - val_accuracy: 0.7187 - val_auc: 0.9603\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6091 - accuracy: 0.7856 - auc: 0.9778 - val_loss: 0.8153 - val_accuracy: 0.7184 - val_auc: 0.9614\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5001 - accuracy: 0.8245 - auc: 0.9846 - val_loss: 0.7937 - val_accuracy: 0.7363 - val_auc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23bfa1a5788>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7937 - accuracy: 0.7363 - auc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7937123775482178, 0.736299991607666, 0.9630627632141113]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 0.7811 - accuracy: 0.7343 - auc: 0.9641\n",
      "Node 0: -0.09342378377914429\n",
      "Node 1: 0.0\n",
      "Node 2: -0.028251349925994873\n",
      "Node 3: 0.002001166343688965\n",
      "Node 4: 0.0\n",
      "Node 5: -0.0011225342750549316\n",
      "Node 6: -0.05288398265838623\n",
      "Node 7: 0.0002676248550415039\n",
      "Node 8: 0.0\n",
      "Node 9: -0.0004940032958984375\n",
      "Node 10: 0.007366597652435303\n",
      "Node 11: 0.0\n",
      "Node 12: 0.0\n",
      "Node 13: -0.05894893407821655\n",
      "Node 14: 0.0\n",
      "Node 15: 0.0\n",
      "Node 16: -0.11095565557479858\n",
      "Node 17: -0.006670236587524414\n",
      "Node 18: 8.64267349243164e-06\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0\n",
      "Node 21: -0.07053190469741821\n",
      "Node 22: 0.0\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0004349350929260254\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: -0.014485180377960205\n",
      "Node 28: -0.19459402561187744\n",
      "Node 29: 0.0\n",
      "Node 30: 0.0\n",
      "Node 31: -0.003442823886871338\n",
      "Node 32: 0.0\n",
      "Node 33: 2.384185791015625e-06\n",
      "Node 34: 0.007597565650939941\n",
      "Node 35: -0.06364792585372925\n",
      "Node 36: 0.0\n",
      "Node 37: -0.1290287971496582\n",
      "Node 38: 0.0\n",
      "Node 39: -1.6093254089355469e-06\n",
      "Node 40: -0.005423784255981445\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: -2.5033950805664062e-06\n",
      "Node 45: -0.06784862279891968\n",
      "Node 46: 0.0\n",
      "Node 47: -0.1245458722114563\n",
      "Node 48: -1.6808509826660156e-05\n",
      "Node 49: 0.0\n",
      "Node 50: 0.004028260707855225\n",
      "Node 51: -0.04378163814544678\n",
      "Node 52: 0.0\n",
      "Node 53: 0.0\n",
      "Node 54: 0.0\n",
      "Node 55: 0.0003241300582885742\n",
      "Node 56: 0.0\n",
      "Node 57: -0.09989100694656372\n",
      "Node 58: 0.0\n",
      "Node 59: 0.0\n",
      "Node 60: 0.0\n",
      "Node 61: 0.0\n",
      "Node 62: 0.0\n",
      "Node 63: 0.0\n",
      "Node 0: -0.0011832118034362793\n",
      "Node 1: -0.0007817745208740234\n",
      "Node 2: -0.0016353130340576172\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: -0.0017101168632507324\n",
      "Node 6: -1.0848045349121094e-05\n",
      "Node 7: -0.0030570030212402344\n",
      "Node 8: 0.0004990100860595703\n",
      "Node 9: -3.0100345611572266e-05\n",
      "Node 10: -1.6093254089355469e-06\n",
      "Node 11: -0.00033992528915405273\n",
      "Node 12: 0.0021581053733825684\n",
      "Node 13: 0.0\n",
      "Node 14: -0.002244293689727783\n",
      "Node 15: 0.0\n",
      "Node 16: -0.001527547836303711\n",
      "Node 17: -0.00018721818923950195\n",
      "Node 18: 0.002030014991760254\n",
      "Node 19: -0.0005434751510620117\n",
      "Node 20: 0.0001800060272216797\n",
      "Node 21: -0.00012367963790893555\n",
      "Node 22: -6.258487701416016e-05\n",
      "Node 23: 0.0\n",
      "Node 24: -0.0012730956077575684\n",
      "Node 25: -0.002358555793762207\n",
      "Node 26: -6.622076034545898e-05\n",
      "Node 27: 0.002368032932281494\n",
      "Node 28: 0.0\n",
      "Node 29: 0.0\n",
      "Node 30: 0.0006042718887329102\n",
      "Node 31: 0.00021213293075561523\n",
      "Node 32: -0.0016361474990844727\n",
      "Node 33: -0.0019237995147705078\n",
      "Node 34: 0.0\n",
      "Node 35: -0.0011538267135620117\n",
      "Node 36: -5.960464477539063e-08\n",
      "Node 37: -0.002699613571166992\n",
      "Node 38: -0.0014051198959350586\n",
      "Node 39: 0.0\n",
      "Node 40: 0.0\n",
      "Node 41: 0.0031224489212036133\n",
      "Node 42: 0.0017426013946533203\n",
      "Node 43: 0.0005015134811401367\n",
      "Node 44: 0.0032544732093811035\n",
      "Node 45: 0.001054227352142334\n",
      "Node 46: -0.01096254587173462\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.006660521030426025\n",
      "Node 50: -5.739927291870117e-05\n",
      "Node 51: 0.0\n",
      "Node 52: 0.0\n",
      "Node 53: -9.882450103759766e-05\n",
      "Node 54: 0.0\n",
      "Node 55: -0.002787768840789795\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: 0.0\n",
      "Node 59: -4.76837158203125e-07\n",
      "Node 60: -0.0042803287506103516\n",
      "Node 61: 0.0\n",
      "Node 62: 0.0004691481590270996\n",
      "Node 63: 0.0\n",
      "Node 64: 0.0\n",
      "Node 65: 0.0030063986778259277\n",
      "Node 66: -0.0010252594947814941\n",
      "Node 67: -0.000905454158782959\n",
      "Node 68: 0.0\n",
      "Node 69: -0.0006330013275146484\n",
      "Node 70: -0.0019286274909973145\n",
      "Node 71: 0.0\n",
      "Node 72: 0.0\n",
      "Node 73: -0.001710355281829834\n",
      "Node 74: -0.001181185245513916\n",
      "Node 75: 0.00034677982330322266\n",
      "Node 76: -0.0007477402687072754\n",
      "Node 77: 0.0\n",
      "Node 78: -0.0017963647842407227\n",
      "Node 79: 0.0\n",
      "Node 80: 0.0\n",
      "Node 81: 0.001062154769897461\n",
      "Node 82: 0.0\n",
      "Node 83: 6.830692291259766e-05\n",
      "Node 84: 0.0\n",
      "Node 85: 0.0\n",
      "Node 86: -0.0010530352592468262\n",
      "Node 87: 0.004005312919616699\n",
      "Node 88: -7.861852645874023e-05\n",
      "Node 89: 0.0\n",
      "Node 90: 0.0\n",
      "Node 91: 0.0\n",
      "Node 92: 0.00014281272888183594\n",
      "Node 93: -0.004969537258148193\n",
      "Node 94: -0.00017243623733520508\n",
      "Node 95: -0.00037592649459838867\n",
      "Node 96: 0.0\n",
      "Node 97: 0.0\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0011914372444152832\n",
      "Node 100: 0.004390537738800049\n",
      "Node 101: 0.0022571682929992676\n",
      "Node 102: 1.5497207641601562e-06\n",
      "Node 103: 0.004755616188049316\n",
      "Node 104: 0.003926992416381836\n",
      "Node 105: 6.556510925292969e-05\n",
      "Node 106: 0.0\n",
      "Node 107: 0.0\n",
      "Node 108: 0.00037729740142822266\n",
      "Node 109: 0.0\n",
      "Node 110: -0.003355085849761963\n",
      "Node 111: 0.00051116943359375\n",
      "Node 112: 0.0008373260498046875\n",
      "Node 113: 0.0028777122497558594\n",
      "Node 114: 0.0010794997215270996\n",
      "Node 115: 0.0014827847480773926\n",
      "Node 116: 0.0\n",
      "Node 117: 5.960464477539063e-08\n",
      "Node 118: 0.0\n",
      "Node 119: 0.0018159747123718262\n",
      "Node 120: 0.0\n",
      "Node 121: 0.0013872385025024414\n",
      "Node 122: -0.0007803440093994141\n",
      "Node 123: -6.496906280517578e-06\n",
      "Node 124: 0.001766979694366455\n",
      "Node 125: 0.0\n",
      "Node 126: 0.0009725689888000488\n",
      "Node 127: 0.0\n",
      "Node 128: 0.0\n",
      "Node 129: 0.0\n",
      "Node 130: 0.0\n",
      "Node 131: -0.002733170986175537\n",
      "Node 132: 0.0\n",
      "Node 133: 0.0026911497116088867\n",
      "Node 134: 0.0028418898582458496\n",
      "Node 135: 0.0009124279022216797\n",
      "Node 136: 0.0\n",
      "Node 137: -0.002158641815185547\n",
      "Node 138: 0.0\n",
      "Node 139: -0.0016410350799560547\n",
      "Node 140: 0.0\n",
      "Node 141: -0.0006131529808044434\n",
      "Node 142: 0.0016235113143920898\n",
      "Node 143: -0.0012078285217285156\n",
      "Node 144: 0.0\n",
      "Node 145: 0.0009078383445739746\n",
      "Node 146: 0.0007505416870117188\n",
      "Node 147: -0.0012301802635192871\n",
      "Node 148: 0.0007990002632141113\n",
      "Node 149: 0.0\n",
      "Node 150: -4.011392593383789e-05\n",
      "Node 151: 0.0\n",
      "Node 152: -0.0019172430038452148\n",
      "Node 153: -0.00045049190521240234\n",
      "Node 154: 0.0\n",
      "Node 155: 0.00045377016067504883\n",
      "Node 156: -0.002457559108734131\n",
      "Node 157: 0.002427995204925537\n",
      "Node 158: -0.0014167428016662598\n",
      "Node 159: -0.0008376240730285645\n",
      "Node 160: 0.002523064613342285\n",
      "Node 161: -0.003653407096862793\n",
      "Node 162: 0.0006281137466430664\n",
      "Node 163: 0.0\n",
      "Node 164: 0.0028513669967651367\n",
      "Node 165: 6.854534149169922e-06\n",
      "Node 166: 0.002839982509613037\n",
      "Node 167: 0.0\n",
      "Node 168: 0.0012357234954833984\n",
      "Node 169: 0.0\n",
      "Node 170: 0.0\n",
      "Node 171: 0.0\n",
      "Node 172: -8.70823860168457e-05\n",
      "Node 173: 0.001216888427734375\n",
      "Node 174: -0.0002803206443786621\n",
      "Node 175: -0.002488255500793457\n",
      "Node 176: -0.000319063663482666\n",
      "Node 177: 0.00010037422180175781\n",
      "Node 178: 0.007264554500579834\n",
      "Node 179: -0.0006480216979980469\n",
      "Node 180: 0.0005881190299987793\n",
      "Node 181: 0.0\n",
      "Node 182: 0.001077890396118164\n",
      "Node 183: -6.771087646484375e-05\n",
      "Node 184: -0.003358125686645508\n",
      "Node 185: 0.0004399418830871582\n",
      "Node 186: -0.0014476776123046875\n",
      "Node 187: 0.0016950368881225586\n",
      "Node 188: 0.008881866931915283\n",
      "Node 189: 0.005012333393096924\n",
      "Node 190: 0.0\n",
      "Node 191: 0.0006405115127563477\n",
      "Node 192: 0.0\n",
      "Node 193: -0.0009236931800842285\n",
      "Node 194: 0.0\n",
      "Node 195: 0.0\n",
      "Node 196: 0.002062380313873291\n",
      "Node 197: -0.0011777877807617188\n",
      "Node 198: 0.0\n",
      "Node 199: -0.0018342137336730957\n",
      "Node 200: 0.0\n",
      "Node 201: 0.004102945327758789\n",
      "Node 202: 0.0\n",
      "Node 203: -0.003361225128173828\n",
      "Node 204: -0.005140542984008789\n",
      "Node 205: -0.004472672939300537\n",
      "Node 206: -0.001020193099975586\n",
      "Node 207: 0.0\n",
      "Node 208: 0.0020776987075805664\n",
      "Node 209: 0.00038880109786987305\n",
      "Node 210: -0.0018411874771118164\n",
      "Node 211: 0.0006371140480041504\n",
      "Node 212: -0.00022077560424804688\n",
      "Node 213: -0.0003802776336669922\n",
      "Node 214: 0.0005428791046142578\n",
      "Node 215: 0.0001341700553894043\n",
      "Node 216: 0.0\n",
      "Node 217: 0.0022757649421691895\n",
      "Node 218: 0.00033104419708251953\n",
      "Node 219: 0.0011749863624572754\n",
      "Node 220: -0.0015526413917541504\n",
      "Node 221: -0.0014807581901550293\n",
      "Node 222: -0.001296401023864746\n",
      "Node 223: 0.0\n",
      "Node 224: -0.0006327033042907715\n",
      "Node 225: 0.0\n",
      "Node 226: -0.0008823871612548828\n",
      "Node 227: 0.0002757906913757324\n",
      "Node 228: 0.0\n",
      "Node 229: -0.0017442107200622559\n",
      "Node 230: -0.004848599433898926\n",
      "Node 231: 0.0016893744468688965\n",
      "Node 232: -0.0013889074325561523\n",
      "Node 233: 0.004427433013916016\n",
      "Node 234: -0.0016086697578430176\n",
      "Node 235: -0.0010817646980285645\n",
      "Node 236: 0.0\n",
      "Node 237: -0.00022274255752563477\n",
      "Node 238: -0.0011987090110778809\n",
      "Node 239: 5.5849552154541016e-05\n",
      "Node 240: 0.002000153064727783\n",
      "Node 241: 0.0\n",
      "Node 242: 0.0\n",
      "Node 243: -0.004965484142303467\n",
      "Node 244: -0.002488076686859131\n",
      "Node 245: 0.004338860511779785\n",
      "Node 246: 0.0\n",
      "Node 247: -8.821487426757812e-06\n",
      "Node 248: 0.0\n",
      "Node 249: -0.000559389591217041\n",
      "Node 250: -0.0015348196029663086\n",
      "Node 251: -0.003906548023223877\n",
      "Node 252: -1.901388168334961e-05\n",
      "Node 253: -0.002400338649749756\n",
      "Node 254: -5.960464477539063e-08\n",
      "Node 255: -0.0018262267112731934\n",
      "Node 0: -0.003408372402191162\n",
      "Node 1: 0.00011903047561645508\n",
      "Node 2: 0.0029645562171936035\n",
      "Node 3: -0.024524688720703125\n",
      "Node 4: -0.0007654428482055664\n",
      "Node 5: -0.007897555828094482\n",
      "Node 6: 0.00022453069686889648\n",
      "Node 7: -0.009954392910003662\n",
      "Node 8: -0.006294667720794678\n",
      "Node 9: 0.0\n",
      "Node 10: 0.00018793344497680664\n",
      "Node 11: -4.082918167114258e-05\n",
      "Node 12: -0.0028128623962402344\n",
      "Node 13: 0.0030063986778259277\n",
      "Node 14: 0.0\n",
      "Node 15: 0.003234386444091797\n",
      "Node 16: -0.004235386848449707\n",
      "Node 17: -0.002466142177581787\n",
      "Node 18: 0.0\n",
      "Node 19: 0.003631770610809326\n",
      "Node 20: 0.006082415580749512\n",
      "Node 21: -0.0035338401794433594\n",
      "Node 22: -0.0039708614349365234\n",
      "Node 23: 0.0\n",
      "Node 24: -0.005543172359466553\n",
      "Node 25: -0.007281839847564697\n",
      "Node 26: -0.006570398807525635\n",
      "Node 27: 0.0\n",
      "Node 28: -0.007807135581970215\n",
      "Node 29: -0.012219548225402832\n",
      "Node 30: 0.0\n",
      "Node 31: -0.002389848232269287\n",
      "Node 32: -0.004650592803955078\n",
      "Node 33: -0.0034720301628112793\n",
      "Node 34: -0.022769451141357422\n",
      "Node 35: -0.0006483793258666992\n",
      "Node 36: -0.002050161361694336\n",
      "Node 37: -0.004272758960723877\n",
      "Node 38: 0.0013520121574401855\n",
      "Node 39: 0.0\n",
      "Node 40: -0.01690465211868286\n",
      "Node 41: -0.0014935135841369629\n",
      "Node 42: -0.0030421018600463867\n",
      "Node 43: -0.011236250400543213\n",
      "Node 44: 2.181529998779297e-05\n",
      "Node 45: 0.0\n",
      "Node 46: 0.0\n",
      "Node 47: 0.0\n",
      "Node 48: -0.005006015300750732\n",
      "Node 49: 0.0015709400177001953\n",
      "Node 50: 0.0\n",
      "Node 51: 0.005516767501831055\n",
      "Node 52: -0.010618805885314941\n",
      "Node 53: -0.002821207046508789\n",
      "Node 54: -0.004031062126159668\n",
      "Node 55: 0.0011596083641052246\n",
      "Node 56: 0.0\n",
      "Node 57: 0.0\n",
      "Node 58: -0.016306579113006592\n",
      "Node 59: 0.009589076042175293\n",
      "Node 60: -0.0013456344604492188\n",
      "Node 61: 7.963180541992188e-05\n",
      "Node 62: 0.0\n",
      "Node 63: -0.011187732219696045\n",
      "Node 64: -0.008125364780426025\n",
      "Node 65: -0.015600979328155518\n",
      "Node 66: -0.00855177640914917\n",
      "Node 67: 0.0024914145469665527\n",
      "Node 68: -0.01131218671798706\n",
      "Node 69: -0.0047844648361206055\n",
      "Node 70: 0.0\n",
      "Node 71: -0.0015515685081481934\n",
      "Node 72: 0.0\n",
      "Node 73: -0.0006588101387023926\n",
      "Node 74: 0.0\n",
      "Node 75: -0.00010693073272705078\n",
      "Node 76: 0.000864565372467041\n",
      "Node 77: 0.002782464027404785\n",
      "Node 78: -4.172325134277344e-07\n",
      "Node 79: -0.002331674098968506\n",
      "Node 80: -2.980232238769531e-07\n",
      "Node 81: 0.001099228858947754\n",
      "Node 82: -0.009340822696685791\n",
      "Node 83: -1.5974044799804688e-05\n",
      "Node 84: 0.0\n",
      "Node 85: -0.002823173999786377\n",
      "Node 86: -0.00954735279083252\n",
      "Node 87: 0.006717205047607422\n",
      "Node 88: 0.008302390575408936\n",
      "Node 89: -0.0028383731842041016\n",
      "Node 90: -0.0040808916091918945\n",
      "Node 91: -0.001324474811553955\n",
      "Node 92: -0.0021865367889404297\n",
      "Node 93: -0.0057874321937561035\n",
      "Node 94: 0.0027025341987609863\n",
      "Node 95: -0.004040777683258057\n",
      "Node 96: -0.002893984317779541\n",
      "Node 97: -2.4437904357910156e-06\n",
      "Node 98: -0.0005811452865600586\n",
      "Node 99: 0.006788074970245361\n",
      "Node 100: -0.0013592839241027832\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0012163519859313965\n",
      "Node 103: 0.0\n",
      "Node 104: 0.0\n",
      "Node 105: -0.001060187816619873\n",
      "Node 106: -0.004301786422729492\n",
      "Node 107: -0.002975285053253174\n",
      "Node 108: 0.00017952919006347656\n",
      "Node 109: 0.002549886703491211\n",
      "Node 110: 0.0\n",
      "Node 111: 0.0029854774475097656\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: -0.007471919059753418\n",
      "Node 115: -0.0009251236915588379\n",
      "Node 116: -0.001635432243347168\n",
      "Node 117: 0.0\n",
      "Node 118: 0.0\n",
      "Node 119: -0.011283457279205322\n",
      "Node 120: -0.008332967758178711\n",
      "Node 121: 0.0\n",
      "Node 122: -0.008440256118774414\n",
      "Node 123: -0.015561282634735107\n",
      "Node 124: 0.004235029220581055\n",
      "Node 125: 0.0\n",
      "Node 126: -0.01666170358657837\n",
      "Node 127: 0.0011851191520690918\n",
      "Node 0: 0.0014824867248535156\n",
      "Node 1: 0.0\n",
      "Node 2: 0.0\n",
      "Node 3: -0.013020098209381104\n",
      "Node 4: -0.015917956829071045\n",
      "Node 5: -0.007037937641143799\n",
      "Node 6: -0.06582456827163696\n",
      "Node 7: -0.004909038543701172\n",
      "Node 8: 0.0039081573486328125\n",
      "Node 9: -0.06661748886108398\n",
      "Node 10: 0.00020271539688110352\n",
      "Node 11: -0.00028246641159057617\n",
      "Node 12: -0.0064966678619384766\n",
      "Node 13: -0.007930934429168701\n",
      "Node 14: -0.013394355773925781\n",
      "Node 15: -0.022588372230529785\n",
      "Node 16: 0.0004197359085083008\n",
      "Node 17: 0.0026779770851135254\n",
      "Node 18: -0.011873781681060791\n",
      "Node 19: 0.0015709400177001953\n",
      "Node 20: 0.0\n",
      "Node 21: 0.002709329128265381\n",
      "Node 22: -0.0026035308837890625\n",
      "Node 23: -0.010142266750335693\n",
      "Node 24: 0.0015599727630615234\n",
      "Node 25: 0.001520395278930664\n",
      "Node 26: -0.005089223384857178\n",
      "Node 27: -0.0030251145362854004\n",
      "Node 28: -0.02127021551132202\n",
      "Node 29: -0.0014843344688415527\n",
      "Node 30: -0.02094435691833496\n",
      "Node 31: -0.0040375590324401855\n",
      "Node 32: 0.003252387046813965\n",
      "Node 33: 0.003925323486328125\n",
      "Node 34: -0.009962737560272217\n",
      "Node 35: -0.00013953447341918945\n",
      "Node 36: 0.0\n",
      "Node 37: -0.006748318672180176\n",
      "Node 38: -0.01592564582824707\n",
      "Node 39: -0.0005141496658325195\n",
      "Node 40: -0.014619946479797363\n",
      "Node 41: -0.009571313858032227\n",
      "Node 42: 0.0\n",
      "Node 43: -0.018112778663635254\n",
      "Node 44: 0.0\n",
      "Node 45: 0.0\n",
      "Node 46: -0.007613778114318848\n",
      "Node 47: 0.006222963333129883\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0050220489501953125\n",
      "Node 50: 0.0014548301696777344\n",
      "Node 51: -0.004376113414764404\n",
      "Node 52: 0.0003464221954345703\n",
      "Node 53: 0.006574809551239014\n",
      "Node 54: 0.0\n",
      "Node 55: 0.000512540340423584\n",
      "Node 56: 0.0\n",
      "Node 57: 0.007804274559020996\n",
      "Node 58: 0.0\n",
      "Node 59: -7.838010787963867e-05\n",
      "Node 60: 0.002122938632965088\n",
      "Node 61: -0.0005104541778564453\n",
      "Node 62: -0.015042245388031006\n",
      "Node 63: -0.0424613356590271\n",
      "Node 0: -0.016811728477478027\n",
      "Node 1: 0.0010405778884887695\n",
      "Node 2: -0.006476342678070068\n",
      "Node 3: -0.007506310939788818\n",
      "Node 4: -0.018420815467834473\n",
      "Node 5: 0.001157522201538086\n",
      "Node 6: -0.008889913558959961\n",
      "Node 7: -0.024925827980041504\n",
      "Node 8: -0.0014365315437316895\n",
      "Node 9: -0.003457486629486084\n",
      "Node 10: -8.100271224975586e-05\n",
      "Node 11: 0.00035184621810913086\n",
      "Node 12: -0.008551478385925293\n",
      "Node 13: -0.015579700469970703\n",
      "Node 14: -0.03586983680725098\n",
      "Node 15: -0.1603413224220276\n",
      "Node 16: -0.0361943244934082\n",
      "Node 17: -0.009172618389129639\n",
      "Node 18: -0.051997244358062744\n",
      "Node 19: -0.0025857090950012207\n",
      "Node 20: -0.004125475883483887\n",
      "Node 21: -0.0690232515335083\n",
      "Node 22: -0.01083838939666748\n",
      "Node 23: 2.1278858184814453e-05\n",
      "Node 24: 0.0\n",
      "Node 25: -0.016379833221435547\n",
      "Node 26: -0.005203902721405029\n",
      "Node 27: -0.004020273685455322\n",
      "Node 28: -0.13005781173706055\n",
      "Node 29: -0.004964172840118408\n",
      "Node 30: -9.232759475708008e-05\n",
      "Node 31: 0.0020343661308288574\n"
     ]
    }
   ],
   "source": [
    "l, a, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights2 = model2.get_weights()\n",
    "weight_len = len(or_weights2) - 3\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "amounts = []\n",
    "places = []\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "\n",
    "#For-loop over dense layers\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights2)\n",
    "        w[weight_len - (2*layer+1)][:,i] = 0\n",
    "        w[weight_len - 2*layer][i] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))\n",
    "    \n",
    "#For-loop over convolutional layers\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights2)\n",
    "        w[conv_len - (2*layer+1)][:, :, :, i] = 0\n",
    "        w[conv_len - 2*layer][i] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 0.*(na - a) + 1.*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]\n",
    "    amounts.append((num_zeros, num_worse, num_important))\n",
    "    places.append((z, wr, imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### LAYER 0 #########\n",
      "Zero Nodes: 1\n",
      "Worse Nodes: 5\n",
      "Important Nodes: 26\n",
      "######### LAYER 1 #########\n",
      "Zero Nodes: 11\n",
      "Worse Nodes: 19\n",
      "Important Nodes: 34\n",
      "######### LAYER 2 #########\n",
      "Zero Nodes: 31\n",
      "Worse Nodes: 29\n",
      "Important Nodes: 68\n",
      "######### LAYER 3 #########\n",
      "Zero Nodes: 87\n",
      "Worse Nodes: 76\n",
      "Important Nodes: 93\n",
      "######### LAYER 4 #########\n",
      "Zero Nodes: 37\n",
      "Worse Nodes: 7\n",
      "Important Nodes: 20\n"
     ]
    }
   ],
   "source": [
    "for i, (nz, nw, ni) in enumerate(reversed(amounts)):\n",
    "    print(f'######### LAYER {i} #########')\n",
    "    print(\"Zero Nodes:\", nz)\n",
    "    print(\"Worse Nodes:\", nw)\n",
    "    print(\"Important Nodes:\", ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7811 - accuracy: 0.7343 - auc: 0.9641\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7811414003372192 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7791403532028198 --- Change: 0.0012207210063934326 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7791403532028198 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7333999872207642 --- Loss: 0.7788729667663574 --- Change: 9.717345237731933e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7333999872207642 --- Loss: 0.7788729667663574 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7716400623321533 --- Change: 0.0053630292415618895 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.001049727201461792 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7700976133346558 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 3.808140754699707e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700860500335693 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7700837254524231 --- Change: 1.6272068023681639e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688760161399841 --- Change: 0.0006654024124145508 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688760161399841 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688760161399841 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: -1.043081283569336e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688775062561035 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688751220703125 --- Change: 1.6689300537109375e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688751220703125 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688727378845215 --- Change: 1.6689300537109375e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7337999939918518 --- Loss: 0.7688727378845215 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0005234420299530029 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7676106691360474 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0003397107124328613 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671253681182861 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7671268582344055 --- Change: -1.043081283569336e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7325999736785889 --- Loss: 0.7661057114601135 --- Change: 0.0007148027420043945 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7322999835014343 --- Loss: 0.7657871842384338 --- Change: 0.00013297200202941892 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7322999835014343 --- Loss: 0.7657871842384338 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7322999835014343 --- Loss: 0.7657871842384338 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7644184827804565 --- Change: 0.001558101177215576 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.736299991607666 --- Loss: 0.7636383771896362 --- Change: 0.0011460661888122556 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0014307498931884766 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7607801556587219 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7329999804496765 --- Loss: 0.7599818706512451 --- Change: 0.00013880133628845208 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7330999970436096 --- Loss: 0.7598163485527039 --- Change: 0.00014587044715881346 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.732699990272522 --- Loss: 0.7596465945243835 --- Change: -1.174211502075198e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7335000038146973 --- Loss: 0.7598873376846313 --- Change: 7.148385047912599e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7335000038146973 --- Loss: 0.7598873376846313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7335000038146973 --- Loss: 0.7598873376846313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7333999872207642 --- Loss: 0.7562488317489624 --- Change: 0.0025169491767883297 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.00037263631820678707 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561450600624084 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: -1.6689300537109374e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7561452984809875 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7551780939102173 --- Change: 0.0004670441150665283 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336000204086304 --- Loss: 0.7549936771392822 --- Change: 9.910464286804198e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7547692656517029 --- Change: 0.000187075138092041 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7336999773979187 --- Loss: 0.7547692656517029 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0007869541645050049 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7347999811172485 --- Loss: 0.7541164755821228 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.001280355453491211 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7340999841690063 --- Loss: 0.7519873976707458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 9.145736694335937e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343999743461609 --- Loss: 0.7519853115081787 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7343000173568726 --- Loss: 0.7516787648200989 --- Change: 0.00018459558486938477 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7353000044822693 --- Loss: 0.751879870891571 --- Change: 0.00015922188758850096 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7371000051498413 --- Loss: 0.7502419948577881 --- Change: 0.0016865134239196777 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.737500011920929 --- Loss: 0.75013267993927 --- Change: 0.00019652247428894042 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.737500011920929 --- Loss: 0.7501348257064819 --- Change: -1.5020370483398437e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 5.632638931274414e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.738099992275238 --- Loss: 0.7503839135169983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7383999824523926 --- Loss: 0.7503913044929504 --- Change: 8.482336997985839e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 4.27842140197754e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.739300012588501 --- Loss: 0.7507159113883972 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394000291824341 --- Loss: 0.7506734132766724 --- Change: 5.97536563873291e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394000291824341 --- Loss: 0.7506734132766724 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 3.303289413452148e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7506690621376038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7394999861717224 --- Loss: 0.7504774928092957 --- Change: 0.0001340985298156738 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0004308104515075683 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7404000163078308 --- Loss: 0.7502477765083313 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7401999831199646 --- Loss: 0.7500375509262085 --- Change: 8.714795112609861e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7398999929428101 --- Loss: 0.7498258352279663 --- Change: 5.820393562316893e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7398999929428101 --- Loss: 0.7498258352279663 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7400000095367432 --- Loss: 0.7498515248298645 --- Change: 1.2022256851196288e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 2.4086236953735373e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502885460853577 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502560019493103 --- Change: 2.2780895233154296e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7502560019493103 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 7.105469703674316e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501544952392578 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 5.757808685302734e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.741100013256073 --- Loss: 0.7501462697982788 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7415000200271606 --- Loss: 0.7496580481529236 --- Change: 0.00046175718307495117 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0002638101577758789 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493668794631958 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493680715560913 --- Change: -8.344650268554688e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7416999936103821 --- Loss: 0.7493683695793152 --- Change: -2.086162567138672e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493864297866821 --- Change: 1.7362833023071286e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493864297866821 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: -3.880262374877929e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493919730186462 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 1.8942356109619138e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7421000003814697 --- Loss: 0.7494934797286987 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493222951889038 --- Change: 2.9832124710083008e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7418000102043152 --- Loss: 0.7493222951889038 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_weights4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-efc1a00f7d00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mplaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_removed2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mtester_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_weights4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_weights4' is not defined"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "tol = -1e-5\n",
    "layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "for layer, size in enumerate(layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.3*(na - oa) + 0.7*(ol - nl) >= tol:\n",
    "            best_change = 0.3*(na - oa) + 0.7*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.806, 0.92, 0.672, 0.477, 0.694, 0.608, 0.772, 0.805, 0.852, 0.737]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.777, 0.837, 0.626, 0.467, 0.751, 0.67, 0.812, 0.803, 0.867, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7744 - accuracy: 0.7343 - auc: 0.9650\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.013817846107482955 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536677002906799 --- Change: 0.05909848084449766 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 1.3107061386108399e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0008232743263244635 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.897 --- Loss: 1.1212269067764282 --- Change: 0.048295856666564954 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.121446132659912 --- Change: 0.0041342322349548375 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.006408405303955078 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.902 --- Loss: 1.0952787399291992 --- Change: 0.0007418125152587884 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0015961946487426768 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0027626694679260266 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.907 --- Loss: 1.0889948606491089 --- Change: 0.001026299667358399 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.908 --- Loss: 1.088670253753662 --- Change: 0.0007973820686340337 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0902987718582153 --- Change: 0.0009114445686340343 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0899895429611206 --- Change: 9.276866912841796e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0030559185028076192 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.090509295463562 --- Change: 0.0002881557464599615 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.0894718170166016 --- Change: 0.0003112435340881348 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0015142022132873523 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.917 --- Loss: 1.0784984827041626 --- Change: 0.003177798080444338 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.00047988004684448296 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0002508909225463873 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.08073091506958 --- Change: -5.006790161132813e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.92 --- Loss: 1.0827049016952515 --- Change: 0.00010780401229858454 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0836422443389893 --- Change: 0.00041879720687866263 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0834838151931763 --- Change: 4.752874374389648e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0004411697387695312 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0808788537979126 --- Change: 0.0003403186798095703 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0007631720542907726 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.926 --- Loss: 1.085551381111145 --- Change: 0.0013350697517395035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0872161388397217 --- Change: 0.0002005726814270025 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.000680989170074464 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 1.7881393432617188e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0895538330078125 --- Change: 1.7523765563964844e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0848712921142578 --- Change: 4.76226806640518e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0842589139938354 --- Change: 0.00018371343612670897 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0870466232299805 --- Change: 0.0005636872291564952 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.931 --- Loss: 1.0885720252990723 --- Change: 0.0009423793792724621 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.933 --- Loss: 1.0920823812484741 --- Change: 0.0003468932151794444 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0923717021942139 --- Change: 0.0006132037162780767 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0916045904159546 --- Change: 0.0002301335334777832 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.095198631286621 --- Change: 0.00032178773880005003 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: -5.0067901611328125e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0005976676940917968 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0006748795509338379 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: -1.430511474609375e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909996032714844 --- Change: -7.68899917602539e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 3.247261047363281e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.935 --- Loss: 1.0879974365234375 --- Change: 0.00016817741394042917 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.937 --- Loss: 1.0896828174591064 --- Change: 0.0008943857192993175 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.938 --- Loss: 1.092002034187317 --- Change: 4.2349815367881405e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.939 --- Loss: 1.0930116176605225 --- Change: 0.00039712495803833063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.941 --- Loss: 1.0943353176116943 --- Change: 0.0010028900146484385 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0956448316574097 --- Change: 0.0003071457862854009 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961695909500122 --- Change: 0.0005425722122192388 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961976051330566 --- Change: -8.404254913330078e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0938652753829956 --- Change: -3.010749816899881e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.096069097518921 --- Change: 3.885335922241271e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0941189527511597 --- Change: 0.0005850434303283691 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.944 --- Loss: 1.0953398942947388 --- Change: 0.0003337175369262701 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.946 --- Loss: 1.097354531288147 --- Change: 0.0007956089019775401 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.947 --- Loss: 1.0990265607833862 --- Change: 0.00019839115142822317 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.948 --- Loss: 1.0994091033935547 --- Change: 0.0005852372169494634 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.101301670074463 --- Change: 0.00013222999572753958 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.1008661985397339 --- Change: 0.00013064146041870116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.0969769954681396 --- Change: 0.0011667609214782715 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0005680502891540538 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0006372961997985868 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 4.874467849731445e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 1.1086463928222655e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: -2.5033950805664064e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 1.5020370483398437e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 1.0728836059570311e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 2.1457672119140625e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.00014959573745727537 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.00038502216339111325 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 2.1457672119140623e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: -3.2186508178710934e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073286533355713 --- Change: 1.2874603271484374e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0005374581336975103 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078757047653198 --- Change: -1.5735626220703124e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078639030456543 --- Change: 3.540515899658203e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.955 --- Loss: 1.1005418300628662 --- Change: 0.0007966218948364245 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "oa = min(acc)\n",
    "ia = np.argmin(acc)\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 90.17798533837001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.782, 0.861, 0.71, 0.589, 0.574, 0.641, 0.783, 0.728, 0.877, 0.798]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.764, 0.702, 0.177, 0.193, 0.955, 0.619, 0.7, 0.377, 0.737, 0.862]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7744 - accuracy: 0.7343 - auc: 0.9650\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.574 --- Loss: 0.7743887901306152 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.013817846107482955 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.648 --- Loss: 0.9009959697723389 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536677002906799 --- Change: 0.05909848084449766 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 1.3107061386108399e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.755 --- Loss: 0.9536240100860596 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0008232743263244635 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.756 --- Loss: 0.953213095664978 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.897 --- Loss: 1.1212269067764282 --- Change: 0.048295856666564954 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.121446132659912 --- Change: 0.0041342322349548375 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.006408405303955078 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.903 --- Loss: 1.1000847816467285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.902 --- Loss: 1.0952787399291992 --- Change: 0.0007418125152587884 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0015961946487426768 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.904 --- Loss: 1.0946247577667236 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0027626694679260266 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.906 --- Loss: 1.0900825262069702 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.907 --- Loss: 1.0889948606491089 --- Change: 0.001026299667358399 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.908 --- Loss: 1.088670253753662 --- Change: 0.0007973820686340337 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0902987718582153 --- Change: 0.0009114445686340343 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.91 --- Loss: 1.0899895429611206 --- Change: 9.276866912841796e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0030559185028076192 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.914 --- Loss: 1.0891364812850952 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.090509295463562 --- Change: 0.0002881557464599615 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.915 --- Loss: 1.0894718170166016 --- Change: 0.0003112435340881348 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0015142022132873523 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.913 --- Loss: 1.079757809638977 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.917 --- Loss: 1.0784984827041626 --- Change: 0.003177798080444338 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.00047988004684448296 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.918 --- Loss: 1.0792322158813477 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0002508909225463873 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.0807292461395264 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.919 --- Loss: 1.08073091506958 --- Change: -5.006790161132813e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.92 --- Loss: 1.0827049016952515 --- Change: 0.00010780401229858454 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0836422443389893 --- Change: 0.00041879720687866263 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0834838151931763 --- Change: 4.752874374389648e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0004411697387695312 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0820132493972778 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.921 --- Loss: 1.0808788537979126 --- Change: 0.0003403186798095703 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0007631720542907726 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.923 --- Loss: 1.0830016136169434 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.926 --- Loss: 1.085551381111145 --- Change: 0.0013350697517395035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0872161388397217 --- Change: 0.0002005726814270025 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.000680989170074464 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896128416061401 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 1.7881393432617188e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0896122455596924 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0895538330078125 --- Change: 1.7523765563964844e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0848712921142578 --- Change: 4.76226806640518e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.927 --- Loss: 1.0842589139938354 --- Change: 0.00018371343612670897 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.929 --- Loss: 1.0870466232299805 --- Change: 0.0005636872291564952 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.931 --- Loss: 1.0885720252990723 --- Change: 0.0009423793792724621 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.933 --- Loss: 1.0920823812484741 --- Change: 0.0003468932151794444 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0923717021942139 --- Change: 0.0006132037162780767 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.934 --- Loss: 1.0916045904159546 --- Change: 0.0002301335334777832 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.095198631286621 --- Change: 0.00032178773880005003 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: -5.0067901611328125e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0952153205871582 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0005976676940917968 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0932230949401855 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0006748795509338379 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909734964370728 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: -1.430511474609375e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.090973973274231 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0909996032714844 --- Change: -7.68899917602539e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 3.247261047363281e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.936 --- Loss: 1.0908913612365723 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.935 --- Loss: 1.0879974365234375 --- Change: 0.00016817741394042917 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.937 --- Loss: 1.0896828174591064 --- Change: 0.0008943857192993175 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.938 --- Loss: 1.092002034187317 --- Change: 4.2349815367881405e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.939 --- Loss: 1.0930116176605225 --- Change: 0.00039712495803833063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.941 --- Loss: 1.0943353176116943 --- Change: 0.0010028900146484385 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0956448316574097 --- Change: 0.0003071457862854009 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961695909500122 --- Change: 0.0005425722122192388 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0961976051330566 --- Change: -8.404254913330078e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.942 --- Loss: 1.0938652753829956 --- Change: -3.010749816899881e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.096069097518921 --- Change: 3.885335922241271e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.943 --- Loss: 1.0941189527511597 --- Change: 0.0005850434303283691 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.944 --- Loss: 1.0953398942947388 --- Change: 0.0003337175369262701 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.946 --- Loss: 1.097354531288147 --- Change: 0.0007956089019775401 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.947 --- Loss: 1.0990265607833862 --- Change: 0.00019839115142822317 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.948 --- Loss: 1.0994091033935547 --- Change: 0.0005852372169494634 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.101301670074463 --- Change: 0.00013222999572753958 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.1008661985397339 --- Change: 0.00013064146041870116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.949 --- Loss: 1.0969769954681396 --- Change: 0.0011667609214782715 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0005680502891540538 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.951 --- Loss: 1.0997501611709595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0006372961997985868 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1092925071716309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 4.874467849731445e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091300249099731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 1.1086463928222655e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109126329421997 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: -2.5033950805664064e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109127163887024 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 1.5020370483398437e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091221570968628 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 1.0728836059570311e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1091217994689941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 2.1457672119140625e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.109114646911621 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.00014959573745727537 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1086159944534302 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.00038502216339111325 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073325872421265 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 2.1457672119140623e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073318719863892 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: -3.2186508178710934e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073329448699951 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.956 --- Loss: 1.1073286533355713 --- Change: 1.2874603271484374e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0005374581336975103 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078704595565796 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078757047653198 --- Change: -1.5735626220703124e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.957 --- Loss: 1.1078639030456543 --- Change: 3.540515899658203e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.955 --- Loss: 1.1005418300628662 --- Change: 0.0007966218948364245 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "oa = min(acc)\n",
    "ia = np.argmin(acc)\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    current_pos = 0\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        if current_pos in nodes_removed2:\n",
    "            current_pos += 1\n",
    "            if current_pos - num_removed2 >= size:\n",
    "                print(\"Layer optimized\")\n",
    "                end_not_reached = False\n",
    "            continue\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        K = len(set(y_test_flat))\n",
    "        yp = tf.argmax(y_pred, axis=1)\n",
    "        acc = []\n",
    "        for i in range(K):\n",
    "            a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "            acc.append(a)\n",
    "        na = acc[ia]\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.7*(na - oa) + 0.3*(ol - nl) >= tol:\n",
    "            best_change = 0.7*(na - oa) + 0.3*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            current_pos = 0\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7605 - accuracy: 0.7492 - auc: 0.9653\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605359554290771 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 8.344650268554688e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: -2.0027160644531247e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0005593955516815185 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 2.1195411682128904e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596960663795471 --- Change: 0.00012850165367126463 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: -6.258487701416016e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: -4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 8.131861686706543e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0016119062900543212 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.004112809896469116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.000922095775604248 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 9.938478469848632e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7501125931739807 --- Change: 0.00012117028236389161 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7494815587997437 --- Change: 0.000501716136932373 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.00027591586112976076 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0019015729427337646 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7460865378379822 --- Change: 4.903674125671387e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.746314287185669 --- Change: -9.43541526794432e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463226914405823 --- Change: -6.133317947387695e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7457163333892822 --- Change: 0.0003644585609436035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457607388496399 --- Change: 2.8908252716064453e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457746863365173 --- Change: -9.763240814208983e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.00021591782569885254 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.00023236870765686035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.744422435760498 --- Change: 0.0001982927322387695 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7428980469703674 --- Change: 0.001157069206237793 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7425541281700134 --- Change: 3.074407577514647e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0001464664936065674 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426019310951233 --- Change: 8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7421849370002747 --- Change: 0.000171893835067749 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7391334176063538 --- Change: 0.002346062660217285 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7394374012947083 --- Change: 2.7197599411010726e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 4.941225051879883e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0010465919971466063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7381114959716797 --- Change: 0.00013164281845092773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: -8.225440979003906e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380809187889099 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7381917834281921 --- Change: 0.0001623988151550293 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7378065586090088 --- Change: 0.0001796603202819824 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0001419425010681152 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 2.5451183319091794e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377282381057739 --- Change: 3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: -8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7374983429908752 --- Change: 0.00013179779052734374 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7371328473091125 --- Change: 7.58528709411621e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.737308144569397 --- Change: 8.729100227355957e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 4.234910011291504e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7364974021911621 --- Change: 0.0005251705646514892 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 2.8067827224731444e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.00012409687042236328 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: -5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7360435724258423 --- Change: 0.00032130479812622067 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 7.528662681579589e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361932396888733 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: -2.8371810913085934e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 6.712079048156738e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361469864845276 --- Change: -1.543760299682617e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361464500427246 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361381649971008 --- Change: 5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 2.515316009521487e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: -1.251697540283203e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362189888954163 --- Change: 5.424022674560547e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 4.342794418334961e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7541999816894531 --- Loss: 0.7359620332717896 --- Change: 0.0001964330673217773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359568476676941 --- Change: 3.363490104675293e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359568476676941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 2.7954578399658202e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.00033099055290222165 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0001447677612304687 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344157099723816 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "frac = 0.3\n",
    "ignore_tol = -1e-2\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 110.70612378517787\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.787, 0.897, 0.654, 0.513, 0.729, 0.624, 0.842, 0.789, 0.849, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.783, 0.867, 0.637, 0.535, 0.727, 0.668, 0.81, 0.792, 0.871, 0.833]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7605 - accuracy: 0.7492 - auc: 0.9653\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: -2.0027160644531247e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0005593955516815185 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 2.1195411682128904e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596960663795471 --- Change: 0.00012850165367126463 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: -6.258487701416016e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: -4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 8.131861686706543e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0016119062900543212 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.004112809896469116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.000922095775604248 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 9.938478469848632e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7501125931739807 --- Change: 0.00012117028236389161 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7494815587997437 --- Change: 0.000501716136932373 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.00027591586112976076 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0019015729427337646 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7461994290351868 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7460865378379822 --- Change: 4.903674125671387e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.746314287185669 --- Change: -9.43541526794432e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463139295578003 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7463226914405823 --- Change: -6.133317947387695e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7457163333892822 --- Change: 0.0003644585609436035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457607388496399 --- Change: 2.8908252716064453e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7457746863365173 --- Change: -9.763240814208983e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.00021591782569885254 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7455947995185852 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.00023236870765686035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.745091438293457 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.744422435760498 --- Change: 0.0001982927322387695 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7428980469703674 --- Change: 0.001157069206237793 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7425541281700134 --- Change: 3.074407577514647e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0001464664936065674 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426020503044128 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7426019310951233 --- Change: 8.344650268554687e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7421849370002747 --- Change: 0.000171893835067749 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7391334176063538 --- Change: 0.002346062660217285 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7394374012947083 --- Change: 2.7197599411010726e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 4.941225051879883e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393239736557007 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508000135421753 --- Loss: 0.7393232583999634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0010465919971466063 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7382566928863525 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7381114959716797 --- Change: 0.00013164281845092773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: -8.225440979003906e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380803823471069 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.751800000667572 --- Loss: 0.7380809187889099 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7381917834281921 --- Change: 0.0001623988151550293 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7378065586090088 --- Change: 0.0001796603202819824 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0001419425010681152 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377323508262634 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 2.5451183319091794e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377287149429321 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377282381057739 --- Change: 3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: -8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7377294898033142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7374983429908752 --- Change: 0.00013179779052734374 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7519000172615051 --- Loss: 0.7371328473091125 --- Change: 7.58528709411621e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.737308144569397 --- Change: 8.729100227355957e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 4.234910011291504e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.737504780292511 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526000142097473 --- Loss: 0.7364974021911621 --- Change: 0.0005251705646514892 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 2.8067827224731444e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7526999711990356 --- Loss: 0.7365001440048218 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.00012409687042236328 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364085912704468 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: -5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7529000043869019 --- Loss: 0.7364168763160706 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7360435724258423 --- Change: 0.00032130479812622067 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 7.528662681579589e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361931800842285 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.7361932396888733 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: -2.8371810913085934e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537000179290771 --- Loss: 0.736197292804718 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 6.712079048156738e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361442446708679 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: -3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361447811126709 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361469864845276 --- Change: -1.543760299682617e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361464500427246 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7537999749183655 --- Loss: 0.7361381649971008 --- Change: 5.799531936645508e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 2.515316009521487e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362203001976013 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 5.006790161132812e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.736219584941864 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: -1.251697540283203e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362197637557983 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7540000081062317 --- Loss: 0.7362189888954163 --- Change: 5.424022674560547e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 4.342794418334961e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7538999915122986 --- Loss: 0.7361140847206116 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7541999816894531 --- Loss: 0.7359620332717896 --- Change: 0.0001964330673217773 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359568476676941 --- Change: 3.363490104675293e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 2.7954578399658202e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7542999982833862 --- Loss: 0.7359528541564941 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.00033099055290222165 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7545999884605408 --- Loss: 0.7356085777282715 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0001447677612304687 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344160676002502 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7523000240325928 --- Loss: 0.7344157099723816 --- Change: 2.503395080566406e-07 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "frac = 0.3\n",
    "ignore_tol = -1e-2\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 47.23251302242279\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.787, 0.897, 0.654, 0.513, 0.729, 0.624, 0.842, 0.789, 0.849, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.783, 0.867, 0.637, 0.535, 0.727, 0.668, 0.81, 0.792, 0.871, 0.833]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 0.7605 - accuracy: 0.7492 - auc: 0.9653\n",
      "Considering layer 5\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 8.761882781982421e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605347633361816 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: -2.0027160644531247e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7491999864578247 --- Loss: 0.7605376243591309 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0005593955516815185 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7598242163658142 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 2.1195411682128904e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494000196456909 --- Loss: 0.7597939372062683 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596960663795471 --- Change: 0.00012850165367126463 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: -6.258487701416016e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: -4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596975564956665 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596969604492188 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7596970200538635 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 8.131861686706543e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7495999932289124 --- Loss: 0.7595808506011963 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0016119062900543212 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 4\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573638558387756 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498000264167786 --- Loss: 0.7573632597923279 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.004112809896469116 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.751359224319458 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.000922095775604248 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7503848075866699 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 9.938478469848632e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7502428293228149 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7504000067710876 --- Loss: 0.7501125931739807 --- Change: 0.00012117028236389161 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7494815587997437 --- Change: 0.000501716136932373 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.00027591586112976076 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.748830258846283 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.75 --- Loss: 0.7482198476791382 --- Change: 0.00042728781700134276 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470213770866394 --- Change: 0.0009289264678955078 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470213770866394 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470208406448364 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.7470208406448364 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7502999901771545 --- Loss: 0.746618926525116 --- Change: 0.00028133988380432126 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7465864419937134 --- Change: 8.27491283416748e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7465864419937134 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7465864419937134 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7466005682945251 --- Change: -9.888410568237303e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505000233650208 --- Loss: 0.7466089129447937 --- Change: -5.841255187988281e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7505999803543091 --- Loss: 0.7464084029197693 --- Change: 0.00017034411430358885 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501999735832214 --- Loss: 0.7458956837654114 --- Change: 0.00023890137672424313 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7454628348350525 --- Change: 0.0002129971981048584 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0001942157745361328 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7494999766349792 --- Loss: 0.745013952255249 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7480999827384949 --- Loss: 0.7441245913505554 --- Change: 0.00020255446434020992 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7442034482955933 --- Change: 0.000154799222946167 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7442034482955933 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7442033886909485 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7487999796867371 --- Loss: 0.7433729767799377 --- Change: 0.0005812883377075195 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.7434123158454895 --- Change: 2.467632293701174e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401872277259827 --- Change: 0.0021075546741485597 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401872277259827 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401856184005737 --- Change: 1.1265277862548827e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7483999729156494 --- Loss: 0.7401856184005737 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.7387703061103821 --- Change: 0.001140725612640381 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.7387703061103821 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7490000128746033 --- Loss: 0.7386330366134644 --- Change: 0.00012609362602233887 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7490000128746033 --- Loss: 0.7386330366134644 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7387228608131409 --- Change: 0.00020711421966552735 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.00033801794052124023 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387542724609375 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387546896934509 --- Change: -2.9206275939941403e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7511000037193298 --- Loss: 0.7387546896934509 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7509999871253967 --- Loss: 0.7383127808570862 --- Change: 0.0002793312072753906 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7377004027366638 --- Change: 0.00015867352485656735 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7501000165939331 --- Loss: 0.7377004027366638 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.737541913986206 --- Change: 5.093216896057129e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.737541913986206 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375418543815613 --- Change: 4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 1.043081283569336e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375403642654419 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375417947769165 --- Change: -1.0013580322265623e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 4.172325134277344e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7498999834060669 --- Loss: 0.7375411987304688 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7488999962806702 --- Loss: 0.737116813659668 --- Change: -2.926588058471647e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7493000030517578 --- Loss: 0.737097978591919 --- Change: 0.00013318657875061035 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7506999969482422 --- Loss: 0.737538754940033 --- Change: 0.00011145472526550291 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.00015570521354675294 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7508999705314636 --- Loss: 0.7374020218849182 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7509999871253967 --- Loss: 0.7369372844696045 --- Change: 0.0003553211688995361 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7516999840736389 --- Loss: 0.7372460961341858 --- Change: -6.16908073425293e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7516999840736389 --- Loss: 0.7372574210166931 --- Change: -7.927417755126953e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7370762228965759 --- Change: 0.0002468407154083252 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7375624179840088 --- Change: 1.9669532775878906e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7375624179840088 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.737557590007782 --- Change: 3.379583358764648e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 7.998943328857438e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372032999992371 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372033596038818 --- Change: -4.1723251342773435e-08 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 1.4185905456542967e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372013330459595 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372083067893982 --- Change: -4.881620407104492e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372087836265564 --- Change: -3.337860107421875e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372087836265564 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7372079491615295 --- Change: 5.841255187988281e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7524999976158142 --- Loss: 0.7371987104415894 --- Change: 6.467103958129883e-06 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.00015337467193603515 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.737236738204956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 3.7550926208496093e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372362017631531 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: -1.251697540283203e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 3\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7372363805770874 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7534000277519226 --- Loss: 0.7372525930404663 --- Change: 7.866621017456056e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7534000277519226 --- Loss: 0.7372525930404663 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7534000277519226 --- Loss: 0.7372525930404663 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371766567230225 --- Change: 2.3150444030761717e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 5.841255187988281e-07 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7533000111579895 --- Loss: 0.7371758222579956 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.00024483203887939455 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.753000020980835 --- Loss: 0.7366974949836731 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7366942167282104 --- Change: 3.228187561035156e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7366942167282104 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7531999945640564 --- Loss: 0.7365595698356628 --- Change: 0.00012425780296325683 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 7.008910179138183e-05 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 2\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7527999877929688 --- Loss: 0.7362880110740662 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0005673468112945557 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7520999908447266 --- Loss: 0.7351775169372559 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0003720462322235107 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7515000104904175 --- Loss: 0.7343888878822327 --- Change: 0.0 --- New tol: -1e-05\n",
      "Layer optimized\n",
      "Considering layer 1\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7342455983161926 --- Change: 0.0005802929401397704 --- New tol: -1e-05\n",
      "Improvement has occured!! Accuracy: 0.7530999779701233 --- Loss: 0.7342453002929688 --- Change: 2.086162567138672e-07 --- New tol: -1e-05\n",
      "Layer optimized\n"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "frac = 0.3\n",
    "ignore_tol = -1e-3\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 12.586614600817363\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.787, 0.897, 0.654, 0.513, 0.729, 0.624, 0.842, 0.789, 0.849, 0.808]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.766, 0.87, 0.632, 0.547, 0.711, 0.669, 0.828, 0.795, 0.876, 0.837]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "tol = -1e-5\n",
    "ignore_tol = -1e-2\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "amounts = []\n",
    "places = []\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "ofp = fp.result().numpy()\n",
    "otp = tp.result().numpy()\n",
    "\n",
    "fp.reset_states()\n",
    "tp.reset_states()\n",
    "\n",
    "start = time.time()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "\n",
    "        fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        \n",
    "        nfp = fp.result().numpy()\n",
    "        ntp = tp.result().numpy()\n",
    "        fp.reset_states()\n",
    "        tp.reset_states()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.5*(ntp - otp) + 0.5*(ofp - nfp) >= tol:\n",
    "            best_change = 0.5*(ntp - otp) + 0.5*(ofp - nfp)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            otp = ntp\n",
    "            ofp = nfp\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! True Positives:\", ntp, \"--- False Positives:\", nfp, \"--- Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif 0.5*(ntp - otp) + 0.5*(ofp - nfp) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "\n",
    "        fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "        \n",
    "        nfp = fp.result().numpy()\n",
    "        ntp = tp.result().numpy()\n",
    "        fp.reset_states()\n",
    "        tp.reset_states()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if 0.5*(ntp - otp) + 0.5*(ofp - nfp) >= tol:\n",
    "            best_change = 0.5*(ntp - otp) + 0.5*(ofp - nfp)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            otp = ntp\n",
    "            ofp = nfp\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! True Positives:\", ntp, \"--- False Positives:\", nfp, \"--- Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif 0.5*(ntp - otp) + 0.5*(ofp - nfp) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test_flat[y_test_flat==4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run the removal: 6.011486442883809\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time to run the removal: {(end-start) / 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205.0\n",
      "626.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = tester_model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9405\n",
      "[ True  True  True ...  True False  True]\n",
      "323.0\n",
      "728.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = np.array([1 if y == 4 else 0 for y in y_pred_flat])\n",
    "y_test_class_5 = np.array([1 if y == 4 else 0 for y in y_test_flat])\n",
    "\n",
    "acc = (y_pred_class_5 == y_test_class_5).mean()\n",
    "print(acc)\n",
    "print(y_pred_class_5 == y_test_class_5)\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36692652a799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moriginal2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mweight_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfrac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model2' is not defined"
     ]
    }
   ],
   "source": [
    "loss, acc, auc = model2.evaluate(x_test, y_test, verbose=2, batch_size=512)\n",
    "original2 = model2.get_weights()\n",
    "weight_len = len(original2) - 3\n",
    "frac = 0.75\n",
    "tol = -1e-5\n",
    "ignore_tol = -1e-3\n",
    "k = 9\n",
    "dense_layer_sizes = [64]\n",
    "conv_layer_sizes = [256, 128, 64, 32]\n",
    "conv_len = weight_len - 2 * len(dense_layer_sizes)\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights = model2.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "num_removed2 = 0\n",
    "amount = []\n",
    "place = []\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class = np.array([1 if y == k else 0 for y in y_pred_flat])\n",
    "y_test_class = np.array([1 if y == k else 0 for y in y_test_flat])\n",
    "\n",
    "oa = (y_pred_class == y_test_class).mean()\n",
    "for layer, size in enumerate(dense_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(dense_layer_sizes+conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "        w[weight_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class = np.array([1 if y == k else 0 for y in y_pred_flat])\n",
    "\n",
    "        na = (y_pred_class == y_test_class).mean()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            dense_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[weight_len - (2*layer+1)][:,current_pos] = 0\n",
    "            best_weights[weight_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:  # Ignoring very important nodes\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amount.append(num_removed2)\n",
    "    place.append(nodes_removed2)\n",
    "\n",
    "\n",
    "for layer, size in enumerate(conv_layer_sizes):\n",
    "    end_not_reached = True\n",
    "    num_removed2 = 0\n",
    "    nodes_removed2 = []\n",
    "    nodes_included = list(np.arange(size))\n",
    "    current_pos = nodes_included[0]\n",
    "    idx = 0\n",
    "    print(f'Considering layer {len(conv_layer_sizes) - layer}')\n",
    "    while end_not_reached:\n",
    "        w = copy.deepcopy(best_weights)\n",
    "        w[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "        w[conv_len - 2*layer][current_pos] = 0\n",
    "        tester_model2.set_weights(w)\n",
    "        del w\n",
    "        nl, na, nauc = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=1024)\n",
    "        y_pred = tester_model2.predict(x_test)\n",
    "        y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "        y_pred_class = np.array([1 if y == k else 0 for y in y_pred_flat])\n",
    "\n",
    "        na = (y_pred_class == y_test_class).mean()\n",
    "        # print(f\"Node {current_pos}:\", 0.*(na - oa) + 1.*(ol - nl))\n",
    "        if frac*(na - oa) + (1.-frac)*(ol - nl) >= tol:\n",
    "            best_change = frac*(na - oa) + (1.-frac)*(ol - nl)\n",
    "            ol = nl\n",
    "            oa = na\n",
    "            size -= 1\n",
    "            conv_layer_sizes[layer] -= 1\n",
    "            nodes_removed2 += [current_pos]\n",
    "            nodes_included.remove(current_pos)\n",
    "            best_weights[conv_len - (2*layer+1)][:,:,:,current_pos] = 0\n",
    "            best_weights[conv_len - 2*layer][current_pos] = 0\n",
    "            num_removed2 += 1\n",
    "            print(\"Improvement has occured!! Accuracy:\", na, \"--- Loss:\", nl, '--- Change:', best_change, '--- New tol:', tol)\n",
    "            idx = 0\n",
    "        elif frac*(na - oa) + (1.-frac)*(ol - nl) <= ignore_tol:\n",
    "            size -= 1\n",
    "            nodes_included.remove(current_pos)\n",
    "            idx += 1\n",
    "        else:\n",
    "            idx += 1\n",
    "        if idx >= size:\n",
    "            print(\"Layer optimized\")\n",
    "            end_not_reached = False\n",
    "        else:\n",
    "            current_pos = nodes_included[idx]\n",
    "    amounts.append(num_removed2)\n",
    "    places.append(nodes_removed2)\n",
    "\n",
    "end = time.time()\n",
    "tester_model2.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388.0\n",
      "731.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = tester_model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = [1 if y == 4 else 0 for y in y_pred_flat]\n",
    "y_test_class_5 = [1 if y == 4 else 0 for y in y_test_flat]\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442.0\n",
      "774.0\n"
     ]
    }
   ],
   "source": [
    "fp = tf.keras.metrics.FalsePositives()\n",
    "tp = tf.keras.metrics.TruePositives()\n",
    "\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "y_pred_class_5 = np.array([1 if y == 4 else 0 for y in y_pred_flat])\n",
    "y_test_class_5 = np.array([1 if y == 4 else 0 for y in y_test_flat])\n",
    "\n",
    "\n",
    "fp.update_state(y_test_class_5, y_pred_class_5)\n",
    "tp.update_state(y_test_class_5, y_pred_class_5)\n",
    "\n",
    "print(fp.result().numpy())\n",
    "print(tp.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.716, 0.765, 0.574, 0.653, 0.774, 0.559, 0.821, 0.775, 0.888, 0.838]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.773, 0.856, 0.599, 0.577, 0.731, 0.608, 0.816, 0.79, 0.848, 0.812]\n"
     ]
    }
   ],
   "source": [
    "y_pred = tester_model2.predict(x_test)\n",
    "K = len(set(y_test_flat))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test_flat == i] == y_test_flat[y_test_flat == i]).numpy())\n",
    "    acc.append(a)\n",
    "accuracies = tf.convert_to_tensor(acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Specific removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class3 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class4 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class5 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class6 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class7 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class8 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class9 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class10 = [amounts, places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 116, 22, 7, 3],\n",
       " [[0,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   25,\n",
       "   26,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   37,\n",
       "   39,\n",
       "   40,\n",
       "   42,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   56,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61],\n",
       "  [0,\n",
       "   4,\n",
       "   5,\n",
       "   2,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   10,\n",
       "   13,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   23,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   19,\n",
       "   21,\n",
       "   9,\n",
       "   30,\n",
       "   37,\n",
       "   40,\n",
       "   45,\n",
       "   48,\n",
       "   25,\n",
       "   39,\n",
       "   24,\n",
       "   31,\n",
       "   43,\n",
       "   32,\n",
       "   51,\n",
       "   53,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   69,\n",
       "   70,\n",
       "   52,\n",
       "   58,\n",
       "   57,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   79,\n",
       "   80,\n",
       "   86,\n",
       "   92,\n",
       "   99,\n",
       "   102,\n",
       "   110,\n",
       "   114,\n",
       "   116,\n",
       "   118,\n",
       "   119,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   128,\n",
       "   129,\n",
       "   134,\n",
       "   138,\n",
       "   144,\n",
       "   145,\n",
       "   153,\n",
       "   156,\n",
       "   83,\n",
       "   123,\n",
       "   94,\n",
       "   78,\n",
       "   159,\n",
       "   162,\n",
       "   163,\n",
       "   165,\n",
       "   167,\n",
       "   169,\n",
       "   171,\n",
       "   182,\n",
       "   183,\n",
       "   181,\n",
       "   184,\n",
       "   185,\n",
       "   187,\n",
       "   188,\n",
       "   192,\n",
       "   194,\n",
       "   195,\n",
       "   203,\n",
       "   205,\n",
       "   210,\n",
       "   211,\n",
       "   215,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   225,\n",
       "   226,\n",
       "   232,\n",
       "   208,\n",
       "   82,\n",
       "   89,\n",
       "   204,\n",
       "   229,\n",
       "   200,\n",
       "   87,\n",
       "   111,\n",
       "   149,\n",
       "   238,\n",
       "   240,\n",
       "   241,\n",
       "   244,\n",
       "   252,\n",
       "   253,\n",
       "   255],\n",
       "  [9,\n",
       "   12,\n",
       "   19,\n",
       "   20,\n",
       "   23,\n",
       "   27,\n",
       "   33,\n",
       "   39,\n",
       "   42,\n",
       "   48,\n",
       "   57,\n",
       "   63,\n",
       "   69,\n",
       "   74,\n",
       "   76,\n",
       "   88,\n",
       "   94,\n",
       "   98,\n",
       "   102,\n",
       "   113,\n",
       "   120,\n",
       "   122],\n",
       "  [4, 6, 8, 23, 37, 50, 56],\n",
       "  [9, 11, 25]]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 111, 22, 7, 1],\n",
       " [[0,\n",
       "   3,\n",
       "   2,\n",
       "   4,\n",
       "   7,\n",
       "   6,\n",
       "   10,\n",
       "   9,\n",
       "   11,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   23,\n",
       "   22,\n",
       "   26,\n",
       "   25,\n",
       "   29,\n",
       "   28,\n",
       "   30,\n",
       "   33,\n",
       "   35,\n",
       "   34,\n",
       "   40,\n",
       "   37,\n",
       "   39,\n",
       "   45,\n",
       "   42,\n",
       "   50,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   52,\n",
       "   59,\n",
       "   58,\n",
       "   56,\n",
       "   60,\n",
       "   61],\n",
       "  [0,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   2,\n",
       "   10,\n",
       "   13,\n",
       "   1,\n",
       "   9,\n",
       "   16,\n",
       "   17,\n",
       "   21,\n",
       "   23,\n",
       "   18,\n",
       "   26,\n",
       "   27,\n",
       "   31,\n",
       "   32,\n",
       "   37,\n",
       "   40,\n",
       "   48,\n",
       "   36,\n",
       "   45,\n",
       "   53,\n",
       "   12,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   51,\n",
       "   5,\n",
       "   30,\n",
       "   58,\n",
       "   65,\n",
       "   69,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   86,\n",
       "   87,\n",
       "   92,\n",
       "   102,\n",
       "   99,\n",
       "   110,\n",
       "   114,\n",
       "   116,\n",
       "   118,\n",
       "   119,\n",
       "   38,\n",
       "   15,\n",
       "   39,\n",
       "   19,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   128,\n",
       "   129,\n",
       "   144,\n",
       "   134,\n",
       "   138,\n",
       "   24,\n",
       "   145,\n",
       "   153,\n",
       "   159,\n",
       "   162,\n",
       "   163,\n",
       "   167,\n",
       "   165,\n",
       "   169,\n",
       "   182,\n",
       "   184,\n",
       "   171,\n",
       "   185,\n",
       "   188,\n",
       "   187,\n",
       "   194,\n",
       "   192,\n",
       "   195,\n",
       "   205,\n",
       "   210,\n",
       "   211,\n",
       "   221,\n",
       "   215,\n",
       "   222,\n",
       "   223,\n",
       "   226,\n",
       "   225,\n",
       "   229,\n",
       "   94,\n",
       "   200,\n",
       "   85,\n",
       "   14,\n",
       "   66,\n",
       "   105,\n",
       "   166,\n",
       "   106,\n",
       "   82,\n",
       "   109,\n",
       "   161,\n",
       "   209,\n",
       "   232,\n",
       "   241,\n",
       "   238,\n",
       "   240,\n",
       "   252,\n",
       "   244,\n",
       "   253,\n",
       "   255],\n",
       "  [12,\n",
       "   19,\n",
       "   9,\n",
       "   20,\n",
       "   23,\n",
       "   42,\n",
       "   27,\n",
       "   33,\n",
       "   48,\n",
       "   57,\n",
       "   94,\n",
       "   63,\n",
       "   69,\n",
       "   74,\n",
       "   76,\n",
       "   98,\n",
       "   88,\n",
       "   112,\n",
       "   39,\n",
       "   102,\n",
       "   113,\n",
       "   122],\n",
       "  [4, 37, 8, 6, 23, 50, 56],\n",
       "  [9]]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 116, 24, 8, 1],\n",
       " [[0,\n",
       "   3,\n",
       "   2,\n",
       "   4,\n",
       "   7,\n",
       "   6,\n",
       "   10,\n",
       "   9,\n",
       "   11,\n",
       "   12,\n",
       "   17,\n",
       "   20,\n",
       "   21,\n",
       "   23,\n",
       "   22,\n",
       "   26,\n",
       "   25,\n",
       "   29,\n",
       "   28,\n",
       "   30,\n",
       "   33,\n",
       "   35,\n",
       "   34,\n",
       "   40,\n",
       "   37,\n",
       "   39,\n",
       "   44,\n",
       "   42,\n",
       "   45,\n",
       "   50,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   52,\n",
       "   59,\n",
       "   58,\n",
       "   56,\n",
       "   60,\n",
       "   61],\n",
       "  [0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   5,\n",
       "   10,\n",
       "   16,\n",
       "   12,\n",
       "   13,\n",
       "   17,\n",
       "   18,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   32,\n",
       "   21,\n",
       "   23,\n",
       "   9,\n",
       "   19,\n",
       "   28,\n",
       "   30,\n",
       "   31,\n",
       "   22,\n",
       "   37,\n",
       "   36,\n",
       "   15,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   43,\n",
       "   45,\n",
       "   48,\n",
       "   24,\n",
       "   53,\n",
       "   54,\n",
       "   60,\n",
       "   63,\n",
       "   65,\n",
       "   72,\n",
       "   69,\n",
       "   74,\n",
       "   76,\n",
       "   79,\n",
       "   80,\n",
       "   92,\n",
       "   86,\n",
       "   102,\n",
       "   99,\n",
       "   110,\n",
       "   114,\n",
       "   118,\n",
       "   116,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   128,\n",
       "   129,\n",
       "   138,\n",
       "   134,\n",
       "   145,\n",
       "   153,\n",
       "   144,\n",
       "   159,\n",
       "   162,\n",
       "   163,\n",
       "   165,\n",
       "   180,\n",
       "   169,\n",
       "   171,\n",
       "   182,\n",
       "   34,\n",
       "   78,\n",
       "   123,\n",
       "   75,\n",
       "   58,\n",
       "   103,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   188,\n",
       "   187,\n",
       "   195,\n",
       "   192,\n",
       "   194,\n",
       "   200,\n",
       "   2,\n",
       "   66,\n",
       "   59,\n",
       "   94,\n",
       "   164,\n",
       "   64,\n",
       "   205,\n",
       "   210,\n",
       "   211,\n",
       "   215,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   225,\n",
       "   226,\n",
       "   238,\n",
       "   241,\n",
       "   240,\n",
       "   244,\n",
       "   245,\n",
       "   87,\n",
       "   181,\n",
       "   167,\n",
       "   203,\n",
       "   232,\n",
       "   252,\n",
       "   253,\n",
       "   255],\n",
       "  [12,\n",
       "   5,\n",
       "   9,\n",
       "   19,\n",
       "   20,\n",
       "   23,\n",
       "   33,\n",
       "   27,\n",
       "   57,\n",
       "   42,\n",
       "   48,\n",
       "   74,\n",
       "   63,\n",
       "   69,\n",
       "   76,\n",
       "   88,\n",
       "   94,\n",
       "   112,\n",
       "   24,\n",
       "   98,\n",
       "   102,\n",
       "   113,\n",
       "   120,\n",
       "   122],\n",
       "  [4, 37, 8, 6, 23, 50, 25, 56],\n",
       "  [9]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.save_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
