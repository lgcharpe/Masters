{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import tqdm\n",
    "import IProgress\n",
    "from hfunc import models\n",
    "from hfunc import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "class_accuracy = metrics.ClassAccuracy()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Converting interger values to floats (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_cross_entropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.04140123,  0.02092297,  0.07910059, ...,  0.01431214,\n",
      "         0.07908142, -0.07038791],\n",
      "       [ 0.02322625, -0.04434475, -0.04943981, ..., -0.00874022,\n",
      "        -0.03492042, -0.07444277],\n",
      "       [-0.05422504,  0.07055966, -0.04144818, ...,  0.03295391,\n",
      "        -0.04739096, -0.05361735],\n",
      "       ...,\n",
      "       [ 0.01923709, -0.01245946,  0.0348358 , ..., -0.05117496,\n",
      "        -0.05010323, -0.07360099],\n",
      "       [ 0.05129444, -0.01525372, -0.05118312, ...,  0.04241318,\n",
      "        -0.06969061,  0.07823717],\n",
      "       [ 0.07548442,  0.03861769, -0.0103631 , ..., -0.00405139,\n",
      "        -0.01304345,  0.0637982 ]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.02815443,  0.1325833 , -0.06017967, ..., -0.10675287,\n",
      "        -0.07129241,  0.09684996],\n",
      "       [ 0.06940933, -0.0103997 , -0.0738114 , ..., -0.09572481,\n",
      "        -0.05478222, -0.08200698],\n",
      "       [-0.02408485, -0.155465  , -0.0609604 , ..., -0.14139134,\n",
      "         0.13289525,  0.09843515],\n",
      "       ...,\n",
      "       [-0.10762817, -0.13648957, -0.05837148, ..., -0.00871238,\n",
      "         0.01715882, -0.03738481],\n",
      "       [-0.05112101,  0.09746204,  0.04428028, ..., -0.1081749 ,\n",
      "         0.04741074,  0.04240689],\n",
      "       [-0.08212709, -0.14098933,  0.03057367, ...,  0.10921349,\n",
      "         0.01945356, -0.0591845 ]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-1.08292222e-01, -1.74779922e-01, -2.09332347e-01,\n",
      "         3.96291018e-02, -8.62838775e-02,  2.59624153e-01,\n",
      "        -6.64260536e-02, -2.43343621e-01, -1.82252213e-01,\n",
      "        -4.84275073e-02],\n",
      "       [ 1.79544091e-02,  1.45719141e-01, -8.07463825e-02,\n",
      "         2.27406830e-01, -1.38723850e-03, -9.50602442e-02,\n",
      "         8.73549581e-02,  1.91433400e-01, -2.07872599e-01,\n",
      "         2.21402794e-01],\n",
      "       [-1.73402801e-01, -2.04494566e-01, -1.57326743e-01,\n",
      "         2.37113506e-01, -2.83326685e-01,  2.25525111e-01,\n",
      "        -2.31067836e-02,  2.58321017e-01, -1.04243189e-01,\n",
      "        -1.36939928e-01],\n",
      "       [-7.98251182e-02, -1.75457194e-01,  1.27954274e-01,\n",
      "        -2.98419595e-03, -2.35964686e-01,  2.05605716e-01,\n",
      "        -2.14741945e-01, -4.52169031e-02,  2.43271083e-01,\n",
      "         6.31525218e-02],\n",
      "       [-1.72776282e-02, -8.08468461e-02, -2.69916952e-01,\n",
      "        -8.29007626e-02, -2.38330692e-01, -2.37251118e-01,\n",
      "         7.41929710e-02, -3.22014093e-02,  2.10231006e-01,\n",
      "        -1.16861999e-01],\n",
      "       [-2.39517316e-01, -6.74989820e-02,  3.13687623e-02,\n",
      "         2.23451585e-01,  2.51377732e-01,  2.03463525e-01,\n",
      "        -2.83506930e-01,  2.35944539e-01, -1.50644556e-01,\n",
      "        -1.17507875e-02],\n",
      "       [ 2.06632316e-01,  2.66358644e-01,  7.34770894e-02,\n",
      "        -1.19089708e-01, -2.69096732e-01,  1.06230170e-01,\n",
      "        -1.68514520e-01, -3.45750749e-02,  6.46904707e-02,\n",
      "        -2.72680253e-01],\n",
      "       [ 1.90378666e-01, -2.11727738e-01, -2.07659081e-01,\n",
      "        -2.32430920e-01,  1.80039436e-01,  7.61413872e-02,\n",
      "         3.19331884e-02,  7.41221607e-02,  1.47334546e-01,\n",
      "         2.26558357e-01],\n",
      "       [ 1.51448160e-01,  2.55857557e-01,  1.95960790e-01,\n",
      "        -2.40301907e-01, -1.37552142e-01, -6.39213622e-02,\n",
      "         2.61965781e-01,  5.11745214e-02,  2.36963600e-01,\n",
      "        -1.43772885e-01],\n",
      "       [-3.76948565e-02,  2.62712866e-01, -3.16040516e-02,\n",
      "         2.73701340e-01,  2.08734453e-01, -1.01351380e-01,\n",
      "         8.35298896e-02,  3.55260670e-02, -1.53549388e-01,\n",
      "         1.13407820e-01],\n",
      "       [ 2.28062063e-01, -1.45374238e-02,  2.40603894e-01,\n",
      "        -8.14060569e-02, -6.60412610e-02, -6.45048618e-02,\n",
      "        -1.55364811e-01,  9.06482339e-03,  6.59167767e-03,\n",
      "        -1.51668191e-01],\n",
      "       [-1.13394290e-01,  2.69054472e-02, -2.76469141e-01,\n",
      "         2.33507186e-01, -1.16280735e-01, -1.01759136e-01,\n",
      "        -9.21658576e-02,  2.12847173e-01,  1.28571689e-02,\n",
      "        -1.42038733e-01],\n",
      "       [-1.39199674e-01,  1.74191594e-01,  8.29695463e-02,\n",
      "        -6.54805750e-02,  6.12239242e-02, -1.40139744e-01,\n",
      "        -4.33232784e-02, -7.31036216e-02, -2.03060478e-01,\n",
      "         1.45788372e-01],\n",
      "       [ 3.45480740e-02, -2.15185940e-01,  1.74276412e-01,\n",
      "        -5.92860281e-02,  5.16799986e-02, -2.47156680e-01,\n",
      "         7.48994946e-02, -1.94974542e-02,  1.35099918e-01,\n",
      "        -4.71463054e-02],\n",
      "       [-1.15225747e-01, -1.26317933e-01, -2.47340202e-02,\n",
      "        -1.84972584e-02,  2.87383199e-02, -2.75465935e-01,\n",
      "         7.45175481e-02, -1.27285719e-03,  1.66665018e-02,\n",
      "        -8.65830034e-02],\n",
      "       [ 5.81424534e-02, -4.75227535e-02,  3.74158323e-02,\n",
      "         1.04025006e-03, -7.34314024e-02,  6.34280741e-02,\n",
      "        -2.12649465e-01,  7.49817789e-02, -1.10353753e-01,\n",
      "        -1.51085854e-02],\n",
      "       [ 2.46686190e-01,  1.45795941e-02, -5.68150729e-02,\n",
      "        -1.37383044e-02,  2.18302161e-01, -4.09400910e-02,\n",
      "         2.31964737e-01, -2.44969010e-01, -2.71973580e-01,\n",
      "        -5.34296036e-04],\n",
      "       [-2.65958428e-01, -2.16866806e-01,  2.55276233e-01,\n",
      "        -1.14547938e-01, -1.22226670e-01, -9.73582268e-04,\n",
      "        -6.31101578e-02,  1.84840202e-01,  1.38092279e-01,\n",
      "        -1.05502740e-01],\n",
      "       [-1.37836605e-01, -3.68912667e-02,  3.51745486e-02,\n",
      "         1.45488232e-01,  6.63707256e-02,  2.67776698e-01,\n",
      "        -2.21496776e-01,  1.76498622e-01,  2.50116438e-01,\n",
      "         2.60929912e-01],\n",
      "       [-5.98427951e-02, -9.38319266e-02, -1.28852367e-01,\n",
      "        -7.43659586e-02, -1.52024627e-02, -4.33477759e-02,\n",
      "         7.84773827e-02,  2.44170576e-01, -2.33008519e-01,\n",
      "        -8.60463381e-02],\n",
      "       [ 5.93290031e-02,  1.86834872e-01,  2.09103912e-01,\n",
      "        -5.22616208e-02,  2.60790795e-01, -8.90105069e-02,\n",
      "        -2.63880044e-01, -6.62825406e-02, -1.11252874e-01,\n",
      "         1.50092483e-01],\n",
      "       [ 1.31700665e-01, -1.37314677e-01, -1.02730006e-01,\n",
      "         2.16096938e-02,  5.86061180e-02, -1.96967840e-01,\n",
      "        -2.67871737e-01, -1.98475599e-01,  4.39701974e-02,\n",
      "        -2.06476793e-01],\n",
      "       [-1.59887522e-01, -1.07344985e-03,  8.76871645e-02,\n",
      "        -1.54085755e-02, -2.00813696e-01,  1.95405096e-01,\n",
      "        -1.58345625e-01, -2.73951888e-01, -2.69737989e-01,\n",
      "         2.72601277e-01],\n",
      "       [ 2.72213966e-01, -1.79414451e-01, -2.03863412e-01,\n",
      "         1.99462295e-01,  1.01105750e-01, -1.69822544e-01,\n",
      "         1.50271505e-01, -2.89924443e-02, -2.33745724e-01,\n",
      "         9.93084013e-02],\n",
      "       [-1.88772753e-01, -3.58832479e-02, -1.13807321e-02,\n",
      "         1.43217623e-01,  8.59015882e-02, -1.72422290e-01,\n",
      "         1.67345941e-01, -2.16903329e-01,  1.24301970e-01,\n",
      "        -6.25204742e-02],\n",
      "       [-1.24384105e-01, -2.06808090e-01,  6.79464936e-02,\n",
      "        -1.17039397e-01,  8.27254653e-02,  1.60912573e-01,\n",
      "        -4.54934835e-02,  2.83690959e-01, -1.32810637e-01,\n",
      "        -2.01110706e-01],\n",
      "       [-2.75769204e-01, -8.72284174e-02, -1.03607610e-01,\n",
      "         1.07912600e-01,  2.47823685e-01,  1.54066712e-01,\n",
      "        -1.47166595e-01,  3.55894864e-02, -7.53688812e-02,\n",
      "        -1.69831902e-01],\n",
      "       [ 5.52953780e-02, -9.07600075e-02, -2.52991915e-02,\n",
      "         4.47661877e-02, -2.47670591e-01, -1.92048550e-02,\n",
      "         1.82717383e-01,  5.11725247e-02,  1.03171766e-02,\n",
      "        -1.48438022e-01],\n",
      "       [ 2.73863822e-01,  1.27690583e-01, -6.45338595e-02,\n",
      "         1.06390715e-01,  1.97870135e-02,  2.09551424e-01,\n",
      "        -2.79483467e-01, -9.40454006e-03,  2.52471656e-01,\n",
      "        -1.25110462e-01],\n",
      "       [-2.00958848e-01, -1.66692987e-01,  2.08939075e-01,\n",
      "        -1.68857008e-01,  2.82976836e-01, -2.51025617e-01,\n",
      "        -1.25404343e-01, -2.32528821e-01, -4.80799824e-02,\n",
      "         1.49592489e-01],\n",
      "       [-2.09407151e-01,  1.39926910e-01,  1.46102637e-01,\n",
      "         1.25685483e-01, -1.61950856e-01, -6.51322901e-02,\n",
      "        -1.61937892e-01,  1.99908674e-01,  2.28329301e-02,\n",
      "        -1.47369504e-02],\n",
      "       [-2.22960114e-03, -2.50796735e-01,  8.92069936e-03,\n",
      "         2.49590784e-01,  3.77232432e-02, -7.43773580e-02,\n",
      "        -6.48838878e-02, -1.41023040e-01,  2.28082746e-01,\n",
      "        -2.21305192e-01],\n",
      "       [-8.40279311e-02, -2.06429482e-01,  1.76503360e-02,\n",
      "        -1.17849246e-01,  2.80287534e-01, -2.31357604e-01,\n",
      "         2.51263380e-04, -1.55038327e-01,  1.10847354e-02,\n",
      "         2.21322149e-01],\n",
      "       [ 1.40005529e-01,  2.21013337e-01, -2.26764351e-01,\n",
      "         6.38891757e-02, -3.07020545e-03, -1.35592818e-02,\n",
      "         3.95685434e-02,  2.79528499e-02, -2.10166216e-01,\n",
      "        -1.58782274e-01],\n",
      "       [-2.76008248e-01,  1.25091523e-01, -1.99688971e-01,\n",
      "        -1.44881725e-01, -2.07778707e-01,  1.76441640e-01,\n",
      "        -6.32047951e-02, -2.14734375e-02,  1.91850841e-01,\n",
      "         2.06537157e-01],\n",
      "       [ 1.61833078e-01, -5.58600128e-02,  2.25358099e-01,\n",
      "         2.55516261e-01,  5.44818044e-02,  1.13322824e-01,\n",
      "        -1.15891874e-01,  2.34569401e-01,  3.56057286e-03,\n",
      "         1.47295505e-01],\n",
      "       [ 1.12059593e-01, -1.63723528e-01,  2.16199785e-01,\n",
      "         2.46778637e-01, -1.41053393e-01, -7.48842210e-02,\n",
      "        -2.29983628e-02,  1.63475454e-01, -1.84399754e-01,\n",
      "        -2.65436947e-01],\n",
      "       [ 2.04174936e-01,  2.38217920e-01,  1.38894111e-01,\n",
      "         1.27870291e-01, -1.71983242e-03, -1.75475121e-01,\n",
      "        -1.05606914e-02, -2.49165714e-01,  2.53180057e-01,\n",
      "         6.13437593e-02],\n",
      "       [ 4.43851948e-02,  1.75755292e-01, -2.46732771e-01,\n",
      "         1.11920357e-01,  1.71337813e-01, -1.75151229e-02,\n",
      "         1.08111203e-02,  1.73842192e-01,  2.03978956e-01,\n",
      "         9.78149176e-02],\n",
      "       [-1.72955140e-01,  2.46123761e-01,  2.06594169e-01,\n",
      "         9.90134776e-02, -4.91328090e-02,  2.65063912e-01,\n",
      "        -5.51437140e-02,  2.52232283e-01,  2.17832923e-02,\n",
      "         1.61993563e-01],\n",
      "       [-2.33630598e-01, -2.31073350e-01, -8.89611244e-03,\n",
      "         4.81512845e-02,  1.79111987e-01,  1.50356919e-01,\n",
      "         2.44897634e-01, -5.49810529e-02, -2.38062114e-01,\n",
      "         1.50804162e-01],\n",
      "       [ 1.46786273e-02, -1.69023693e-01,  8.09299946e-03,\n",
      "        -8.43654126e-02, -6.47512078e-03, -1.35461301e-01,\n",
      "        -4.76136655e-02, -1.61363006e-01, -2.71656275e-01,\n",
      "        -2.41014063e-01],\n",
      "       [-6.91574365e-02,  1.35632783e-01, -1.82712764e-01,\n",
      "        -1.30916938e-01,  2.55323380e-01,  2.19582468e-01,\n",
      "         5.28949499e-03,  2.88285613e-02, -2.79486597e-01,\n",
      "         1.35113835e-01],\n",
      "       [-1.66363180e-01,  1.97780132e-03, -6.81481957e-02,\n",
      "        -1.87306702e-02, -8.02433789e-02,  1.35979235e-01,\n",
      "         8.13203156e-02,  9.16095078e-02,  9.69889760e-02,\n",
      "        -2.33254954e-01],\n",
      "       [-2.19606876e-01,  9.98159349e-02,  2.28175849e-01,\n",
      "        -1.84352696e-01, -7.97040015e-02, -2.70197093e-02,\n",
      "         9.66209471e-02,  1.21936768e-01, -2.64355242e-02,\n",
      "         2.17352301e-01],\n",
      "       [ 2.13061750e-01,  1.45675004e-01,  1.68592781e-01,\n",
      "         1.37222558e-01,  2.46167183e-02,  2.04295456e-01,\n",
      "         1.62364453e-01,  5.98251820e-03, -1.67815268e-01,\n",
      "        -1.66555047e-02],\n",
      "       [-1.04065180e-01, -1.73916250e-01,  1.31762832e-01,\n",
      "        -1.37818754e-01, -9.17246491e-02,  2.60564476e-01,\n",
      "         2.81916469e-01,  2.11820215e-01,  1.62799686e-01,\n",
      "         1.85769260e-01],\n",
      "       [-2.47283280e-01,  2.64605731e-01,  1.46320760e-02,\n",
      "        -7.53622353e-02, -2.34476626e-02,  4.85563576e-02,\n",
      "         2.15462059e-01,  9.76217687e-02,  1.06739819e-01,\n",
      "         5.67583144e-02],\n",
      "       [-2.77213126e-01, -2.43815988e-01,  2.72157639e-01,\n",
      "         7.49443769e-02, -1.49066478e-01,  1.23706847e-01,\n",
      "         8.09546709e-02,  1.57963127e-01,  1.78057969e-01,\n",
      "        -2.58505017e-01],\n",
      "       [-9.75615531e-02, -2.13862658e-01, -2.59138197e-01,\n",
      "         2.75879830e-01, -1.21700868e-01,  5.81341684e-02,\n",
      "        -2.03546494e-01, -1.97864920e-01, -1.87815100e-01,\n",
      "         4.56234217e-02],\n",
      "       [ 2.78008252e-01, -2.44612873e-01,  9.92124677e-02,\n",
      "         6.54879212e-03,  8.61003995e-03, -1.16632879e-02,\n",
      "         2.66104013e-01, -2.46284366e-01, -1.72334298e-01,\n",
      "         1.84011966e-01],\n",
      "       [-9.86760855e-02,  2.57661015e-01, -2.42984414e-01,\n",
      "        -1.54587552e-01,  4.99039888e-03,  9.30322707e-02,\n",
      "         6.50668144e-03, -9.50868577e-02, -3.36525440e-02,\n",
      "        -2.50758857e-01],\n",
      "       [ 2.20657796e-01,  1.51575655e-01,  1.98612809e-01,\n",
      "        -1.55140162e-02,  9.54395533e-02, -3.05379331e-02,\n",
      "         1.18754357e-01, -3.83897126e-02, -1.50800839e-01,\n",
      "         2.72120029e-01],\n",
      "       [ 1.66086257e-01,  1.06137484e-01, -2.47273237e-01,\n",
      "        -8.08402598e-02,  9.14451480e-03, -7.07362741e-02,\n",
      "         9.58322883e-02,  1.86872333e-01, -2.34349132e-01,\n",
      "         1.76141113e-01],\n",
      "       [-2.56867111e-01,  2.75614828e-01,  1.08586192e-01,\n",
      "        -2.59550422e-01,  2.80629009e-01, -2.74248421e-01,\n",
      "         1.74104095e-01, -1.61247194e-01, -1.90427065e-01,\n",
      "         5.48980832e-02],\n",
      "       [-1.59503192e-01,  7.53995776e-03, -1.01471081e-01,\n",
      "         8.48033726e-02, -1.32133961e-02,  1.48261786e-01,\n",
      "        -2.55362153e-01,  5.67271411e-02,  1.73169613e-01,\n",
      "        -4.43794876e-02],\n",
      "       [ 1.79187626e-01,  9.65912044e-02,  2.79843807e-04,\n",
      "         1.95795119e-01,  1.02813721e-01,  8.95959139e-03,\n",
      "        -9.31203812e-02, -1.45479441e-02, -1.47237539e-01,\n",
      "        -3.46961617e-03],\n",
      "       [-2.32797995e-01, -1.62534237e-01, -7.17408210e-02,\n",
      "         9.11185443e-02, -2.26722270e-01, -2.10764393e-01,\n",
      "         1.58552349e-01,  2.27754682e-01,  1.15563482e-01,\n",
      "         9.46033597e-02],\n",
      "       [ 1.46401882e-01,  1.59951597e-01, -2.71650165e-01,\n",
      "         2.84504026e-01, -3.47693861e-02, -2.79891223e-01,\n",
      "        -8.43815655e-02, -2.00858429e-01, -1.40638947e-02,\n",
      "        -1.02373928e-01],\n",
      "       [ 2.62730092e-01,  1.89107984e-01,  2.00393051e-01,\n",
      "        -1.70960560e-01, -4.08720672e-02, -2.57039696e-01,\n",
      "         1.86428010e-01,  2.61733919e-01, -3.01284790e-02,\n",
      "         1.99612737e-01],\n",
      "       [ 5.27156591e-02, -2.15330422e-01,  1.20509475e-01,\n",
      "         4.54476476e-02, -1.84340000e-01,  2.08565742e-01,\n",
      "        -2.13140786e-01,  2.67182887e-02, -3.78184915e-02,\n",
      "         2.25590497e-01],\n",
      "       [ 1.53977424e-01,  2.80506015e-02, -1.41005322e-01,\n",
      "        -8.39828551e-02,  3.82564366e-02,  1.33934319e-02,\n",
      "        -1.77871197e-01,  1.25542015e-01, -1.64911300e-01,\n",
      "         1.00419670e-01],\n",
      "       [-1.57090083e-01, -9.14785266e-03, -5.35847694e-02,\n",
      "         1.11076832e-02,  2.19570369e-01,  2.65830725e-01,\n",
      "         1.71949297e-01, -1.91233322e-01,  5.10137379e-02,\n",
      "         1.72706813e-01],\n",
      "       [ 1.24975741e-02,  3.29552591e-02,  5.59149981e-02,\n",
      "         1.43542260e-01, -2.04387784e-01,  1.77621216e-01,\n",
      "         6.88276291e-02, -4.09481078e-02, -1.56929046e-01,\n",
      "        -1.83516100e-01]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "tmp = model.get_weights()\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, a = model.evaluate(x_test, y_test, verbose=2, batch_size=256)\n",
    "or_weights = model.get_weights()\n",
    "tol_low = -1e-5\n",
    "tol_high = 1e-5\n",
    "num_zeros, num_worse, num_important = (0, 0, 0)\n",
    "z = []\n",
    "wr = []\n",
    "imp = []\n",
    "num_layers = 2\n",
    "layer_sizes = [64, 128]\n",
    "for layer_since_last, size in enumerate(layer_sizes):\n",
    "    layer = num_layers - layer_since_last\n",
    "    for i in range(size):\n",
    "        w = copy.deepcopy(or_weights)\n",
    "        w[0][:,i] = 0\n",
    "        w[1][i] = 0\n",
    "        w[2][i,:] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na = tester_model2.evaluate(x_test, y_test, verbose=0, batch_size=256)\n",
    "        print(f\"Node {i}:\", 1.*(na - a) + 0*(l - nl))\n",
    "        change = l - nl\n",
    "        if change <= tol_high and change >= tol_low:\n",
    "            num_zeros += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important += 1\n",
    "            imp += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations\n",
      "0.0002429306507110596\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8906000256538391 --- Loss: 0.32161515951156616 --- Change: 0.0002429306507110596 --- New tol: -0.005443316698074341\n",
      "-0.0021391630172729495\n",
      "Found something better\n",
      "-0.0016697704792022706\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "7.74383544921875e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0001444399356842041\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00019713044166564942\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8906999826431274 --- Loss: 0.32102933526039124 --- Change: 0.00019713044166564942 --- New tol: -0.00564044713973999\n",
      "-0.002483254671096802\n",
      "Found something better\n",
      "-0.002012455463409424\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "6.430745124816895e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.00015971064567565918\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8909000158309937 --- Loss: 0.3210309147834778 --- Change: 0.00015971064567565918 --- New tol: -0.005800157785415649\n",
      "-0.0024858832359313963\n",
      "Found something better\n",
      "-0.002011680603027344\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "6.446242332458496e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "8.901357650756836e-05\n",
      "Found something better\n",
      "0.00011453628540039063\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8909000158309937 --- Loss: 0.32045823335647583 --- Change: 0.00011453628540039063 --- New tol: -0.00591469407081604\n",
      "-0.0024834811687469483\n",
      "Found something better\n",
      "-0.002247452735900879\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "6.908774375915528e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0002321779727935791\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8912000060081482 --- Loss: 0.3204973042011261 --- Change: 0.0002321779727935791 --- New tol: -0.00614687204360962\n",
      "-0.0024833202362060545\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "3.5762786865234375e-07\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "2.195835113525391e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078757882118225 --- Change: 2.195835113525391e-05 --- New tol: -0.006168830394744873\n",
      "-0.0023998916149139404\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "3.993511199951172e-07\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.3207855820655823 --- Change: 3.993511199951172e-07 --- New tol: -0.006169229745864868\n",
      "-0.0024002552032470707\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "4.172325134277344e-08\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 4.172325134277344e-08 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "0.0\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32078537344932556 --- Change: 0.0 --- New tol: -0.006169271469116211\n",
      "-0.002400261163711548\n",
      "Found something better\n",
      "-8.444786071777345e-05\n",
      "Found something better\n",
      "-1.5556812286376954e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-1.3113021850585939e-07\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.3207860291004181 --- Change: -1.3113021850585939e-07 --- New tol: -0.006169140338897705\n",
      "-0.0024002671241760256\n",
      "Found something better\n",
      "-8.444786071777345e-05\n",
      "Found something better\n",
      "-1.5556812286376954e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.3207938075065613 --- Change: -1.5556812286376954e-06 --- New tol: -0.006167584657669067\n",
      "-0.0024002969264984133\n",
      "Found something better\n",
      "-8.444786071777345e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-4.9579143524169926e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.32104170322418213 --- Change: -4.9579143524169926e-05 --- New tol: -0.006118005514144897\n",
      "-0.00240246057510376\n",
      "Found something better\n",
      "-8.456110954284668e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-4.9954652786254884e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8913000226020813 --- Loss: 0.3212914764881134 --- Change: -4.9954652786254884e-05 --- New tol: -0.006068050861358642\n",
      "-0.0023995935916900635\n",
      "Found something better\n",
      "-8.562803268432618e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8912000060081482 --- Loss: 0.3213195502758026 --- Change: -8.562803268432618e-05 --- New tol: -0.005982422828674315\n",
      "-0.0023173451423645023\n",
      "Found something better\n",
      "-0.00018467903137207032\n",
      "Found something better\n",
      "-7.62939453125e-06\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8912000060081482 --- Loss: 0.32135769724845886 --- Change: -7.62939453125e-06 --- New tol: -0.005974793434143065\n",
      "-0.002314114570617676\n",
      "Found something better\n",
      "-0.00018492937088012695\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.0001214444637298584\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8912000060081482 --- Loss: 0.32196491956710815 --- Change: -0.0001214444637298584 --- New tol: -0.005853348970413207\n",
      "-0.002622246742248535\n",
      "Found something better\n",
      "-0.0025653839111328125\n",
      "Found something better\n",
      "-0.0001854836940765381\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8909000158309937 --- Loss: 0.3216923773288727 --- Change: -0.0001854836940765381 --- New tol: -0.005667865276336669\n",
      "-0.0024393200874328613\n",
      "Found something better\n",
      "-0.002325314283370972\n",
      "Found something better\n",
      "-0.00039308071136474616\n",
      "Found something better\n",
      "-0.00018364191055297852\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.00013895630836486817\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8906999826431274 --- Loss: 0.3215870261192322 --- Change: -0.00013895630836486817 --- New tol: -0.005528908967971801\n",
      "-0.0025294899940490724\n",
      "Found something better\n",
      "-0.0024260699748992923\n",
      "Found something better\n",
      "-0.0006387174129486085\n",
      "Found something better\n",
      "-0.0005374252796173096\n",
      "Found something better\n",
      "-0.00017561912536621097\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8906000256538391 --- Loss: 0.3220652937889099 --- Change: -0.00017561912536621097 --- New tol: -0.00535328984260559\n",
      "-0.002611827850341797\n",
      "Found something better\n",
      "-0.0006379544734954834\n",
      "Found something better\n",
      "-0.0006160140037536621\n",
      "Found something better\n",
      "-0.0004103541374206543\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.00021765232086181643\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8903999924659729 --- Loss: 0.32235342264175415 --- Change: -0.00021765232086181643 --- New tol: -0.005135637521743773\n",
      "-0.002693140506744385\n",
      "Found something better\n",
      "-0.0007188975811004639\n",
      "Found something better\n",
      "-0.0006139814853668214\n",
      "Found something better\n",
      "-0.0004267990589141846\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.00023933649063110352\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8901000022888184 --- Loss: 0.3223501443862915 --- Change: -0.00023933649063110352 --- New tol: -0.00489630103111267\n",
      "-0.0028119564056396486\n",
      "Found something better\n",
      "-0.00040225386619567875\n",
      "Found something better\n",
      "-0.00038796067237854\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8899000287055969 --- Loss: 0.32349005341529846 --- Change: -0.00038796067237854 --- New tol: -0.0045083403587341305\n",
      "-0.002726036310195923\n",
      "Found something better\n",
      "-0.0008567750453948975\n",
      "Found something better\n",
      "-0.0006940662860870361\n",
      "Found something better\n",
      "-0.0006387770175933838\n",
      "Found something better\n",
      "-0.0004258096218109131\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8894000053405762 --- Loss: 0.32361900806427 --- Change: -0.0004258096218109131 --- New tol: -0.004082530736923218\n",
      "-0.002481973171234131\n",
      "Found something better\n",
      "-0.0007742226123809815\n",
      "Found something better\n",
      "-0.0007176637649536134\n",
      "Found something better\n",
      "-0.000488722324371338\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.00042054057121276855\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.88919997215271 --- Loss: 0.324921578168869 --- Change: -0.00042054057121276855 --- New tol: -0.0036619901657104492\n",
      "-0.002436894178390503\n",
      "Found something better\n",
      "-0.000997239351272583\n",
      "Found something better\n",
      "-0.00038142800331115724\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8888999819755554 --- Loss: 0.32562875747680664 --- Change: -0.00038142800331115724 --- New tol: -0.003280562162399292\n",
      "-0.002453345060348511\n",
      "Found something better\n",
      "-0.0011163115501403808\n",
      "Found something better\n",
      "-0.0010035872459411622\n",
      "Found something better\n",
      "-0.0009107530117034913\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.00028468966484069825\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8888000249862671 --- Loss: 0.3266523778438568 --- Change: -0.00028468966484069825 --- New tol: -0.0029958724975585934\n",
      "-0.0027882277965545653\n",
      "Found something better\n",
      "-0.001191699504852295\n",
      "Found something better\n",
      "-0.0010040283203125\n",
      "Found something better\n",
      "-0.0009878575801849365\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.0007130026817321778\n",
      "Found something better\n",
      "-0.0005667567253112793\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.0005119740962982178\n",
      "Found something better\n",
      "Improvement has occured!! Accuracy: 0.8881999850273132 --- Loss: 0.32681208848953247 --- Change: -0.0005119740962982178 --- New tol: -0.0024838984012603756\n",
      "-0.0007530510425567628\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.00042380094528198247\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8877999782562256 --- Loss: 0.3273310661315918 --- Change: -0.00042380094528198247 --- New tol: -0.0020600974559783934\n",
      "-0.000628805160522461\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.0005485177040100098\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.0005297660827636719\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.00040593743324279785\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8870999813079834 --- Loss: 0.32656076550483704 --- Change: -0.00040593743324279785 --- New tol: -0.0016541600227355955\n",
      "-0.0010215044021606445\n",
      "Found something better\n",
      "-0.0005382239818572998\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.0004841148853302002\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8867999911308289 --- Loss: 0.3277813792228699 --- Change: -0.0004841148853302002 --- New tol: -0.0011700451374053954\n",
      "-0.0010222017765045166\n",
      "Found something better\n",
      "-0.0003980398178100586\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8863999843597412 --- Loss: 0.3281715512275696 --- Change: -0.0003980398178100586 --- New tol: -0.0007720053195953368\n",
      "Did 20 iterations\n",
      "-0.0005594134330749513\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.0005039393901824952\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "-0.0002585113048553467\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8865000009536743 --- Loss: 0.32986417412757874 --- Change: -0.0002585113048553467 --- New tol: -0.0005134940147399901\n",
      "-7.783770561218262e-05\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8867999911308289 --- Loss: 0.3314533233642578 --- Change: -7.783770561218262e-05 --- New tol: -0.00043565630912780753\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "-0.00042560696601867676\n",
      "Found something better\n",
      "Did 20 iterations\n",
      "Improvement has occured!! Accuracy: 0.8865000009536743 --- Loss: 0.33238139748573303 --- Change: -0.00042560696601867676 --- New tol: -1.0049343109130773e-05\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "Did 20 iterations\n",
      "313/313 - 1s - loss: 0.3324 - accuracy: 0.8865\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "end_not_reached = True\n",
    "improved = False\n",
    "size = 128\n",
    "tol = -1e-30\n",
    "current_pos = 0\n",
    "best_pos = -1\n",
    "best_change = tol\n",
    "original = model.get_weights()\n",
    "bas = [acc]\n",
    "bls = [loss]\n",
    "best_weights = model.get_weights()\n",
    "nodes_removed = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed = 0\n",
    "while end_not_reached or improved:\n",
    "    if not(end_not_reached):\n",
    "        end_not_reached = True\n",
    "        improved = False\n",
    "        current_pos = 0\n",
    "        size -= 1\n",
    "        nodes_removed += [best_pos]\n",
    "        best_weights[0][:,best_pos] = 0\n",
    "        best_weights[1][best_pos] = 0\n",
    "        best_weights[2][best_pos,:] = 0\n",
    "        best_pos = -1\n",
    "        tol -= best_change\n",
    "        ol = best_loss\n",
    "        oa = best_acc\n",
    "        bas += [best_acc]\n",
    "        bls += [best_loss]\n",
    "        print(\"Improvement has occured!! Accuracy:\", best_acc, \"--- Loss:\", best_loss, '--- Change:', best_change, '--- New tol:', tol)\n",
    "        best_change = tol\n",
    "        num_removed += 1\n",
    "    if current_pos in nodes_removed:\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed >= size:\n",
    "            end_not_reached = False\n",
    "        continue\n",
    "    w = copy.deepcopy(best_weights)\n",
    "    w[0][:,current_pos] = 0\n",
    "    w[1][current_pos] = 0\n",
    "    w[2][current_pos,:] = 0\n",
    "    model.set_weights(w)\n",
    "    nl, na = model.evaluate(x_test, y_test, verbose=0)\n",
    "    if 0.8*(na - oa) + 0.2*(ol - nl) >= best_change:\n",
    "        best_change = 0.8*(na - oa) + 0.2*(ol - nl)\n",
    "        print(best_change)\n",
    "        best_pos = current_pos\n",
    "        improved = True\n",
    "        best_acc = na\n",
    "        best_loss = nl\n",
    "        print(\"Found something better\")\n",
    "    current_pos += 1\n",
    "    if current_pos - num_removed >= size:\n",
    "        end_not_reached = False\n",
    "    if current_pos%20 == 0:\n",
    "        print(\"Did 20 iterations\")\n",
    "\n",
    "model.set_weights(best_weights)\n",
    "loss2, acc2 = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.005549728870391846\n",
      "0.001399993896484375\n",
      "-1.6980388999339908\n",
      "0.15817352675649857\n",
      "57\n",
      "[ 4.5890132e-01 -1.3752976e-01  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  4.9236059e-02  0.0000000e+00\n",
      "  0.0000000e+00  6.4935058e-01  0.0000000e+00  0.0000000e+00\n",
      "  2.7358076e-01  0.0000000e+00  0.0000000e+00  1.9085664e-01\n",
      "  3.1870264e-01 -3.4913820e-01  1.6804899e-01  0.0000000e+00\n",
      " -4.4668129e-01  2.4767795e-01  2.1857737e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  2.9320963e-02\n",
      "  2.7298591e-01 -3.1684133e-01  0.0000000e+00  1.1506892e-01\n",
      "  4.7184643e-01  0.0000000e+00  0.0000000e+00  2.7141310e-04\n",
      "  0.0000000e+00  2.2542518e-01  4.1248152e-01  6.2194014e-01\n",
      " -6.8283193e-02  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  6.0038650e-01  0.0000000e+00  2.6078582e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  6.3469553e-01 -7.5865209e-02\n",
      "  5.4575580e-01  2.0256771e-01  0.0000000e+00  2.5241551e-01\n",
      " -8.2020074e-02  4.8264244e-01  0.0000000e+00  6.1429638e-01\n",
      "  0.0000000e+00  6.8113250e-01  0.0000000e+00  3.1842384e-01\n",
      " -4.1091666e-02 -9.3956895e-02  1.4803955e-01  6.2051070e-01\n",
      "  9.8633952e-02  0.0000000e+00  0.0000000e+00 -3.9817911e-01\n",
      "  3.9050329e-01  4.8059902e-01  2.7996850e-01 -7.3592149e-02\n",
      "  0.0000000e+00  1.1756032e-03  0.0000000e+00  4.8062563e-01\n",
      "  2.4221817e-02  3.7928635e-01  0.0000000e+00 -1.9133715e-01\n",
      "  6.3706124e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  4.8371190e-01  2.2542335e-01\n",
      "  6.1366510e-01  3.6257935e-01  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -1.1441048e-01  0.0000000e+00  0.0000000e+00\n",
      "  1.4049508e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  7.8458810e-01  0.0000000e+00  1.9234765e-01  4.1470423e-01\n",
      "  0.0000000e+00  0.0000000e+00  5.9712911e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  3.1954366e-01  5.4250192e-02\n",
      "  0.0000000e+00  4.0689588e-01  1.7242508e-01  3.8469163e-01\n",
      "  4.7821146e-01  1.6184023e-01  1.6110249e-01  1.7041522e-01\n",
      " -3.0728468e-01  6.7156786e-01  0.0000000e+00  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(loss - loss2)\n",
    "print(acc2 - acc)\n",
    "print((loss - loss2)/loss * 100)\n",
    "print((acc2 - acc)/acc * 100)\n",
    "print(num_removed)\n",
    "print(best_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.58901316e-01 -1.37529761e-01  5.09513974e-01 -1.15306377e-02\n",
      " -1.86158419e-01 -3.09453029e-02  4.92360592e-02  1.89392045e-01\n",
      "  2.77105391e-01  6.49350584e-01 -1.61763560e-02  1.37479961e-01\n",
      "  2.73580760e-01  2.62714744e-01 -1.65532231e-02  1.90856636e-01\n",
      "  3.18702638e-01 -3.49138200e-01  1.68048993e-01 -1.09401187e-02\n",
      " -4.46681291e-01  2.47677952e-01  2.18577370e-01  3.25502306e-01\n",
      " -4.29187119e-01 -1.64396197e-01 -1.96925864e-01  2.93209627e-02\n",
      "  2.72985905e-01 -3.16841334e-01 -6.65629432e-02  1.15068920e-01\n",
      "  4.71846431e-01 -2.49029741e-01 -5.96166030e-03  2.71413097e-04\n",
      "  2.90325373e-01  2.25425184e-01  4.12481517e-01  6.21940136e-01\n",
      " -6.82831928e-02  5.01024663e-01  2.75324076e-01  5.63114345e-01\n",
      "  6.00386500e-01 -2.57192124e-02  2.60785818e-01 -3.08213145e-01\n",
      " -6.93467185e-02 -2.48844251e-01  6.34695530e-01 -7.58652091e-02\n",
      "  5.45755804e-01  2.02567711e-01 -2.07985993e-02  2.52415508e-01\n",
      " -8.20200741e-02  4.82642442e-01 -1.99871942e-01  6.14296377e-01\n",
      " -1.71641279e-02  6.81132495e-01 -1.55536430e-02  3.18423837e-01\n",
      " -4.10916656e-02 -9.39568952e-02  1.48039550e-01  6.20510697e-01\n",
      "  9.86339524e-02 -3.72430719e-02 -1.88061222e-02 -3.98179114e-01\n",
      "  3.90503287e-01  4.80599016e-01  2.79968500e-01 -7.35921487e-02\n",
      "  3.75648513e-02  1.17560325e-03  1.52856320e-01  4.80625629e-01\n",
      "  2.42218170e-02  3.79286349e-01  1.57873780e-01 -1.91337153e-01\n",
      "  6.37061238e-01  3.18504483e-01  1.82982907e-01  1.67270780e-01\n",
      " -3.52242589e-02  2.21986234e-01  4.83711898e-01  2.25423351e-01\n",
      "  6.13665104e-01  3.62579346e-01 -1.20344656e-02 -7.43310945e-03\n",
      " -1.74210683e-01 -1.14410482e-01  4.18924749e-01  3.49019289e-01\n",
      "  1.40495077e-01 -3.25971395e-02 -2.47521996e-02  2.16301531e-02\n",
      "  7.84588099e-01  7.49694109e-01  1.92347646e-01  4.14704233e-01\n",
      "  3.43689248e-02  3.42002183e-01  5.97129107e-01  1.17199749e-01\n",
      "  5.66502333e-01  1.18946075e-01  3.19543660e-01  5.42501919e-02\n",
      "  1.97750013e-02  4.06895876e-01  1.72425076e-01  3.84691626e-01\n",
      "  4.78211462e-01  1.61840230e-01  1.61102489e-01  1.70415223e-01\n",
      " -3.07284683e-01  6.71567857e-01  2.78761089e-01 -1.27638103e-02]\n"
     ]
    }
   ],
   "source": [
    "or_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "or_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "or_model.set_weights(original)\n",
    "print(or_model.get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 3s - loss: 0.2440 - accuracy: 0.9126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24402140080928802, 0.9126333594322205]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train, y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 3s - loss: 0.2147 - accuracy: 0.9197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21473637223243713, 0.9197333455085754]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_model.evaluate(x_train, y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3324 - accuracy: 0.8865\n",
      "[0.83, 0.973, 0.773, 0.885, 0.848, 0.959, 0.701, 0.966, 0.976, 0.954]\n",
      "tf.Tensor(0.9924697, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "y_pred = model.predict(x_test)\n",
    "# class_accuracy.update_state(tf.convert_to_tensor(y_test), y_pred)\n",
    "# print(class_accuracy.result())\n",
    "K = len(set(y_test))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test == i] == y_test[y_test == i]).numpy())\n",
    "    acc.append(a)\n",
    "print(acc)\n",
    "\n",
    "m = tf.keras.metrics.AUC()\n",
    "m.update_state(tf.one_hot(y_test, 10), y_pred)\n",
    "print(m.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3268 - accuracy: 0.8851\n",
      "[0.828, 0.967, 0.78, 0.901, 0.861, 0.961, 0.649, 0.97, 0.972, 0.962]\n",
      "tf.Tensor(0.9917027, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "or_model.evaluate(x_test, y_test, verbose=2)\n",
    "y_pred = or_model.predict(x_test)\n",
    "# class_accuracy.update_state(y_test, y_pred)\n",
    "# print(class_accuracy.result())\n",
    "\n",
    "K = len(set(y_test))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test == i] == y_test[y_test == i]).numpy())\n",
    "    acc.append(a)\n",
    "print(acc)\n",
    "\n",
    "m = tf.keras.metrics.AUC()\n",
    "m.update_state(tf.one_hot(y_test, 10), y_pred)\n",
    "print(m.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5084 - accuracy: 0.8229\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3844 - accuracy: 0.8634\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3455 - accuracy: 0.8763\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3237 - accuracy: 0.8824\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3086 - accuracy: 0.8868\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2934 - accuracy: 0.8930\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2791 - accuracy: 0.8978\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2700 - accuracy: 0.8999\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2626 - accuracy: 0.9040\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2555 - accuracy: 0.9064\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2468 - accuracy: 0.9077\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2399 - accuracy: 0.9121\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2348 - accuracy: 0.9133\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2275 - accuracy: 0.9153\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2216 - accuracy: 0.9173\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2181 - accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2113 - accuracy: 0.9208\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2069 - accuracy: 0.9227\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2050 - accuracy: 0.9226\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1999 - accuracy: 0.9250\n",
      "313/313 - 0s - loss: 0.3546 - accuracy: 0.8819\n",
      "[0.763, 0.973, 0.851, 0.935, 0.711, 0.957, 0.74, 0.973, 0.974, 0.942]\n",
      "tf.Tensor(0.98948926, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "red_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128-num_removed, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "red_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "red_model.fit(x_train, y_train, epochs=20, verbose=1)\n",
    "red_model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "y_pred = red_model.predict(x_test)\n",
    "\n",
    "K = len(set(y_test))\n",
    "yp = tf.argmax(y_pred, axis=1)\n",
    "acc = []\n",
    "for i in range(K):\n",
    "    a = np.mean((yp[y_test == i] == y_test[y_test == i]).numpy())\n",
    "    acc.append(a)\n",
    "print(acc)\n",
    "\n",
    "m = tf.keras.metrics.AUC()\n",
    "m.update_state(tf.one_hot(y_test, 10), y_pred)\n",
    "print(m.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_path = f'C:/Users/lucas/Documents/Masters/models/reduced/fashion_mnist_128_{128-num_removed}'\n",
    "original_path = f'C:/Users/lucas/Documents/Masters/models/orininal/fashion_mnist_128_{128-num_removed}'\n",
    "full_reduced_path = f'C:/Users/lucas/Documents/Masters/models/full_reduced/fashion_mnist_128_{128-num_removed}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(reduced_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_model.save_weights(original_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = [np.zeros((best_weights[0].shape[0], best_weights[0].shape[1] - num_removed)), np.zeros((best_weights[1].shape[0] - num_removed)), np.zeros((best_weights[2].shape[0] - num_removed, best_weights[2].shape[1])), best_weights[3]]\n",
    "\n",
    "j = 0\n",
    "for i in range(len(best_weights[1])):\n",
    "    if i not in nodes_removed:\n",
    "        new_weights[0][:, j] = best_weights[0][:, i]\n",
    "        new_weights[1][j] = best_weights[1][i]\n",
    "        new_weights[2][j, :] = best_weights[2][i, :]\n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3324 - accuracy: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33238136768341064, 0.8865000009536743]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128-num_removed, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "red_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "red_model.set_weights(new_weights)\n",
    "red_model.save_weights(full_reduced_path)\n",
    "red_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.2254 - accuracy: 0.9166\n",
      "Epoch 2/15\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.2146 - accuracy: 0.9207\n",
      "Epoch 3/15\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.2105 - accuracy: 0.9227\n",
      "Epoch 4/15\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.2080 - accuracy: 0.9239\n",
      "Epoch 5/15\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2056 - accuracy: 0.9241\n",
      "Epoch 6/15\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.2030 - accuracy: 0.9256\n",
      "Epoch 7/15\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2016 - accuracy: 0.9259\n",
      "Epoch 8/15\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1999 - accuracy: 0.9265\n",
      "Epoch 9/15\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1985 - accuracy: 0.9270\n",
      "Epoch 10/15\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.1968 - accuracy: 0.9277\n",
      "Epoch 11/15\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1960 - accuracy: 0.9280\n",
      "Epoch 12/15\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.1949 - accuracy: 0.9287\n",
      "Epoch 13/15\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.1937 - accuracy: 0.9292\n",
      "Epoch 14/15\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.1922 - accuracy: 0.9297\n",
      "Epoch 15/15\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.1907 - accuracy: 0.9299\n",
      "40/40 - 0s - loss: 0.3112 - accuracy: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31121399998664856, 0.8920000195503235]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_model.set_weights(new_weights)\n",
    "red_model.fit(x_train, y_train, epochs=15, verbose=1, batch_size=4096)\n",
    "red_model.evaluate(x_test, y_test, verbose=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3324 - accuracy: 0.8865\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.2230 - accuracy: 0.9175\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2123 - accuracy: 0.9214\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2089 - accuracy: 0.9231\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2059 - accuracy: 0.9247\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2034 - accuracy: 0.9251\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2008 - accuracy: 0.9259\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1989 - accuracy: 0.9267\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1976 - accuracy: 0.9272\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1952 - accuracy: 0.9277\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1945 - accuracy: 0.9283\n",
      "313/313 - 1s - loss: 0.3139 - accuracy: 0.8903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31389063596725464, 0.8902999758720398]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "new_model.set_weights(best_weights)\n",
    "new_model.evaluate(x_test, y_test, verbose=2)\n",
    "new_model.fit(x_train, y_train, epochs=10, verbose=1, batch_size=2048)\n",
    "new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3139 - accuracy: 0.8903\n",
      "Node 0: -0.0014394640922546387\n",
      "Node 1: -0.002711939811706543\n",
      "Node 2: 0.0\n",
      "0.0\n",
      "Found something better\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: -0.0002560496330261231\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: -0.0047052264213562015\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: -0.00038411021232604987\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: -0.0027272582054138185\n",
      "Node 16: -0.002950209379196167\n",
      "Node 17: -0.0006514310836791993\n",
      "Node 18: -0.0029980480670928956\n",
      "Node 19: 0.0\n",
      "Did 20 iterations\n",
      "Node 20: -0.0025429069995880128\n",
      "Node 21: -0.0010326504707336426\n",
      "Node 22: -0.00021579861640930176\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: -0.005143517255783082\n",
      "Node 28: -0.0032790184020996095\n",
      "Node 29: -0.013628959655761719\n",
      "Node 30: 0.0\n",
      "Node 31: 0.0004489600658416748\n",
      "0.0004489600658416748\n",
      "Found something better\n",
      "Node 32: -0.0017570316791534425\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: 9.98377799987793e-06\n",
      "Node 36: 0.0\n",
      "Node 37: -0.0014190077781677248\n",
      "Node 38: -0.0055414259433746345\n",
      "Node 39: -0.0018332839012145997\n",
      "Did 20 iterations\n",
      "Node 40: -0.0071552276611328135\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: -0.0012900471687316895\n",
      "Node 45: 0.0\n",
      "Node 46: -0.0008590340614318848\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: -0.0006476044654846191\n",
      "Node 51: -0.0016164958477020265\n",
      "Node 52: 4.926323890686034e-05\n",
      "Node 53: -0.004443293809890747\n",
      "Node 54: 0.0\n",
      "Node 55: -0.0019753754138946535\n",
      "Node 56: -0.006591010093688965\n",
      "Node 57: -0.0031362950801849367\n",
      "Node 58: 0.0\n",
      "Node 59: -0.01074252724647522\n",
      "Did 20 iterations\n",
      "Node 60: 0.0\n",
      "Node 61: -0.004520684480667114\n",
      "Node 62: 0.0\n",
      "Node 63: -0.003096938133239746\n",
      "Node 64: -0.0009534239768981933\n",
      "Node 65: -0.0015328168869018556\n",
      "Node 66: -0.0037966430187225345\n",
      "Node 67: -0.0009172260761260986\n",
      "Node 68: -0.0023628950119018556\n",
      "Node 69: 0.0\n",
      "Node 70: 0.0\n",
      "Node 71: -0.00639801025390625\n",
      "Node 72: -0.004630088806152344\n",
      "Node 73: -0.006416141986846924\n",
      "Node 74: -0.00027040839195251464\n",
      "Node 75: -0.0009989202022552492\n",
      "Node 76: 0.0\n",
      "Node 77: 0.00021654367446899414\n",
      "Node 78: 0.0\n",
      "Node 79: -0.007365673780441284\n",
      "Did 20 iterations\n",
      "Node 80: -0.0007963836193084717\n",
      "Node 81: -0.0020715355873107914\n",
      "Node 82: 0.0\n",
      "Node 83: -0.00485687255859375\n",
      "Node 84: -0.002972334623336792\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0\n",
      "Node 90: -0.0010391354560852053\n",
      "Node 91: -0.0037441015243530277\n",
      "Node 92: -0.0029948890209198\n",
      "Node 93: -0.001083242893218994\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: 0.0\n",
      "Node 97: -0.0007710576057434082\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Did 20 iterations\n",
      "Node 100: -0.0030179500579833986\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: -0.0020674467086791992\n",
      "Node 105: 0.0\n",
      "Node 106: -0.00037426948547363286\n",
      "Node 107: -0.0002591192722320557\n",
      "Node 108: 0.0\n",
      "Node 109: 0.0\n",
      "Node 110: -0.004704385995864868\n",
      "Node 111: 0.0\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: -0.0011535286903381348\n",
      "Node 115: -0.0018490910530090333\n",
      "Node 116: 0.0\n",
      "Node 117: -0.0023045480251312257\n",
      "Node 118: -0.0010370790958404543\n",
      "Node 119: -0.0021989822387695314\n",
      "Did 20 iterations\n",
      "Node 120: -0.0010091245174407959\n",
      "Node 121: -0.0030420184135437015\n",
      "Node 122: -0.0015169918537139893\n",
      "Node 123: -0.004982292652130127\n",
      "Node 124: -0.0021453261375427245\n",
      "Node 125: -0.0009771347045898439\n",
      "Node 126: 0.0\n",
      "Node 127: 0.0\n",
      "Improvement has occured!! Accuracy: 0.8906999826431274 --- Loss: 0.31324586272239685 --- Change: 0.0004489600658416748 --- New tol: -0.0004489600658416748\n",
      "Node 0: -0.0014398813247680664\n",
      "Node 1: -0.002635133266448975\n",
      "Node 2: 0.0\n",
      "0.0\n",
      "Found something better\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: -0.0004556834697723389\n",
      "Node 7: 0.0\n",
      "Node 8: 0.0\n",
      "Node 9: -0.004355466365814209\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0\n",
      "Node 12: -0.0002535045146942139\n",
      "Node 13: 0.0\n",
      "Node 14: 0.0\n",
      "Node 15: -0.003563112020492554\n",
      "Node 16: -0.0027769744396209718\n",
      "Node 17: -0.0020815134048461914\n",
      "Node 18: -0.0029630661010742188\n",
      "Node 19: 0.0\n",
      "Did 20 iterations\n",
      "Node 20: -0.004685646295547486\n",
      "Node 21: -0.0008097410202026368\n",
      "Node 22: -0.0004713535308837891\n",
      "Node 23: 0.0\n",
      "Node 24: 0.0\n",
      "Node 25: 0.0\n",
      "Node 26: 0.0\n",
      "Node 27: -0.005441039800643921\n",
      "Node 28: -0.003491342067718506\n",
      "Node 29: -0.014631062746047975\n",
      "Node 30: 0.0\n",
      "Node 32: -0.0019524097442626955\n",
      "Node 33: 0.0\n",
      "Node 34: 0.0\n",
      "Node 35: -0.00018551349639892581\n",
      "Node 36: 0.0\n",
      "Node 37: -0.0016692638397216798\n",
      "Node 38: -0.0055682361125946045\n",
      "Node 39: -0.0030601024627685547\n",
      "Did 20 iterations\n",
      "Node 40: -0.009078598022460938\n",
      "Node 41: 0.0\n",
      "Node 42: 0.0\n",
      "Node 43: 0.0\n",
      "Node 44: -0.0008779227733612061\n",
      "Node 45: 0.0\n",
      "Node 46: -0.0008696794509887696\n",
      "Node 47: 0.0\n",
      "Node 48: 0.0\n",
      "Node 49: 0.0\n",
      "Node 50: -0.0005625069141387939\n",
      "Node 51: -0.004128181934356689\n",
      "Node 52: 1.1342763900756833e-05\n",
      "1.1342763900756833e-05\n",
      "Found something better\n",
      "Node 53: -0.004456537961959839\n",
      "Node 54: 0.0\n",
      "Node 55: -0.0015381813049316407\n",
      "Node 56: -0.005656182765960693\n",
      "Node 57: -0.003495419025421143\n",
      "Node 58: 0.0\n",
      "Node 59: -0.010275328159332277\n",
      "Did 20 iterations\n",
      "Node 60: 0.0\n",
      "Node 61: -0.003906112909317017\n",
      "Node 62: 0.0\n",
      "Node 63: -0.002862972021102905\n",
      "Node 64: -0.0011829853057861329\n",
      "Node 65: -0.001769000291824341\n",
      "Node 66: -0.0032562196254730226\n",
      "Node 67: -0.0010923266410827637\n",
      "Node 68: -0.0020560026168823242\n",
      "Node 69: 0.0\n",
      "Node 70: 0.0\n",
      "Node 71: -0.006821221113204956\n",
      "Node 72: -0.004627770185470581\n",
      "Node 73: -0.006425881385803223\n",
      "Node 74: -0.0009082078933715821\n",
      "Node 75: -0.0010734915733337403\n",
      "Node 76: 0.0\n",
      "Node 77: 0.00021857023239135745\n",
      "0.00021857023239135745\n",
      "Found something better\n",
      "Node 78: 0.0\n",
      "Node 79: -0.007180058956146241\n",
      "Did 20 iterations\n",
      "Node 80: -0.0007122516632080078\n",
      "Node 81: -0.0016581594944000246\n",
      "Node 82: 0.0\n",
      "Node 83: -0.005932408571243286\n",
      "Node 84: -0.002971702814102173\n",
      "Node 85: 0.0\n",
      "Node 86: 0.0\n",
      "Node 87: 0.0\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0\n",
      "Node 90: -0.0011046230792999268\n",
      "Node 91: -0.0036417663097381594\n",
      "Node 92: -0.002656364440917969\n",
      "Node 93: -0.0006700932979583741\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: 0.0\n",
      "Node 97: -0.0007588624954223633\n",
      "Node 98: 0.0\n",
      "Node 99: 0.0\n",
      "Did 20 iterations\n",
      "Node 100: -0.002991503477096558\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: 0.0\n",
      "Node 104: -0.0018755376338958742\n",
      "Node 105: 0.0\n",
      "Node 106: -0.0005846261978149414\n",
      "Node 107: -0.0009148120880126953\n",
      "Node 108: 0.0\n",
      "Node 109: 0.0\n",
      "Node 110: -0.004340440034866333\n",
      "Node 111: 0.0\n",
      "Node 112: 0.0\n",
      "Node 113: 0.0\n",
      "Node 114: -0.0009154617786407472\n",
      "Node 115: -0.0021619796752929688\n",
      "Node 116: 0.0\n",
      "Node 117: -0.0022203981876373294\n",
      "Node 118: -0.0008554935455322266\n",
      "Node 119: -0.002200937271118164\n",
      "Did 20 iterations\n",
      "Node 120: -0.0013658463954925536\n",
      "Node 121: -0.002437925338745117\n",
      "Node 122: -0.0014495193958282472\n",
      "Node 123: -0.004664188623428345\n",
      "Node 124: -0.002338796854019165\n",
      "Node 125: -0.000738048553466797\n",
      "Node 126: 0.0\n",
      "Node 127: 0.0\n",
      "Improvement has occured!! Accuracy: 0.8912000060081482 --- Loss: 0.31415310502052307 --- Change: 0.00021857023239135745 --- New tol: -0.0006675302982330323\n",
      "Node 0: -0.001496928930282593\n",
      "Node 1: -0.004896265268325806\n",
      "Node 2: 0.0\n",
      "0.0\n",
      "Found something better\n",
      "Node 3: 0.0\n",
      "Node 4: 0.0\n",
      "Node 5: 0.0\n",
      "Node 6: -0.0005292057991027832\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-75eafbedb268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mnl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Node {current_pos}:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moa\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mol\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moa\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mol\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_change\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1079\u001b[0m                 step_num=step):\n\u001b[0;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, acc = new_model.evaluate(x_test, y_test, verbose=2)\n",
    "end_not_reached = True\n",
    "improved = False\n",
    "size = 128\n",
    "tol = -1e-30\n",
    "current_pos = 0\n",
    "best_pos = -1\n",
    "best_change = tol\n",
    "original2 = new_model.get_weights()\n",
    "bas2 = [acc]\n",
    "bls2 = [loss]\n",
    "best_weights2 = new_model.get_weights()\n",
    "nodes_removed2 = []\n",
    "best_acc = 0\n",
    "best_loss = 1e20\n",
    "ol = loss\n",
    "oa = acc\n",
    "num_removed2 = 0\n",
    "while end_not_reached or improved:\n",
    "    if not(end_not_reached):\n",
    "        end_not_reached = True\n",
    "        improved = False\n",
    "        current_pos = 0\n",
    "        size -= 1\n",
    "        nodes_removed2 += [best_pos]\n",
    "        best_weights2[0][:,best_pos] = 0\n",
    "        best_weights2[1][best_pos] = 0\n",
    "        best_weights2[2][best_pos,:] = 0\n",
    "        best_pos = -1\n",
    "        tol -= best_change\n",
    "        ol = best_loss\n",
    "        oa = best_acc\n",
    "        bas2 += [best_acc]\n",
    "        bls2 += [best_loss]\n",
    "        print(\"Improvement has occured!! Accuracy:\", best_acc, \"--- Loss:\", best_loss, '--- Change:', best_change, '--- New tol:', tol)\n",
    "        best_change = tol\n",
    "        num_removed2 += 1\n",
    "    if current_pos in nodes_removed2:\n",
    "        current_pos += 1\n",
    "        if current_pos - num_removed2 >= size:\n",
    "            end_not_reached = False\n",
    "        continue\n",
    "    w = copy.deepcopy(best_weights2)\n",
    "    w[0][:,current_pos] = 0\n",
    "    w[1][current_pos] = 0\n",
    "    w[2][current_pos,:] = 0\n",
    "    new_model.set_weights(w)\n",
    "    nl, na = new_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {current_pos}:\", 0.8*(na - oa) + 0.2*(ol - nl))\n",
    "    if 0.8*(na - oa) + 0.2*(ol - nl) > best_change:\n",
    "        best_change = 0.8*(na - oa) + 0.2*(ol - nl)\n",
    "        print(best_change)\n",
    "        best_pos = current_pos\n",
    "        improved = True\n",
    "        best_acc = na\n",
    "        best_loss = nl\n",
    "        print(\"Found something better\")\n",
    "    current_pos += 1\n",
    "    if current_pos - num_removed2 >= size:\n",
    "        end_not_reached = False\n",
    "    if current_pos%20 == 0:\n",
    "        print(\"Did 20 iterations\")\n",
    "\n",
    "new_model.set_weights(best_weights2)\n",
    "loss2, acc2 = new_model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "tester_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3268 - accuracy: 0.8851\n",
      "Node 0: -0.0010176539421081544\n",
      "Node 1: -0.0018407821655273436\n",
      "Node 2: 0.00044121742248535154\n",
      "Node 3: 0.0\n",
      "Node 4: 7.524490356445313e-05\n",
      "Node 5: -7.578134536743163e-05\n",
      "Node 6: 0.0008309841156005859\n",
      "Node 7: -0.00022814273834228515\n",
      "Node 8: -0.0009253263473510743\n",
      "Node 9: -0.003994774818420411\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0007683992385864258\n",
      "Node 12: -0.0011681914329528809\n",
      "Node 13: 4.014968872070312e-05\n",
      "Node 14: 0.0\n",
      "Node 15: -0.0018353462219238281\n",
      "Node 16: -0.0006745338439941406\n",
      "Node 17: -0.001815140247344971\n",
      "Node 18: -0.002695620059967041\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0013562440872192383\n",
      "Node 21: -0.0014670729637145995\n",
      "Node 22: -0.0005298018455505371\n",
      "Node 23: -0.001352214813232422\n",
      "Node 24: 0.0001497507095336914\n",
      "Node 25: -0.00017908811569213867\n",
      "Node 26: -0.00031138658523559575\n",
      "Node 27: -0.007463109493255616\n",
      "Node 28: -0.0038578867912292484\n",
      "Node 29: -0.002183234691619873\n",
      "Node 30: -4.6706199645996095e-05\n",
      "Node 31: -0.00023217201232910158\n",
      "Node 32: -0.0005174160003662109\n",
      "Node 33: -9.288787841796873e-05\n",
      "Node 34: -3.409385681152344e-06\n",
      "Node 35: -0.000211024284362793\n",
      "Node 36: -0.00011923313140869141\n",
      "Node 37: -0.00011060237884521482\n",
      "Node 38: -0.006365001201629639\n",
      "Node 39: -0.0026983141899108885\n",
      "Node 40: -0.004924178123474121\n",
      "Node 41: -0.0005797028541564941\n",
      "Node 42: 0.00039932727813720705\n",
      "Node 43: -0.00030653476715087886\n",
      "Node 44: -0.0009437799453735352\n",
      "Node 45: 0.0\n",
      "Node 46: -0.0006805896759033203\n",
      "Node 47: -0.00013967752456665037\n",
      "Node 48: 0.000567615032196045\n",
      "Node 49: 3.0994415283203126e-07\n",
      "Node 50: -0.0007431387901306153\n",
      "Node 51: -0.0025992035865783692\n",
      "Node 52: -0.0001482844352722168\n",
      "Node 53: -0.0020899176597595215\n",
      "Node 54: 0.0\n",
      "Node 55: -0.001860654354095459\n",
      "Node 56: -0.005921304225921631\n",
      "Node 57: -0.002466702461242676\n",
      "Node 58: 0.0009051799774169922\n",
      "Node 59: -0.007501947879791261\n",
      "Node 60: 0.0\n",
      "Node 61: -0.0060723662376403805\n",
      "Node 62: 0.0\n",
      "Node 63: -0.003586208820343018\n",
      "Node 64: 0.0002567887306213379\n",
      "Node 65: -0.0023824453353881838\n",
      "Node 66: -0.0006765961647033691\n",
      "Node 67: -0.0004419207572937012\n",
      "Node 68: -0.003004300594329834\n",
      "Node 69: -0.0002933382987976074\n",
      "Node 70: 7.152557373046876e-08\n",
      "Node 71: -0.004154813289642335\n",
      "Node 72: -0.0013007164001464845\n",
      "Node 73: -0.0062654376029968265\n",
      "Node 74: -6.884336471557612e-05\n",
      "Node 75: -0.0008123517036437988\n",
      "Node 76: -7.613897323608399e-05\n",
      "Node 77: -0.0011734366416931152\n",
      "Node 78: -2.6226043701171877e-07\n",
      "Node 79: -0.01062229871749878\n",
      "Node 80: -0.0005669713020324707\n",
      "Node 81: -0.0015980005264282227\n",
      "Node 82: -0.00022927522659301757\n",
      "Node 83: -0.0013670921325683594\n",
      "Node 84: -0.001287233829498291\n",
      "Node 85: -0.002341556549072266\n",
      "Node 86: -0.00044370889663696293\n",
      "Node 87: -0.0014662742614746094\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0002532482147216797\n",
      "Node 90: -0.0003182172775268555\n",
      "Node 91: -0.002075600624084473\n",
      "Node 92: -0.0010930657386779784\n",
      "Node 93: -0.0006186962127685547\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: -0.00035305023193359374\n",
      "Node 97: -0.0012226819992065428\n",
      "Node 98: -0.0009716033935546875\n",
      "Node 99: -0.0007668495178222656\n",
      "Node 100: -0.0014756679534912109\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: -0.000177001953125\n",
      "Node 104: -0.0023982405662536623\n",
      "Node 105: -0.000702667236328125\n",
      "Node 106: -0.0006182670593261718\n",
      "Node 107: -0.000719451904296875\n",
      "Node 108: -0.0009042024612426758\n",
      "Node 109: -0.0004195094108581543\n",
      "Node 110: -0.003990089893341065\n",
      "Node 111: -0.0001772880554199219\n",
      "Node 112: 0.00021823644638061527\n",
      "Node 113: 0.00015288591384887698\n",
      "Node 114: -0.0018019318580627442\n",
      "Node 115: -0.0032615065574645996\n",
      "Node 116: 0.0021360039710998533\n",
      "Node 117: -0.0012618780136108398\n",
      "Node 118: -0.0003555893898010254\n",
      "Node 119: -0.001127505302429199\n",
      "Node 120: -0.0034661293029785156\n",
      "Node 121: -0.002553629875183105\n",
      "Node 122: -0.0014696002006530761\n",
      "Node 123: -0.0042486429214477536\n",
      "Node 124: -0.0012107968330383303\n",
      "Node 125: -0.0003769516944885254\n",
      "Node 126: -9.025335311889649e-05\n",
      "Node 127: -0.00014737844467163083\n",
      "63.35363292694092\n"
     ]
    }
   ],
   "source": [
    "l, a = or_model.evaluate(x_test, y_test, verbose=2)\n",
    "or_weights = or_model.get_weights()\n",
    "size = 128\n",
    "start = time.time()\n",
    "for i in range(128):\n",
    "    w = copy.deepcopy(or_weights)\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.6*(na - a) + 0.4*(l - nl))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3268 - accuracy: 0.8851\n",
      "Node 0: -0.0010176539421081544\n",
      "Node 1: -0.0018407821655273436\n",
      "Node 2: 0.00044121742248535154\n",
      "Node 3: 0.0\n",
      "Node 4: 7.524490356445313e-05\n",
      "Node 5: -7.578134536743163e-05\n",
      "Node 6: 0.0008309841156005859\n",
      "Node 7: -0.00022814273834228515\n",
      "Node 8: -0.0009253263473510743\n",
      "Node 9: -0.003994774818420411\n",
      "Node 10: 0.0\n",
      "Node 11: 0.0007683992385864258\n",
      "Node 12: -0.0011681914329528809\n",
      "Node 13: 4.014968872070312e-05\n",
      "Node 14: 0.0\n",
      "Node 15: -0.0018353462219238281\n",
      "Node 16: -0.0006745338439941406\n",
      "Node 17: -0.001815140247344971\n",
      "Node 18: -0.002695620059967041\n",
      "Node 19: 0.0\n",
      "Node 20: 0.0013562440872192383\n",
      "Node 21: -0.0014670729637145995\n",
      "Node 22: -0.0005298018455505371\n",
      "Node 23: -0.001352214813232422\n",
      "Node 24: 0.0001497507095336914\n",
      "Node 25: -0.00017908811569213867\n",
      "Node 26: -0.00031138658523559575\n",
      "Node 27: -0.007463109493255616\n",
      "Node 28: -0.0038578867912292484\n",
      "Node 29: -0.002183234691619873\n",
      "Node 30: -4.6706199645996095e-05\n",
      "Node 31: -0.00023217201232910158\n",
      "Node 32: -0.0005174160003662109\n",
      "Node 33: -9.288787841796873e-05\n",
      "Node 34: -3.409385681152344e-06\n",
      "Node 35: -0.000211024284362793\n",
      "Node 36: -0.00011923313140869141\n",
      "Node 37: -0.00011060237884521482\n",
      "Node 38: -0.006365001201629639\n",
      "Node 39: -0.0026983141899108885\n",
      "Node 40: -0.004924178123474121\n",
      "Node 41: -0.0005797028541564941\n",
      "Node 42: 0.00039932727813720705\n",
      "Node 43: -0.00030653476715087886\n",
      "Node 44: -0.0009437799453735352\n",
      "Node 45: 0.0\n",
      "Node 46: -0.0006805896759033203\n",
      "Node 47: -0.00013967752456665037\n",
      "Node 48: 0.000567615032196045\n",
      "Node 49: 3.0994415283203126e-07\n",
      "Node 50: -0.0007431387901306153\n",
      "Node 51: -0.0025992035865783692\n",
      "Node 52: -0.0001482844352722168\n",
      "Node 53: -0.0020899176597595215\n",
      "Node 54: 0.0\n",
      "Node 55: -0.001860654354095459\n",
      "Node 56: -0.005921304225921631\n",
      "Node 57: -0.002466702461242676\n",
      "Node 58: 0.0009051799774169922\n",
      "Node 59: -0.007501947879791261\n",
      "Node 60: 0.0\n",
      "Node 61: -0.0060723662376403805\n",
      "Node 62: 0.0\n",
      "Node 63: -0.003586208820343018\n",
      "Node 64: 0.0002567887306213379\n",
      "Node 65: -0.0023824453353881838\n",
      "Node 66: -0.0006765961647033691\n",
      "Node 67: -0.0004419207572937012\n",
      "Node 68: -0.003004300594329834\n",
      "Node 69: -0.0002933382987976074\n",
      "Node 70: 7.152557373046876e-08\n",
      "Node 71: -0.004154813289642335\n",
      "Node 72: -0.0013007164001464845\n",
      "Node 73: -0.0062654376029968265\n",
      "Node 74: -6.884336471557612e-05\n",
      "Node 75: -0.0008123517036437988\n",
      "Node 76: -7.613897323608399e-05\n",
      "Node 77: -0.0011734366416931152\n",
      "Node 78: -2.6226043701171877e-07\n",
      "Node 79: -0.01062229871749878\n",
      "Node 80: -0.0005669713020324707\n",
      "Node 81: -0.0015980005264282227\n",
      "Node 82: -0.00022927522659301757\n",
      "Node 83: -0.0013670921325683594\n",
      "Node 84: -0.001287233829498291\n",
      "Node 85: -0.002341556549072266\n",
      "Node 86: -0.00044370889663696293\n",
      "Node 87: -0.0014662742614746094\n",
      "Node 88: 0.0\n",
      "Node 89: 0.0002532482147216797\n",
      "Node 90: -0.0003182172775268555\n",
      "Node 91: -0.002075600624084473\n",
      "Node 92: -0.0010930657386779784\n",
      "Node 93: -0.0006186962127685547\n",
      "Node 94: 0.0\n",
      "Node 95: 0.0\n",
      "Node 96: -0.00035305023193359374\n",
      "Node 97: -0.0012226819992065428\n",
      "Node 98: -0.0009716033935546875\n",
      "Node 99: -0.0007668495178222656\n",
      "Node 100: -0.0014756679534912109\n",
      "Node 101: 0.0\n",
      "Node 102: 0.0\n",
      "Node 103: -0.000177001953125\n",
      "Node 104: -0.0023982405662536623\n",
      "Node 105: -0.000702667236328125\n",
      "Node 106: -0.0006182670593261718\n",
      "Node 107: -0.000719451904296875\n",
      "Node 108: -0.0009042024612426758\n",
      "Node 109: -0.0004195094108581543\n",
      "Node 110: -0.003990089893341065\n",
      "Node 111: -0.0001772880554199219\n",
      "Node 112: 0.00021823644638061527\n",
      "Node 113: 0.00015288591384887698\n",
      "Node 114: -0.0018019318580627442\n",
      "Node 115: -0.0032615065574645996\n",
      "Node 116: 0.0021360039710998533\n",
      "Node 117: -0.0012618780136108398\n",
      "Node 118: -0.0003555893898010254\n",
      "Node 119: -0.001127505302429199\n",
      "Node 120: -0.0034661293029785156\n",
      "Node 121: -0.002553629875183105\n",
      "Node 122: -0.0014696002006530761\n",
      "Node 123: -0.0042486429214477536\n",
      "Node 124: -0.0012107968330383303\n",
      "Node 125: -0.0003769516944885254\n",
      "Node 126: -9.025335311889649e-05\n",
      "Node 127: -0.00014737844467163083\n",
      "63.75391149520874\n"
     ]
    }
   ],
   "source": [
    "l, a = or_model.evaluate(x_test, y_test, verbose=2)\n",
    "size = 128\n",
    "start = time.time()\n",
    "for i in range(128):\n",
    "    w = or_model.get_weights()\n",
    "    w[0][:,i] = 0\n",
    "    w[1][i] = 0\n",
    "    w[2][i,:] = 0\n",
    "    tester_model.set_weights(w)\n",
    "    nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Node {i}:\", 0.6*(na - a) + 0.4*(l - nl))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25%|       | 5/20 [06:07<18:23, 73.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-344be550a8b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         ])\n\u001b[0;32m     17\u001b[0m     \u001b[0mblank_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mblank_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblank_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_test = 20\n",
    "num_zeros = np.zeros(num_test)\n",
    "num_worse = np.zeros(num_test)\n",
    "num_important = np.zeros(num_test)\n",
    "losses = np.zeros(num_test)\n",
    "accs = np.zeros(num_test)\n",
    "zero_nodes = []\n",
    "worsening_nodes = []\n",
    "important_nodes = []\n",
    "tol = -1e-4\n",
    "for j in tqdm.trange(num_test):\n",
    "    blank_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    blank_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    blank_model.fit(x_train, y_train, epochs=10, verbose=0)\n",
    "    l, a = blank_model.evaluate(x_test, y_test, verbose=0)\n",
    "    losses[j] = l\n",
    "    accs[j] = a\n",
    "    z = []\n",
    "    wr = []\n",
    "    imp = []\n",
    "    for i in range(128):\n",
    "        w = blank_model.get_weights()\n",
    "        w[0][:,i] = 0\n",
    "        w[1][i] = 0\n",
    "        w[2][i,:] = 0\n",
    "        tester_model.set_weights(w)\n",
    "        nl, na = tester_model.evaluate(x_test, y_test, verbose=0)\n",
    "        change = 0.8*(na - a) + 0.2*(l - nl)\n",
    "        if change <= 0 and change >= tol:\n",
    "            num_zeros[j] += 1\n",
    "            z += [i]\n",
    "        elif change > 0:\n",
    "            num_worse[j] += 1\n",
    "            wr += [i]\n",
    "        else:\n",
    "            num_important[j] += 1\n",
    "            imp += [i]\n",
    "    zero_nodes += [z]\n",
    "    worsening_nodes += [wr]\n",
    "    important_nodes += [imp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25. 19. 20. 24. 20.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[29. 29. 31. 21. 30.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[74. 80. 77. 83. 78.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(num_zeros)\n",
    "print(num_worse)\n",
    "print(num_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
